{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from extraction.extract import *\n",
    "import math\n",
    "import numpy\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM model that trains on a time period of data in order to make a prediction at a predetermined time interval in the future is coded. Because LSTMs are designed to have a loockback interval where they look into the past to make a prediction in the future, a lot of data preparation must be done to be able to feed it to the model, as will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the LSTM model was generated with the external \"GenerateNNdata\". In order to make it compatible with the LSTM workframe the dataset had to be modified such that a certain number of past timesteps are used in order to predict a certain time in the future. The structure of the final version of the dataset is given by the \"series_to_supervised\" function, which takes care of only including data corresponding to the aforementioned timesteps. Moreover, the unnecessary columns ('departuresArrivalDelay' and 'arrivalsDepartureDelay') are dropped and the remaining ones are reordered such that the target features ('departuresDepartureDelay' and 'arrivalsArrivalDelay') are placed at the end. The data is also scaled and then further processed in order to drop the columns corresponding to future steps which we do not want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "# Generate single airport data and move target labels to the last 2 columns \n",
    "dataset = generateNNdata(\"EGLL\", timeslotLength=15, catagoricalFlightDuration=False, start=datetime(2018, 3, 1), end=datetime(2018, 3, 31), forceRegenerateData=True)\n",
    "dataset = dataset.set_index(\"timeslot\")\n",
    "dataset.drop(columns=['departuresArrivalDelay','arrivalsDepartureDelay'])\n",
    "label1 = dataset.pop('departuresDepartureDelay')\n",
    "dataset.insert(len(dataset.columns), 'departuresDepartureDelay', label1)\n",
    "label2 = dataset.pop('arrivalsArrivalDelay')\n",
    "dataset.insert(len(dataset.columns), 'arrivalsArrivalDelay', label2)\n",
    "# display(dataset)\n",
    "test_dataset = generateNNdata(\"EGLL\", timeslotLength=15, catagoricalFlightDuration=False, start=datetime(2018, 6, 1), end=datetime(2018, 6, 30), forceRegenerateData=True)\n",
    "test_dataset = test_dataset.set_index(\"timeslot\")\n",
    "test_dataset.drop(columns=['departuresArrivalDelay','arrivalsDepartureDelay'])\n",
    "label1 = test_dataset.pop('departuresDepartureDelay')\n",
    "test_dataset.insert(len(test_dataset.columns), 'departuresDepartureDelay', label1)\n",
    "label2 = test_dataset.pop('arrivalsArrivalDelay')\n",
    "test_dataset.insert(len(test_dataset.columns), 'arrivalsArrivalDelay', label2)\n",
    "\n",
    "\n",
    "# Normalize values\n",
    "values = dataset.values\n",
    "test_values = test_dataset.values\n",
    "\n",
    "\n",
    "# Ensure all data is float\n",
    "values = values.astype('float32')\n",
    "test_values = test_values.astype('float32')\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "test_scaled = scaler.fit_transform(test_values)\n",
    "# display(scaled)\n",
    "\n",
    "\n",
    "# Frame as supervised learning\n",
    "number_of_past_steps = 12\n",
    "number_of_future_steps = 12\n",
    "# number_of_future_steps2 = 6\n",
    "number_of_outputs = 2\n",
    "reframed = series_to_supervised(scaled, n_in=number_of_past_steps, n_out=number_of_future_steps)\n",
    "test_reframed = series_to_supervised(test_scaled, n_in=number_of_past_steps, n_out=number_of_future_steps)\n",
    "\n",
    "\n",
    "# Create a list of column names to drop\n",
    "column_drop_lst = []\n",
    "for timestep in range(number_of_future_steps):\n",
    "    for variable in range(1, len(dataset.columns) + 1):\n",
    "        if timestep == 0:\n",
    "            if variable > len(dataset.columns) - number_of_outputs:\n",
    "                column_drop_lst.append(f\"var{variable}(t)\")\n",
    "        elif timestep == number_of_future_steps-1 and variable > len(dataset.columns) - number_of_outputs:\n",
    "            pass\n",
    "        else:\n",
    "            column_drop_lst.append(f\"var{variable}(t+{timestep})\")\n",
    "\n",
    "# column_drop_lst.remove(f'var19(t+{number_of_future_steps2-1})')\n",
    "# column_drop_lst.remove(f'var20(t+{number_of_future_steps2-1})')\n",
    "\n",
    "\n",
    "# Drop columns we don't want to predict\n",
    "reframed.drop(columns=reframed[column_drop_lst], inplace=True)\n",
    "test_reframed.drop(columns=test_reframed[column_drop_lst], inplace=True)\n",
    "display(reframed)\n",
    "# display(test_reframed)\n",
    "\n",
    "\n",
    "# Remove rows that contain variables of night\n",
    "for timestep in range(1, number_of_past_steps):\n",
    "    reframed = reframed.query(f'`var1(t-{timestep})` != 0 | `var2(t-{timestep})` != 0')\n",
    "\n",
    "for timestep in range(1, number_of_past_steps):\n",
    "    no_night_reframed = test_reframed.query(f'`var1(t-{timestep})` != 0 | `var2(t-{timestep})` != 0')\n",
    "\n",
    "display(reframed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is used to explore the distribution of the output variables by plotting their normal distributions and also their correspinding histograms (for visualisation purposes). This will give us an indication for the accuracy of our model and whether the obtained performance metrics make sense in relation to the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depdel = dataset[\"departuresDepartureDelay\"].to_numpy()\n",
    "arrdel = dataset[\"arrivalsArrivalDelay\"].to_numpy()\n",
    "\n",
    "mean_depdel= numpy.mean(depdel)\n",
    "std_depdel = numpy.std(depdel)\n",
    "mean_arrdel = numpy.mean(arrdel)\n",
    "std_arrdel = numpy.std(arrdel)\n",
    "\n",
    "x_depdel = numpy.linspace(mean_depdel - 3*std_depdel, mean_depdel + 3*std_depdel, 100)\n",
    "x_arrdel = numpy.linspace(mean_arrdel - 3*std_arrdel, mean_arrdel + 3*std_arrdel, 100)\n",
    "\n",
    "normal_depdel = norm.pdf(x_depdel, mean_depdel, std_depdel)\n",
    "normal_arrdel = norm.pdf(x_arrdel, mean_arrdel, std_arrdel)\n",
    "\n",
    "plt.figure(1)\n",
    "bin_num = 25\n",
    "plt.title('Delays Distribution')\n",
    "plt.hist(depdel, bins=bin_num, label='Departures Departure Delay')\n",
    "plt.hist(arrdel, bins=bin_num, label='Arrivals Arrival Delay')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.title(\"Delays Normal Distribution\")\n",
    "plt.plot(x_depdel, normal_depdel, label='Departures Departure Delay')\n",
    "plt.plot(x_arrdel, normal_arrdel, label='Arrivals Arrival Delay')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Deprartures Departure Delay mean:\", round(mean_depdel, 2))\n",
    "print(\"Arrivals Arrival Delay mean:\", round(mean_arrdel, 2))\n",
    "print(\"Deprartures Departure Delay standard deviation:\", round(std_depdel, 2))\n",
    "print(\"Arrivals Arrival Delay standard deviation:\", round(std_arrdel, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE AND FIT INITIAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be seen below is the splitting of the data into the training and the test sets. Out of the one month worth of data, 600 hours are used for training and the rest are used for testing. After reshaping the sets into the desired form for the proper functioning of the LSTM, the data is fitted on a single-layer LSTM model. After iterating multiple times for different numbers of epochs, we decided that 50 epochs are enough, since the errors did not change significantly after this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "test_values = test_reframed.values\n",
    "no_night_values = no_night_reframed.values\n",
    "val_ratio = 0.3\n",
    "train_index = int(val_ratio * len(reframed))\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "train = values[:(train_index), :]\n",
    "val = values[train_index:, :]\n",
    "test = test_values\n",
    "no_night = no_night_values\n",
    "\n",
    "# Split into input and outputs\n",
    "train_X, train_y = train[:, :-2], train[:, -2:]\n",
    "val_X, val_y = val[:, :-2], val[:, -2:]\n",
    "test_X, test_y = test[:, :-2], test[:, -2:]\n",
    "no_night_X, no_night_y = no_night[:, :-2], no_night[:, -2:]\n",
    "\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], -1, train_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], -1, val_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], -1, test_X.shape[1]))\n",
    "no_night_X = no_night_X.reshape((no_night_X.shape[0], -1, no_night_X.shape[1]))\n",
    "\n",
    "# Design LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(400, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(50))\n",
    "model.add(LSTM(800, return_sequences=True))\n",
    "model.add(Dense(50))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(25))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Fit data\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data = (test_X, test_y), verbose=0, shuffle=False)\n",
    "\n",
    "\n",
    "# Accuracies of seperate month of data\n",
    "# all_times_accuracy = model.evaluate(test_X, test_y)\n",
    "# no_night_accuracy = model.evaluate(no_night_X, no_night_y)\n",
    "# print(\"Accuracy of all data of June:\", round(all_times_accuracy,5))\n",
    "# print(\"Accuracy of no night data of June:\", round(no_night_accuracy, 5))\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "\n",
    "# Split and reshape the two desired outputs in separate arrays to prepare for inverse scaling\n",
    "yhat1 = yhat[:, 0]\n",
    "yhat2 = yhat[:, 1]\n",
    "yhat1 = yhat1.reshape(len(yhat), 1)\n",
    "yhat2 = yhat2.reshape(len(yhat), 1)\n",
    "\n",
    "\n",
    "# Also split the real values of the labels and reshape for inverse scaling\n",
    "test_y_1 = test_y[:, 0]\n",
    "test_y_2 = test_y[:, 1]\n",
    "test_y_1 = test_y_1.reshape(len(test_y), 1)\n",
    "test_y_2 = test_y_2.reshape(len(test_y), 1)\n",
    "\n",
    "\n",
    "# Inverse transform the predictions\n",
    "inv_yhat_1 = concatenate((yhat1, yhat2, test_X[:, 0:18]), axis=1)\n",
    "inv_yhat_1 = scaler.inverse_transform(inv_yhat_1)\n",
    "inv_yhat_1 = inv_yhat_1[:, :2]\n",
    "\n",
    "\n",
    "# Inverse transform the real values of the labels\n",
    "inv_y_1 = concatenate((test_y_1, test_y_2, test_X[:, 0:18]), axis=1)\n",
    "inv_y_1 = scaler.inverse_transform(inv_y_1)\n",
    "inv_y_1 = inv_y_1[:, :2]\n",
    "\n",
    "\n",
    "# calculate MAE between predicted and actual labels\n",
    "mae1 = mean_absolute_error(inv_y_1[:, 0], inv_yhat_1[:, 0])\n",
    "mae2 = mean_absolute_error(inv_y_1[:, 1], inv_yhat_1[:, 1])\n",
    "\n",
    "\n",
    "print('Test MAE1: %.3f' % mae1)\n",
    "print('Test MAE2: %.3f' % mae2)\n",
    "\n",
    "\n",
    "# Plot loss\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "pyplot.plot(history.history['loss'], label='Train Loss')\n",
    "pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are used for tuning the number of neurons of the LSTM model and the look-back parameter (that is, how much back in time we go for training the LSTM), respectively. The mean absolute errors are plotted against both of these parameters and the minimums are found for each case. In order to keep the running time at an acceptable level, the two tunings are performed separately, in order to offer a general view on the best values of these parameters. It is also observed that the minimum values of the MAE's only vary by a very tiny amount, regardless of the parameters' values. The obtained results are also inverse scaled back to the initial scale for visualising them and for having the MAE's in the desired scale as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae1_lst = []\n",
    "mae2_lst = []\n",
    "neurons_lst = numpy.arange(2, 300, 1)\n",
    "\n",
    "for neurons in neurons_lst:\n",
    "\n",
    "\n",
    "    # Design the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(2*neurons, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50))\n",
    "    model.add(LSTM(4*neurons, return_sequences=True))\n",
    "    model.add(Dense(50))\n",
    "    model.add(LSTM(2*neurons))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    # model.summary()\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data = (val_X, val_y), verbose=0, shuffle=False) #validation_data=(val_X, val_y),\n",
    "\n",
    "\n",
    "    # Generate predictions\n",
    "\n",
    "    yhat = model.predict(val_X)\n",
    "    reshaped_val_X = val_X.reshape((val_X.shape[0], val_X.shape[2]))\n",
    "\n",
    "\n",
    "    # Split and reshape the two desired outputs in separate arrays to prepare for inverse scaling\n",
    "    yhat1 = yhat[:, 0]\n",
    "    yhat2 = yhat[:, 1]\n",
    "    yhat1 = yhat1.reshape(len(yhat), 1)\n",
    "    yhat2 = yhat2.reshape(len(yhat), 1)\n",
    "\n",
    "\n",
    "    # Also split the real values of the labels and reshape for inverse scaling\n",
    "    val_y_1 = val_y[:, 0]\n",
    "    val_y_2 = val_y[:, 1]\n",
    "    val_y_1 = val_y_1.reshape(len(val_y), 1)\n",
    "    val_y_2 = val_y_2.reshape(len(val_y), 1)\n",
    "\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    inv_yhat_1 = concatenate((yhat1, yhat2, reshaped_val_X[:, 0:18]), axis=1)\n",
    "    inv_yhat_1 = scaler.inverse_transform(inv_yhat_1)\n",
    "    inv_yhat_1 = inv_yhat_1[:, :2]\n",
    "\n",
    "\n",
    "    # Inverse transform the real values of the labels\n",
    "    inv_y_1 = concatenate((val_y_1, val_y_2, reshaped_val_X[:, 0:18]), axis=1)\n",
    "    inv_y_1 = scaler.inverse_transform(inv_y_1)\n",
    "    inv_y_1 = inv_y_1[:, :2]\n",
    "\n",
    "\n",
    "    # calculate MAE between predicted and actual labels\n",
    "\n",
    "    mae1 = mean_absolute_error(inv_y_1[:, 0], inv_yhat_1[:, 0])\n",
    "    mae1_lst.append(mae1)\n",
    "    mae2 = mean_absolute_error(inv_y_1[:, 1], inv_yhat_1[:, 1])\n",
    "    mae2_lst.append(mae2)\n",
    "\n",
    "\n",
    "# Plot MAE vs neurons\n",
    "plt.title(\"MAE vs number of neurons\")\n",
    "plt.plot(neurons_lst, mae1_lst, label = 'mae1')\n",
    "plt.plot(neurons_lst, mae2_lst, label = 'mae2')\n",
    "plt.xlabel(\"Neurons\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('minimum mae1 is {} for a number of {} neurons'.format(numpy.min(mae1_lst), neurons_lst[mae1_lst.index(numpy.min(mae1_lst))]))\n",
    "print('minimum mae2 is {} for a number of {} neurons'. format(numpy.min(mae2_lst), neurons_lst[mae2_lst.index(numpy.min(mae2_lst))]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae1_lst = []\n",
    "mae2_lst = []\n",
    "lookback_lst = numpy.arange(1, 100, 1) \n",
    "number_of_future_steps = 6\n",
    "\n",
    "\n",
    "\n",
    "for lookback in lookback_lst:\n",
    "\n",
    "    number_of_past_steps = lookback  \n",
    "    reframed = series_to_supervised(scaled, n_in=number_of_past_steps, n_out=number_of_future_steps)\n",
    "\n",
    "    # drop columns we don't want to predict\n",
    "    reframed.drop(columns=reframed.columns[[i for i in range(len(reframed.columns)-20, len(reframed.columns)-2)]], inplace=True)\n",
    "    values = reframed.values\n",
    "    values = values.astype('float32')\n",
    "\n",
    "\n",
    "    # normalize features\n",
    "    number_of_hours_to_train = 600\n",
    "    number_of_timeslots_in_one_hour = 4 # 4 for 15 minute intervals, 1 for 1 hour intervals\n",
    "    train_index = number_of_hours_to_train * number_of_timeslots_in_one_hour\n",
    "\n",
    "\n",
    "    # split into train and validation sets\n",
    "    train = values[:(train_index), :]\n",
    "    val = values[train_index:, :]\n",
    "\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-2], train[:, -2:]\n",
    "    val_X, val_y = val[:, :-2], val[:, -2:]\n",
    "\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], -1, train_X.shape[1]))\n",
    "    val_X = val_X.reshape((val_X.shape[0], -1, val_X.shape[1]))\n",
    "\n",
    "    # fit network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(400, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50))\n",
    "    model.add(LSTM(800, return_sequences=True))\n",
    "    model.add(Dense(50))\n",
    "    model.add(LSTM(300))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    # model.summary()\n",
    "    \n",
    "    history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data = (val_X, val_y), verbose=0, shuffle=False) #validation_data=(val_X, val_y),\n",
    "\n",
    "\n",
    "    # Generate predictions\n",
    "    yhat = model.predict(val_X)\n",
    "    reshaped_val_X = val_X.reshape((val_X.shape[0], val_X.shape[2]))\n",
    "\n",
    "\n",
    "    # Split and reshape the two desired outputs in separate arrays to prepare for inverse scaling\n",
    "    yhat1 = yhat[:, 0]\n",
    "    yhat2 = yhat[:, 1]\n",
    "    yhat1 = yhat1.reshape(len(yhat), 1)\n",
    "    yhat2 = yhat2.reshape(len(yhat), 1)\n",
    "\n",
    "\n",
    "    # Split real values of the labels and reshape for inverse scaling\n",
    "    val_y_1 = val_y[:, 0]\n",
    "    val_y_2 = val_y[:, 1]\n",
    "    val_y_1 = val_y_1.reshape(len(val_y), 1)\n",
    "    val_y_2 = val_y_2.reshape(len(val_y), 1)\n",
    "\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    inv_yhat_1 = concatenate((yhat1, yhat2, reshaped_val_X[:, 0:18]), axis=1)\n",
    "    inv_yhat_1 = scaler.inverse_transform(inv_yhat_1)\n",
    "    inv_yhat_1 = inv_yhat_1[:, :2]\n",
    "\n",
    "\n",
    "    # Inverse transform the real values of the labels\n",
    "    inv_y_1 = concatenate((val_y_1, val_y_2, reshaped_val_X[:, 0:18]), axis=1)\n",
    "    inv_y_1 = scaler.inverse_transform(inv_y_1)\n",
    "    inv_y_1 = inv_y_1[:, :2]\n",
    "\n",
    "\n",
    "    # Calculate MAE between predicted and actual labels\n",
    "    mae1 = mean_absolute_error(inv_y_1[:, 0], inv_yhat_1[:, 0])\n",
    "    mae1_lst.append(mae1)\n",
    "    mae2 = mean_absolute_error(inv_y_1[:, 1], inv_yhat_1[:, 1])\n",
    "    mae2_lst.append(mae2)\n",
    "\n",
    "\n",
    "# Plot MAE vs lookback\n",
    "plt.title(\"MAEs vs lookback\")\n",
    "plt.plot(lookback_lst, mae1_lst, label = 'mae1')\n",
    "plt.plot(lookback_lst, mae2_lst, label = 'mae2')\n",
    "plt.xlabel(\"Lookback\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('minimum mae1 is {} for a lookback of {}'.format(numpy.min(mae1_lst), lookback_lst[mae1_lst.index(numpy.min(mae1_lst))]))\n",
    "print('minimum mae2 is {} for a lookback of {}'. format(numpy.min(mae2_lst), lookback_lst[mae2_lst.index(numpy.min(mae2_lst))]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION OF FINAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the final model consisting of the optimal obtained parameters (which can be seen below) is fitted and run again in order to obtain what should be the most accurate results as far as the performed tuning allows us to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 10\n",
    "number_of_past_steps = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "values = dataset.values\n",
    "# display(values)\n",
    "\n",
    "\n",
    "# Ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# print(scaled)\n",
    "\n",
    "# Frame as supervised learning\n",
    "number_of_future_steps = 12\n",
    "number_of_outputs = 2\n",
    "reframed = series_to_supervised(scaled, n_in=number_of_past_steps, n_out=number_of_future_steps)\n",
    "\n",
    "\n",
    "# Create a list of column names to drop\n",
    "column_drop_lst = []\n",
    "\n",
    "for timestep in range(number_of_future_steps):\n",
    "    for variable in range(1, len(dataset.columns) + 1):\n",
    "        if timestep == 0:\n",
    "            if variable > len(dataset.columns) - number_of_outputs:\n",
    "                column_drop_lst.append(f\"var{variable}(t)\")\n",
    "        elif timestep == number_of_future_steps-1 and variable > len(dataset.columns) - number_of_outputs:\n",
    "            pass\n",
    "        else:\n",
    "            column_drop_lst.append(f\"var{variable}(t+{timestep})\")\n",
    "# print(column_drop_lst)\n",
    "\n",
    "\n",
    "# Drop columns we don't want to predict\n",
    "reframed.drop(columns=reframed[column_drop_lst], inplace=True)\n",
    "\n",
    "\n",
    "display(reframed)\n",
    "\n",
    "\n",
    "# Train index calculation\n",
    "values = reframed.values\n",
    "number_of_hours_to_train = 600\n",
    "number_of_timeslots_in_one_hour = 4 # 4 for 15 minute intervals, 1 for 1 hour intervals\n",
    "train_index = number_of_hours_to_train * number_of_timeslots_in_one_hour\n",
    "\n",
    "\n",
    "# Split into train and test sets\n",
    "train = values[:(train_index), :]\n",
    "test = values[train_index:, :]\n",
    "\n",
    "\n",
    "# Split into input and outputs\n",
    "train_X, train_y = train[:, :-2], train[:, -2:]\n",
    "test_X, test_y = test[:, :-2], test[:, -2:]\n",
    "\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], -1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], -1, test_X.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINE AND FIT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design final model\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "# Fit data\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data = (test_X, test_y), verbose=0, shuffle=False) #validation_data=(val_X, val_y),\n",
    "\n",
    "\n",
    "# Plot loss\n",
    "plt.title(\"Train and Validation loss vs Epochs\")\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUATE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "\n",
    "# Split and reshape the two desired outputs in separate arrays to prepare for inverse scaling\n",
    "yhat1 = yhat[:, 0]\n",
    "yhat2 = yhat[:, 1]\n",
    "yhat1 = yhat1.reshape(len(yhat), 1)\n",
    "yhat2 = yhat2.reshape(len(yhat), 1)\n",
    "\n",
    "\n",
    "# Also split the real values of the labels and reshape for inverse scaling\n",
    "test_y_1 = test_y[:, 0]\n",
    "test_y_2 = test_y[:, 1]\n",
    "test_y_1 = test_y_1.reshape(len(test_y), 1)\n",
    "test_y_2 = test_y_2.reshape(len(test_y), 1)\n",
    "\n",
    "\n",
    "# Inverse transform the predictions\n",
    "inv_yhat_1 = concatenate((yhat1, yhat2, test_X[:, 0:18]), axis=1)\n",
    "inv_yhat_1 = scaler.inverse_transform(inv_yhat_1)\n",
    "inv_yhat_1 = inv_yhat_1[:, :2]\n",
    "\n",
    "\n",
    "# Inverse transform the real values of the labels\n",
    "inv_y_1 = concatenate((test_y_1, test_y_2, test_X[:, 0:18]), axis=1)\n",
    "inv_y_1 = scaler.inverse_transform(inv_y_1)\n",
    "inv_y_1 = inv_y_1[:, :2]\n",
    "\n",
    "\n",
    "# calculate MAE between predicted and actual labels\n",
    "mae1 = mean_absolute_error(inv_y_1[:, 0], inv_yhat_1[:, 0])\n",
    "mae2 = mean_absolute_error(inv_y_1[:, 1], inv_yhat_1[:, 1])\n",
    "\n",
    "\n",
    "print('Test MAE1: %.3f' % mae1)\n",
    "print('Test MAE2: %.3f' % mae2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON SCALED LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt to see how the model would wok if we don't scale the outputs. However it's a bit outdated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "dataset = generateNNdata(\"EHAM\", timeslotLength=15, catagoricalFlightDuration=False)\n",
    "dataset = dataset.set_index(\"timeslot\")\n",
    "dataset.drop(columns=['departuresArrivalDelay','arrivalsArrivalDelay'])\n",
    "label = dataset.pop('departuresDepartureDelay')\n",
    "dataset.insert(len(dataset.columns), 'departuresDepartureDelay', label)\n",
    "\n",
    "\n",
    "# Get first month of Data\n",
    "number_of_months = 1\n",
    "index_slice = number_of_months * 4 * 24 * 31 - 1\n",
    "dataset = dataset.iloc[0:index_slice]\n",
    "\n",
    "\n",
    "# summarize first 5 rows\n",
    "display(dataset)\n",
    "\n",
    "\n",
    "# Normalize values\n",
    "values = dataset.values\n",
    "# display(values)\n",
    "X, y = values[:,:-1], values[:,-1]\n",
    "display(X)\n",
    "display(y)\n",
    "print(y.shape)\n",
    "print(type(X), type(y))\n",
    "\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(X)\n",
    "# display(scaled)\n",
    "y = y.reshape((len(y), 1))\n",
    "scaled = concatenate((X, y), axis=1)\n",
    "display(scaled)\n",
    "\n",
    "\n",
    "# frame as supervised learning\n",
    "number_of_time_steps = 1\n",
    "number_of_outputs = 1\n",
    "reframed = series_to_supervised(scaled, n_in=number_of_time_steps, n_out=number_of_outputs)\n",
    "\n",
    "\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[i for i in range(20,39)]], axis=1, inplace=True)  # I don't think we need this, but not sure\n",
    "display(reframed)\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "number_of_days_to_train = 20\n",
    "number_of_timeslots_in_one_hour = 4 # 4 for 15 minute intervals, 1 for 1 hour intervals\n",
    "train_index = number_of_days_to_train * 24 * number_of_timeslots_in_one_hour\n",
    "train = values[:train_index, :]\n",
    "test = values[train_index:, :]\n",
    "print('Shape of dataset:', reframed.shape)\n",
    "print('Shape of train dataset:', train.shape)\n",
    "print('Shape of test dataset:', test.shape)\n",
    "\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], -1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], -1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=75, batch_size=72, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
    "\n",
    "\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "\n",
    "\n",
    "# calculate MAE\n",
    "mae = mean_absolute_error(test_y, yhat)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a906fffd0068e70e9fb80d992d8da3d10a7e052a8ae415c1855a00961817ff15"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
