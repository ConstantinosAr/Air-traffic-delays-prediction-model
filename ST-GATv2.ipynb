{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogarithmicError\n",
    "\n",
    "from spektral.data.loaders import SingleLoader, DisjointLoader, BatchLoader\n",
    "from spektral.datasets.citation import Citation\n",
    "from spektral.layers import GATConv, DiffusionConv, GCNConv\n",
    "from spektral.transforms import LayerPreprocess\n",
    "\n",
    "from extraction.extract import *\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency\n",
    "# from extraction.adj_data import *\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsFull = ICAOTOP10\n",
    "airports = airportsFull[::]\n",
    "airports.remove(\"LTFM\")  # removing the new istanbul airport as it opened mid-2019\n",
    "\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2018, 3, 1)\n",
    "end = datetime(2019, 1, 1)\n",
    "timeslotLength = 30\n",
    "\n",
    "# Run settings\n",
    "batch_size = 64\n",
    "epochs = 1200\n",
    "patience = 100\n",
    "lookback = 8\n",
    "lookahead = 6\n",
    "multi_horizon = True\n",
    "\n",
    "learning_rate = 0.0001\n",
    "learning_rate = 0.0005\n",
    "# learning_rate = 0.05\n",
    "\n",
    "# runName = \"top50MSE\"\n",
    "saveModel = True\n",
    "loadModel = False\n",
    "# modelSaveFolder = \"kerasModels/\"\n",
    "# modelSaveLocation = modelSaveFolder + runName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 17.30it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21232/2807183411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtimesdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "    airports, timeslotLength, GNNFormat=True, start=start, end=end, disableWeather=True\n",
    ")\n",
    "times = list(dataDict.values())[0][\"T\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['departing', 'arriving', 'lowcost', 'arrivalsFlightDuration',\n",
      "       'departuresFlightDuration', 'capacityFilled', 'arrivalsArrivalDelay',\n",
      "       'departuresDepartureDelay'],\n",
      "      dtype='object')\n",
      "T x N x F:  Xarray = (5855, 9, 8) | Yarray = (5855, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "# we remove some of the most uncorrelated features\n",
    "columnsToDrop = [\n",
    "    \"weekend\",\n",
    "    \"winter\",\n",
    "    \"spring\",\n",
    "    \"summer\",\n",
    "    \"autumn\",\n",
    "    \"night\",\n",
    "    \"morning\",\n",
    "    \"afternoon\",\n",
    "    \"planes\",\n",
    "    \"arrivalsDepartureDelay\",\n",
    "    \"evening\"\n",
    "]\n",
    "\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler = RobustScaler()\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1)\n",
    "    # X = scaler.fit_transform(X)\n",
    "    Y = dataDict[airport][\"Y\"].iloc[:, :]\n",
    "    # T = dataDict[airport][\"T\"].iloc[:, :]\n",
    "\n",
    "\n",
    "    # Adding back delay as a feature\n",
    "    # We shift the delay features back by one to prevent dataleaks.\n",
    "    a, b = X.iloc[1:,:].reset_index(drop=True), Y.iloc[:-1,:].reset_index(drop=True)\n",
    "\n",
    "    X = pd.concat([a, b], axis=1)\n",
    "    if airport == \"EGLL\":\n",
    "        print(X.columns)\n",
    "    Xlist.append(X.to_numpy())\n",
    "    Ylist.append(Y.iloc[1:,:].to_numpy())\n",
    "\n",
    "\n",
    "Xlist = np.stack(Xlist)\n",
    "Ylist = np.stack(Ylist)\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# T x N x F\n",
    "\n",
    "# Normalise over the features\n",
    "Xmean, Xstd = Xarray.mean(axis=0), Xarray.std(axis=0)\n",
    "Xarray = (Xarray - Xmean) / Xstd\n",
    "\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)\n",
    "\n",
    "nFeatures = Xarray.shape[2]\n",
    "\n",
    "# print(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_adj = distance_weight_adjacency(airports, threshold=400)\n",
    "flight_adj = getAdjacencyMatrix(airports, start, end, timeslotLength)\n",
    "\n",
    "Aarray = distance_adj * 0.4 + (1 - 0.4) * flight_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 0:3513 | Validation split: 3513:4977 | Test split: 4977:5856\n",
      "(14688, 9, 9)\n",
      "Train arrays len: 3513 3513 3513 57\n",
      "val arrays len: 1464 1464 1464 56\n",
      "test arrays len: 878 878 9711 46\n",
      "5855\n",
      "(3513, 9, 8)\n"
     ]
    }
   ],
   "source": [
    "train_split, val_split = 0.6, 0.25\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(\n",
    "    f\"Train split: {0}:{train_idx} | Validation split: {train_idx}:{val_idx} | Test split: {val_idx}:{fullLength}\"\n",
    ")\n",
    "\n",
    "\n",
    "# train_idx = train_idx-(train_idx%batch_size)\n",
    "# val_idx = val_idx-((val_idx-train_idx)%batch_size)\n",
    "# final = fullLength-((fullLength-val_idx)%batch_size)\n",
    "# # val_idx = train_idx-(train_idx%batch_size)\n",
    "# print(final)\n",
    "# generate raw splits\n",
    "\n",
    "print(Aarray.shape)\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx:]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx:]\n",
    "Atrain, Aval, Atest = Aarray[0:train_idx], Aarray[train_idx:val_idx], Aarray[val_idx:]\n",
    "\n",
    "# Save test timeslots for plotting purposes\n",
    "testTimes = times.iloc[val_idx::, 0].tolist()\n",
    "\n",
    "print(\"Train arrays len:\", len(Xtrain), len(Ytrain), len(Atrain), len(Xtrain)%batch_size)\n",
    "print(\"val arrays len:\", len(Xval), len(Yval), len(Aval), len(Xval)%batch_size)\n",
    "print(\"test arrays len:\", len(Xtest), len(Ytest), len(Atest), len(Xtest)%batch_size)\n",
    "print(len(Xtrain) + len(Xval) + len(Xtest))\n",
    "\n",
    "print(Xtrain.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tf_dataset(\n",
    "#     features_Array: np.ndarray,\n",
    "#     adjacencies_Array: np.ndarray,\n",
    "#     target_array,\n",
    "#     lookback: int,\n",
    "#     lookahead: int,\n",
    "#     batch_size: int = 128,\n",
    "#     multi_horizon=False,\n",
    "# ):\n",
    "\n",
    "#     X_in = timeseries_dataset_from_array(\n",
    "#         features_Array[:-lookahead],\n",
    "#         None,\n",
    "#         sequence_length=lookback,\n",
    "#         shuffle=False,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "#     A_in = timeseries_dataset_from_array(\n",
    "#         adjacencies_Array[:-lookahead],\n",
    "#         None,\n",
    "#         sequence_length=lookback,\n",
    "#         shuffle=False,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "\n",
    "#     target_offset = lookback if multi_horizon else lookback + lookahead - 1\n",
    "#     target_seq_length = lookahead if multi_horizon else 1\n",
    "   \n",
    "#     targets = timeseries_dataset_from_array(\n",
    "#         target_array[target_offset:],\n",
    "#         None,\n",
    "#         sequence_length=target_seq_length,\n",
    "#         shuffle=False,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "#     # dataset = tf.data.Dataset.zip((X_in, A_in))\n",
    "#     dataset = tf.data.Dataset.zip(((X_in, A_in), targets))\n",
    "#     return dataset#.prefetch(16).cache()\n",
    "\n",
    "\n",
    "# train_dataset = create_tf_dataset(\n",
    "#     Xtrain,\n",
    "#     Atrain,\n",
    "#     Ytrain,\n",
    "#     lookback,\n",
    "#     lookahead,\n",
    "#     batch_size=batch_size,\n",
    "#     multi_horizon=multi_horizon,\n",
    "# )\n",
    "\n",
    "# val_dataset = create_tf_dataset(\n",
    "#     Xval,\n",
    "#     Aval,\n",
    "#     Yval,\n",
    "#     lookback,\n",
    "#     lookahead,\n",
    "#     batch_size=batch_size,\n",
    "#     multi_horizon=multi_horizon,\n",
    "# )\n",
    "# test_dataset = create_tf_dataset(\n",
    "#     Xtest,\n",
    "#     Atest,\n",
    "#     Ytest,\n",
    "#     lookback,\n",
    "#     lookahead,\n",
    "#     batch_size=batch_size,\n",
    "#     multi_horizon=multi_horizon,\n",
    "# )\n",
    "\n",
    "# print((train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3513\n",
      "(3456, 8, 9, 8)\n",
      "(3456, 8, 9, 9)\n",
      "(3456, 6, 9, 2)\n",
      "[[-1.21946439 -0.1298735 ]\n",
      " [-2.44970095  0.06549143]\n",
      " [-1.21946439 -1.88815788]\n",
      " [-1.21946439 -0.52060336]\n",
      " [-1.21946439 -0.52060336]\n",
      " [-1.21946439 -2.27888774]\n",
      " [-1.21946439 -0.52060336]]\n",
      "\n",
      "[[-1.21946439 -0.1298735 ]\n",
      " [-2.44970095  0.06549143]\n",
      " [-1.21946439 -1.88815788]\n",
      " [-1.21946439 -0.52060336]\n",
      " [-1.21946439 -0.52060336]\n",
      " [-1.21946439 -2.27888774]\n",
      " [-1.21946439 -0.52060336]\n",
      " [-1.21946439 -0.52060336]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def createDataset(\n",
    "    features_Array: np.ndarray,\n",
    "    adjacencies_Array: np.ndarray,\n",
    "    target_array,\n",
    "    lookback: int,\n",
    "    lookahead: int,\n",
    "    batch_size: int = 64,\n",
    "    multi_horizon=True,\n",
    "    shuffle_dates=True,\n",
    "    times=None\n",
    "):\n",
    "\n",
    "    print(len(features_Array))\n",
    "\n",
    "    if shuffle_dates:\n",
    "        # Shuffle days\n",
    "        times\n",
    "    \n",
    "    \n",
    "    # Delete time thing\n",
    "\n",
    "    # make windows for the features\n",
    "    idx=0\n",
    "    Fsamples = []\n",
    "    Asamples = []\n",
    "    Ysamples = []\n",
    "\n",
    "    while True:\n",
    "        if len(features_Array[idx::]) <= (lookback + lookahead):\n",
    "            break\n",
    "\n",
    "        # Features\n",
    "        Fsample = features_Array[idx:idx+lookback,:,:]\n",
    "        Fsamples.append(Fsample)\n",
    "\n",
    "        # Adjacencies\n",
    "        Asample = adjacencies_Array[idx:idx+lookback,:,:]\n",
    "        Asamples.append(Asample)\n",
    "\n",
    "        # make labels with multi-horizon\n",
    "        if multi_horizon:\n",
    "            Ysample = target_array[idx+lookback:idx+lookback+lookahead,:,:]\n",
    "            Ysamples.append(Ysample)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Non-mulit-horizon not been implemented\")\n",
    "\n",
    "\n",
    "        idx+=1\n",
    "\n",
    "    features = np.array(Fsamples)\n",
    "    adjacencies = np.array(Asamples)\n",
    "    labels = np.array(Ysamples)\n",
    "\n",
    "    # Ensure only complete batches are made\n",
    "    batchCutoff = features.shape[0] - (features.shape[0] % batch_size)\n",
    "\n",
    "    features, adjacencies, labels = features[:batchCutoff,:,:,:], adjacencies[:batchCutoff,:,:,:], labels[:batchCutoff,:,:,:]\n",
    "    # print((Fsamples[0]))\n",
    "    print(features.shape)\n",
    "    print(adjacencies.shape)\n",
    "    print(labels.shape)\n",
    "    return features, adjacencies, labels\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "x,a,y = createDataset(\n",
    "    Xtrain,\n",
    "    Atrain,\n",
    "    Ytrain,\n",
    "    lookback,\n",
    "    lookahead,\n",
    "    batch_size=batch_size,\n",
    "    multi_horizon=multi_horizon,\n",
    "    times=times\n",
    ")\n",
    "\n",
    "print(Xtrain[0:9,0,6:])\n",
    "# print(Xtrain[0,0,:])\n",
    "print(\"\")\n",
    "print(x[0,:,0,6:])\n",
    "print(y[0,0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookback = 8 # steps\n",
    "l2_reg = 2.5e-4  # L2 regularization rate\n",
    "# learning_rate = 2e-4  # Learning rate\n",
    "\n",
    "channels = 5\n",
    "attn_heads = 4\n",
    "lstm_units = 20\n",
    "\n",
    "num_labels = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = layers.Input(shape=(lookback, n_nodes, nFeatures), batch_size=batch_size, name=\"Features\")\n",
    "a_in = layers.Input(shape=(lookback, n_nodes, n_nodes), batch_size=batch_size, name=\"Adjacencies\")\n",
    "\n",
    "print(x_in.shape)\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "# Xin has shape (batch_size, time, airports, features)\n",
    "# x_in = tf.transpose(x_in, [2, 0, 1, 3])\n",
    "\n",
    "batchsiz = x_in.shape[0]\n",
    "x_inn = tf.reshape(x_in, (batchsiz*lookback, n_airports, nFeatures))\n",
    "a_inn = tf.reshape(a_in, (batchsiz*lookback, n_airports, n_airports))\n",
    "\n",
    "gatout = GATConv(\n",
    "    channels=channels,\n",
    "    attn_heads=attn_heads,\n",
    "    kernel_regularizer=l2(l2_reg),\n",
    "    attn_kernel_regularizer=l2(l2_reg),\n",
    "    bias_regularizer=l2(l2_reg),\n",
    "    name=f\"GAT\"\n",
    ")([x_inn, a_inn])\n",
    "print(gatout.shape)\n",
    "\n",
    "reshape1 = tf.reshape(gatout, (batchsiz, lookback, n_airports, attn_heads*channels))\n",
    "print(reshape1.shape)\n",
    "reshape1 = tf.transpose(reshape1, [0, 2, 1, 3])\n",
    "print(reshape1.shape)\n",
    "reshape1 = tf.reshape(reshape1, (batchsiz*n_airports, lookback, attn_heads*channels))\n",
    "print(reshape1.shape)\n",
    "# break\n",
    "# reshape1 = tf.reshape(concat, (batchsiz, n_nodes, channels * attn_heads, lookback), name=\"Reshape1\")\n",
    "# print(reshape1.shape)\n",
    "# reshape2 = tf.reshape(reshape1, (batchsiz * n_nodes, channels * attn_heads, lookback), name=\"Reshape2\")\n",
    "# reshape2 = tf.transpose(reshape2, [0, 2, 1], name=\"TransposetoLSTM\")\n",
    "# print(reshape2.shape)\n",
    "\n",
    "lstm1 = layers.LSTM(lstm_units, return_sequences=True, name=\"LSTM1\")(reshape1)\n",
    "lstm2 = layers.LSTM(lstm_units, return_sequences=False, name=\"LSTM2\")(lstm1)\n",
    "print(lstm2.shape)\n",
    "dense1 = layers.Dense(lstm_units, name=\"Dense1\")(lstm2)\n",
    "print(dense1.shape)\n",
    "dense2 = layers.Dense(num_labels * lookahead, name=\"DenseFinal\")(dense1)\n",
    "print(dense2.shape)\n",
    "\n",
    "output = tf.reshape(dense2, (batchsiz, n_nodes, lookahead, num_labels), name=\"ReshapeFinal\")\n",
    "# output = tf.reshape(dense2, (n_nodes, batchsiz, lookahead, num_labels), name=\"ReshapeFinal\")\n",
    "print(output.shape)\n",
    "output = tf.transpose(output, [0, 2, 1, 3], name=\"Re-orderToOutput\")\n",
    "print(output.shape)\n",
    "# break\n",
    "\n",
    "\n",
    "\n",
    "# # Test \n",
    "# print(\"hello\")\n",
    "# output2 = layers.Dense(128)(reshape1)\n",
    "# print(output2.shape)\n",
    "# output2 = layers.Dense(1)(output2)\n",
    "# print(output2.shape)\n",
    "# output = tf.reshape(output2, (n_nodes, batch_size, lookahead, num_labels), name=\"ReshapeFinal\")\n",
    "# print(output.shape)\n",
    "# output = tf.transpose(output, [1, 2, 0, 3], name=\"Re-orderToOutput\")\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[x_in, a_in], outputs=output, name=\"Tristan\")\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate*10)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=MeanSquaredError(reduction=\"auto\", name=\"mean_absolute_error\"),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1\n",
    ")\n",
    "reduceLR = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=learning_rate)\n",
    "\n",
    "print(train_dataset.element_spec)\n",
    "\n",
    "history = model.fit(\n",
    "    # x = [train_dataset[0], train_dataset[1]],\n",
    "    # y = train_dataset[2],\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience=10),\n",
    "        tensorboard_callback,\n",
    "        reduceLR\n",
    "    ],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 200  # 1-400\n",
    "\n",
    "forecastlen = 1  # 1-10\n",
    "forecastlen = min(forecastlen, lookahead)\n",
    "print(f\"Hours ahead = {forecastlen / (60/timeslotLength)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelArrays(hour, lookback=lookback):\n",
    "    syncindex = lookback + hour - 1\n",
    "    yTestPred = model.predict(test_dataset, verbose=0)\n",
    "    ypredFull = yTestPred[:windowSize, hour - 1, :, :]\n",
    "    yactualFull = Ytest[syncindex : windowSize + syncindex :, :]\n",
    "\n",
    "    mae1 = mean_absolute_error(yactualFull[:, :, 0], ypredFull[:, :, 0])\n",
    "    mae2 = mean_absolute_error(yactualFull[:, :, 1], ypredFull[:, :, 1])\n",
    "    rmse1 = np.sqrt(mean_squared_error(yactualFull[:, :, 0], ypredFull[:, :, 0]))\n",
    "    rmse2 = np.sqrt(mean_squared_error(yactualFull[:, :, 1], ypredFull[:, :, 1]))\n",
    "    r2_1 = r2_score(yactualFull[:, :, 0], ypredFull[:, :, 0])\n",
    "    r2_2 = r2_score(yactualFull[:, :, 1], ypredFull[:, :, 1])\n",
    "\n",
    "    time = testTimes[syncindex : windowSize + syncindex :]\n",
    "    return ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time\n",
    "\n",
    "\n",
    "def plotComparison(airport_index, hour):\n",
    "    ypredFull, yactualFull, _, _, _, _, _, _, time = getLabelArrays(hour)\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, num=airport_index)\n",
    "    axs[0].plot(\n",
    "        time, yactualFull[:, 0 + airport_index, 0], label=\"Actual Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, yactualFull[:, airport_index, 1], label=\"Actual Departure Delay\")\n",
    "    axs[0].plot(\n",
    "        time, ypredFull[:, 0 + airport_index, 0], label=\"Predicted Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, ypredFull[:, airport_index, 1], label=\"Predicted Departure Delay\")\n",
    "    axs[0].legend()\n",
    "    # axs[1].legend()\n",
    "    axs[1].set_xlabel(\"Time (hours)\")\n",
    "    axs[0].set_ylabel(\"Arrival Delay (mins)\")\n",
    "    axs[1].set_ylabel(\"Departure Delay (mins)\")\n",
    "    plt.suptitle(\n",
    "        f\"Comparison for: {airports[airport_index]}. Forward: {hour / (60/timeslotLength)}h, Backward: {lookback / (60/timeslotLength)}h\"\n",
    "    )\n",
    "    axs[1].xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "    axs[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(6)\n",
    "\n",
    "\n",
    "def plotErrorVsLookback(lookahead=lookahead, save=True, saveFolder=\"GNNPredictions\"):\n",
    "    fig2, axss = plt.subplots(3, 1, sharex=True, num=100)\n",
    "    idxs = list(range(1, lookahead + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, lookahead+1)]\n",
    "    maeList = []\n",
    "    maeList2 = []\n",
    "    rmseList = []\n",
    "    rmseList2 = []\n",
    "    r2List = []\n",
    "    r2List2 = []\n",
    "\n",
    "    for h in idxs:\n",
    "        ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time = getLabelArrays(h)\n",
    "        maeList.append(mae1)\n",
    "        maeList2.append(mae2)\n",
    "        rmseList.append(rmse1)\n",
    "        rmseList2.append(rmse2)\n",
    "        r2List.append(r2_1)\n",
    "        r2List2.append(r2_2)\n",
    "\n",
    "\n",
    "    \n",
    "    axss[0].plot(lookforwards, maeList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[0].plot(lookforwards, maeList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List2, label = \"Departure Delay\", marker=\"o\")\n",
    "\n",
    "    axss[0].set_ylabel(\"MAE (minutes)\")\n",
    "    axss[1].set_ylabel(\"RMSE (minutes)\")\n",
    "    axss[2].set_ylabel(\"R2 Score\")\n",
    "    # axss[0].grid()\n",
    "    # axss[1].grid()\n",
    "    # axss[2].grid()\n",
    "\n",
    "    axss[2].set_xlabel(\"Look forward (hours)\")\n",
    "\n",
    "    axss[0].legend()\n",
    "\n",
    "    fig2.set_figwidth(8)\n",
    "    fig2.set_figheight(12)\n",
    "\n",
    "plotErrorVsLookback()\n",
    "plt.show()\n",
    "\n",
    "for airportidx in range(0, int(len(airports)/5)):\n",
    "    plotComparison(airportidx, forecastlen)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Save to csv\n",
    "def savePredictions(saveFolder = \"predictions\"):\n",
    "    print(\"Saving to CSV\")\n",
    "    idxs = list(range(1, lookahead + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, lookahead+1)]\n",
    "    maxlen = len(Ytest)\n",
    "\n",
    "    if not os.path.exists(saveFolder):\n",
    "        os.makedirs(saveFolder)\n",
    "\n",
    "    for idx, lookforward in zip(idxs, lookforwards):\n",
    "        syncindex = lookback + idx - 1\n",
    "        yTestPred = model.predict(test_dataset, verbose=0)\n",
    "        ypredFull = yTestPred[:maxlen-syncindex, idx - 1, :, :]\n",
    "\n",
    "        yactualFull = Ytest[syncindex:maxlen - idxs[-1] + idx:, :]\n",
    "\n",
    "        time = testTimes[syncindex:maxlen - idxs[-1] + idx]\n",
    "        # print(ypredFull.shape, yactualFull.shape, len(time))\n",
    "\n",
    "        P = pd.DataFrame()\n",
    "        P[\"time\"] = time\n",
    "        # Double for loop here unfortunately ~fix later~\n",
    "        for idd, airport in enumerate(airports):\n",
    "            P[f\"{airport}_arr_actual\"] = yactualFull[:, idd, 0]\n",
    "            P[f\"{airport}_arr_predicted\"] = ypredFull[:, idd, 0]\n",
    "            P[f\"{airport}_dep_actual\"] = yactualFull[:, idd, 1]\n",
    "            P[f\"{airport}_dep_predicted\"] = ypredFull[:, idd, 1]\n",
    "        # print(P)\n",
    "        savelocation = saveFolder + \"/\" + f\"{lookforward}h.csv\"\n",
    "        P.to_csv(savelocation)\n",
    "\n",
    "\n",
    "# savePredictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
