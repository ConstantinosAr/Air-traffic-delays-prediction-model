{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "\n",
    "from extraction.extract import generateNNdataMultiple\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = ICAOTOP10\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2019, 3, 1)\n",
    "end = datetime(2019, 3, 31)\n",
    "timeslotLength = 60\n",
    "\n",
    "# Run settings\n",
    "batch_size = 64\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "            airports,\n",
    "            timeslotLength,\n",
    "            GNNFormat=True,\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "times = (list(dataDict.values())[0][\"T\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToDrop = [\"weekend\", \"winter\", \"spring\", \"summer\", \"autumn\", \"night\", \"morning\", \"afternoon\"]\n",
    "# print(dataDict[\"EGLL\"][\"X\"].shape)\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1).to_numpy()\n",
    "    Xlist.append(X)\n",
    "    \n",
    "    Y = dataDict[airport][\"Y\"].to_numpy()\n",
    "    Ylist.append(Y)\n",
    "\n",
    "# for u in range(len(airports)):\n",
    "#     print(Xlist[u].shape)\n",
    "    \n",
    "print(Xlist[2].shape)\n",
    "Xlist = np.stack(Xlist)\n",
    "Ylist = np.stack(Ylist)\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# Reshape to a flat array that goes arrival then departure delay\n",
    "Yarray = np.reshape(Yarray, newshape=[len(times), len(airports)*2], order=\"F\")\n",
    "\n",
    "# print(Yarray.shape)\n",
    "# T x N x F\n",
    "\n",
    "# Normalise over the features\n",
    "Xmean, Xstd = X.mean(axis=0), X.std(axis=0)\n",
    "X = (X - Xmean) / Xstd\n",
    "print(\"X Mean Shape\", Xmean.shape)\n",
    "Ymean, Ystd = Y.mean(axis=0), Y.std(axis=0)\n",
    "Y = (Y - Ymean) / Ystd\n",
    "print(\"Y Mean Shape\", Ymean.shape)\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = 0.6, 0.1\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(train_idx, val_idx)\n",
    "\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "\n",
    "\n",
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    target_array,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "    `inputs` is a Tensor\n",
    "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "    the `input_sequence_length` past values of the timeseries for each node.\n",
    "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "    containing the `forecast_horizon`\n",
    "    future values of the timeseries for each node.\n",
    "\n",
    "    Args:\n",
    "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "            timeseries `forecast_horizon` steps ahead (only one value).\n",
    "        batch_size: Number of timeseries samples in each batch.\n",
    "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "        multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = inputs\n",
    "    target_offset = (\n",
    "       input_sequence_length\n",
    "       if multi_horizon\n",
    "       else input_sequence_length + forecast_horizon - 1) \n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        target_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,)\n",
    "    \n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_dataset =  create_tf_dataset(\n",
    "    Xtrain, Ytrain,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "val_dataset =  create_tf_dataset(\n",
    "    Xval, Yval,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "test_dataset =  create_tf_dataset(\n",
    "    Xtest, Ytest,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphInfo:\n",
    "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
    "        self.edges = edges\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "adjacency_matrix = distance_weight_adjacency(airports, threshold=100000) # getAdjacencyMatrix(airports)[10]\n",
    "print(adjacency_matrix.shape)\n",
    "node_indices, neighbor_indices = np.where(adjacency_matrix != 0)\n",
    "graph = GraphInfo(\n",
    "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
    "    num_nodes=adjacency_matrix.shape[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adjacency_matrix)\n",
    "plt.xticks(range(len(airports)), airports, fontsize=12, rotation=-90)\n",
    "plt.yticks(range(len(airports)), airports, fontsize=12, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphConv(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        graph_info: GraphInfo,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        activation: typing.Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.graph_info = graph_info\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=keras.initializers.glorot_uniform()(\n",
    "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.activation = layers.Activation(activation)\n",
    "\n",
    "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
    "        aggregation_func = {\n",
    "            \"sum\": tf.math.unsorted_segment_sum,\n",
    "            \"mean\": tf.math.unsorted_segment_mean,\n",
    "            \"max\": tf.math.unsorted_segment_max,\n",
    "        }.get(self.aggregation_type)\n",
    "\n",
    "        if aggregation_func:\n",
    "            return aggregation_func(\n",
    "                neighbour_representations,\n",
    "                self.graph_info.edges[0],\n",
    "                num_segments=self.graph_info.num_nodes,\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
    "\n",
    "    def compute_nodes_representation(self, features: tf.Tensor):\n",
    "        \"\"\"Computes each node's representation.\n",
    "\n",
    "        The nodes' representations are obtained by multiplying the features tensor with\n",
    "        `self.weight`. Note that\n",
    "        `self.weight` has shape `(in_feat, out_feat)`.\n",
    "\n",
    "        Args:\n",
    "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        return tf.matmul(features, self.weight)\n",
    "\n",
    "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
    "        neighbour_representations = tf.gather(features, self.graph_info.edges[1])\n",
    "        aggregated_messages = self.aggregate(neighbour_representations)\n",
    "        return tf.matmul(aggregated_messages, self.weight)\n",
    "\n",
    "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
    "        if self.combination_type == \"concat\":\n",
    "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = nodes_representation + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        return self.activation(h)\n",
    "\n",
    "    def call(self, features: tf.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        nodes_representation = self.compute_nodes_representation(features)\n",
    "        aggregated_messages = self.compute_aggregated_messages(features)\n",
    "        return self.update(nodes_representation, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM including graph convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGC(layers.Layer):\n",
    "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        output_seq_len: int,\n",
    "        graph_info: GraphInfo,\n",
    "        graph_conv_params: typing.Optional[dict] = None,\n",
    "        multi_thing: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.lstm_units = lstm_units\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.graph_info = graph_info\n",
    "        self.graph_conv_params = graph_conv_params\n",
    "        self.multi_thing = multi_thing\n",
    "        super().__init__(**kwargs)\n",
    "        # graph conv layer\n",
    "        if graph_conv_params is None:\n",
    "            graph_conv_params = {\n",
    "                \"aggregation_type\": \"mean\",\n",
    "                \"combination_type\": \"concat\",\n",
    "                \"activation\": None,\n",
    "            }\n",
    "        self.graph_conv = GraphConv(in_feat, out_feat, graph_info, **graph_conv_params)\n",
    "\n",
    "        self.lstm1 = layers.LSTM(lstm_units, return_sequences=True, activation=\"tanh\")\n",
    "        self.lstm2 = layers.LSTM(lstm_units, activation=\"tanh\")\n",
    "        # self.dense = layers.Dense(output_seq_len)\n",
    "        self.denseThick = layers.Dense(64)\n",
    "        self.denseThick2 = layers.Dense(32)\n",
    "        self.dense = layers.Dense(multi_thing)\n",
    "\n",
    "        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"in_feat\": self.in_feat,\n",
    "            \"out_feat\": self.out_feat,\n",
    "            \"lstm_units\": self.lstm_units ,\n",
    "            \"input_seq_len\": self.input_seq_len ,\n",
    "            \"output_seq_len\": self.output_seq_len ,\n",
    "            \"graph_info\": self.graph_info ,\n",
    "            \"graph_conv_params\":  self.graph_conv_params\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "\n",
    "        gcn_out = self.graph_conv(\n",
    "            inputs\n",
    "        )  # gcn_out has shape: (num_nodes, batcsh_ize, input_seq_len, out_feat)\n",
    "        print(f\"The GCN output shape  = {gcn_out}\")\n",
    "        shape = tf.shape(gcn_out)\n",
    "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3],\n",
    "        )\n",
    "        # LSTM takes only 3D tensors as input\n",
    "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
    "        # print(f\"The input shape for the LSTM is {gcn_out}\")\n",
    "        lstm1_hi = self.lstm1(gcn_out)  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        lstm2_hi = self.lstm2(lstm1_hi)  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        # print(f\"The LSTM output shape  = {lstm2_hi}\")\n",
    "        dense_1 = self.denseThick(lstm2_hi)\n",
    "        dense_2 = self.denseThick2(dense_1)\n",
    "\n",
    "        dense_output = self.dense(dense_2)\n",
    "        # dense_output has shape: (batch_size * num_nodes, multi_thing)\n",
    "        reshaped_output = tf.reshape(dense_output, (num_nodes*2, batch_size, self.multi_thing))\n",
    "        final = tf.transpose(reshaped_output, [1, 2, 0])\n",
    "        return final\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"The Dense has output shape {dense_output}\")  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n",
    "        # # output = tf.reshape(dense_output, (num_nodes, batch_size, self.output_seq_len))\n",
    "        # output = tf.reshape(dense_output, (2*num_nodes, batch_size, 1))\n",
    "        # print(f\"output shape = {output}\")\n",
    "        # final = tf.transpose(output, [1, 2, 0])  \n",
    "        # print(f\"final shape = {final}\")\n",
    "        # return final\n",
    "        # # # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feat = 9\n",
    "# batch_size = 64\n",
    "\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "out_feat = 10\n",
    "lstm_units = 64\n",
    "graph_conv_params = {\n",
    "    \"aggregation_type\": \"mean\",\n",
    "    \"combination_type\": \"concat\",\n",
    "    \"activation\": None,\n",
    "}\n",
    "\n",
    "st_gcn = LSTMGC(\n",
    "    in_feat,\n",
    "    out_feat,\n",
    "    lstm_units,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    graph,\n",
    "    graph_conv_params,\n",
    ")\n",
    "inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))#, batch_size=batch_size)\n",
    "outputs = st_gcn(inputs)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\"),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=100), tensorboard_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredFull = model.predict(test_dataset, verbose=0)#[:,0,:]\n",
    "print(ypredFull.shape)\n",
    "yactualFull = Ytest[14::,:]\n",
    "print(yactualFull.shape)\n",
    "\n",
    "\n",
    "def plotComparison(airport_index, ypredFull=ypredFull, yactualFull=yactualFull):\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, num=airport_index)\n",
    "    axs[0].plot(yactualFull[:, airport_index, 0], label=\"Actual Arrival Delay\")\n",
    "    axs[1].plot(yactualFull[:, airport_index, 1], label=\"Actual Departure Delay\")\n",
    "    axs[0].plot(ypredFull[:, airport_index, 0], label=\"Predicted Arrival Delay\")\n",
    "    axs[1].plot(ypredFull[:, airport_index, 1], label=\"Predicted Departure Delay\")\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.suptitle(f\"Comparison for airport: {airports[airport_index]}\")\n",
    "\n",
    "for airportidx in range(0,len(airports)):\n",
    "    plotComparison(airportidx)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add logbook to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir ./logs \\\n",
    "  --name \"Test for tensorboard\" \\\n",
    "  --description \"test\"\\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
