{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "from extraction.extract import generateNNdataMultiple\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = ICAOTOP10\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2019, 3, 1)\n",
    "end = datetime(2019, 3, 31)\n",
    "timeslotLength = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "            airports,\n",
    "            timeslotLength,\n",
    "            GNNFormat=True,\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "times = (list(dataDict.values())[0][\"T\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToDrop = [\"weekend\", \"winter\", \"spring\", \"summer\", \"autumn\", \"night\", \"morning\", \"afternoon\"]\n",
    "# print(dataDict[\"EGLL\"][\"X\"].shape)\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1).to_numpy()\n",
    "    Xlist.append(X)\n",
    "    \n",
    "    Y = dataDict[airport][\"Y\"].to_numpy()\n",
    "    Ylist.append(Y)\n",
    "\n",
    "\n",
    "Xlist = np.array(Xlist)\n",
    "Ylist = np.array(Ylist)\n",
    "\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# Reshape to a flat array that goes arrival then departure delay\n",
    "Yarray = np.reshape(Yarray, newshape=[len(times), len(airports)*2], order=\"F\")\n",
    "\n",
    "# print(Yarray.shape)\n",
    "# T x N x F\n",
    "\n",
    "# Normalise over the features\n",
    "Xmean, Xstd = X.mean(axis=0), X.std(axis=0)\n",
    "X = (X - Xmean) / Xstd\n",
    "print(\"X Mean Shape\", Xmean.shape)\n",
    "Ymean, Ystd = Y.mean(axis=0), Y.std(axis=0)\n",
    "Y = (Y - Ymean) / Ystd\n",
    "print(\"Y Mean Shape\", Ymean.shape)\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = 0.6, 0.1\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(train_idx, val_idx)\n",
    "\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "\n",
    "\n",
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    target_array,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "    `inputs` is a Tensor\n",
    "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "    the `input_sequence_length` past values of the timeseries for each node.\n",
    "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "    containing the `forecast_horizon`\n",
    "    future values of the timeseries for each node.\n",
    "\n",
    "    Args:\n",
    "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "            timeseries `forecast_horizon` steps ahead (only one value).\n",
    "        batch_size: Number of timeseries samples in each batch.\n",
    "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "        multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        # np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = inputs\n",
    "    target_offset = (\n",
    "       input_sequence_length\n",
    "       if multi_horizon\n",
    "       else input_sequence_length + forecast_horizon - 1) \n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        target_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,)\n",
    "    \n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "# train_dataset, val_dataset = (\n",
    " #    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
    " #    for data_array in [train_array, val_arrayylist]) \n",
    "\n",
    "# test_dataset = create_tf_dataset(\n",
    "#     Xarray, Yarray,  input_sequence_length,\n",
    "#     forecast_horizon,\n",
    "#     batch_size=test_array.shape[0],\n",
    "#     shuffle=False,\n",
    "#     multi_horizon=multi_horizon,\n",
    "# )\n",
    "\n",
    "train_dataset =  create_tf_dataset(\n",
    "    Xtrain, Ytrain,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "val_dataset =  create_tf_dataset(\n",
    "    Xval, Yval,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "test_dataset =  create_tf_dataset(\n",
    "    Xtest, Ytest,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "# Ytrain, Yval, Ytest\n",
    "# print(test_dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphInfo:\n",
    "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
    "        self.edges = edges\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "adjacency_matrix = distance_weight_adjacency(airports, threshold=100000) # getAdjacencyMatrix(airports)[10]\n",
    "print(adjacency_matrix.shape)\n",
    "node_indices, neighbor_indices = np.where(adjacency_matrix != 0)\n",
    "graph = GraphInfo(\n",
    "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
    "    num_nodes=adjacency_matrix.shape[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adjacency_matrix)\n",
    "plt.xticks(range(len(airports)), airports, fontsize=12, rotation=-90)\n",
    "plt.yticks(range(len(airports)), airports, fontsize=12, rotation=0)\n",
    "plt.show()\n",
    "# print(len(graph.edges[0]))\n",
    "# print(graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphConv(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        graph_info: GraphInfo,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        activation: typing.Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.graph_info = graph_info\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=keras.initializers.glorot_uniform()(\n",
    "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.activation = layers.Activation(activation)\n",
    "\n",
    "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
    "        aggregation_func = {\n",
    "            \"sum\": tf.math.unsorted_segment_sum,\n",
    "            \"mean\": tf.math.unsorted_segment_mean,\n",
    "            \"max\": tf.math.unsorted_segment_max,\n",
    "        }.get(self.aggregation_type)\n",
    "\n",
    "        if aggregation_func:\n",
    "            return aggregation_func(\n",
    "                neighbour_representations,\n",
    "                self.graph_info.edges[0],\n",
    "                num_segments=self.graph_info.num_nodes,\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
    "\n",
    "    def compute_nodes_representation(self, features: tf.Tensor):\n",
    "        \"\"\"Computes each node's representation.\n",
    "\n",
    "        The nodes' representations are obtained by multiplying the features tensor with\n",
    "        `self.weight`. Note that\n",
    "        `self.weight` has shape `(in_feat, out_feat)`.\n",
    "\n",
    "        Args:\n",
    "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        return tf.matmul(features, self.weight)\n",
    "\n",
    "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
    "        neighbour_representations = tf.gather(features, self.graph_info.edges[1])\n",
    "        aggregated_messages = self.aggregate(neighbour_representations)\n",
    "        return tf.matmul(aggregated_messages, self.weight)\n",
    "\n",
    "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
    "        if self.combination_type == \"concat\":\n",
    "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = nodes_representation + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        return self.activation(h)\n",
    "\n",
    "    def call(self, features: tf.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        nodes_representation = self.compute_nodes_representation(features)\n",
    "        aggregated_messages = self.compute_aggregated_messages(features)\n",
    "        return self.update(nodes_representation, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM including graph convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGC(layers.Layer):\n",
    "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        output_seq_len: int,\n",
    "        graph_info: GraphInfo,\n",
    "        graph_conv_params: typing.Optional[dict] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # graph conv layer\n",
    "        if graph_conv_params is None:\n",
    "            graph_conv_params = {\n",
    "                \"aggregation_type\": \"mean\",\n",
    "                \"combination_type\": \"concat\",\n",
    "                \"activation\": None,\n",
    "            }\n",
    "        self.graph_conv = GraphConv(in_feat, out_feat, graph_info, **graph_conv_params)\n",
    "\n",
    "        self.lstm = layers.LSTM(lstm_units, activation=\"relu\")\n",
    "        self.dense = layers.Dense(output_seq_len)\n",
    "\n",
    "        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "\n",
    "        gcn_out = self.graph_conv(\n",
    "            inputs\n",
    "        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n",
    "        print(f\"The GCN output shape  = {gcn_out}\")\n",
    "        shape = tf.shape(gcn_out)\n",
    "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3],\n",
    "        )\n",
    "\n",
    "        # LSTM takes only 3D tensors as input\n",
    "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
    "        print(f\"The input shape for the LSTM is {gcn_out}\")\n",
    "        lstm_out = self.lstm(gcn_out)  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        print(f\"The LSTM output shape  = {lstm_out}\")\n",
    "        dense_output = self.dense(\n",
    "            lstm_out\n",
    "        )\n",
    "        1\n",
    "        print(f\"The Dense has output shape {dense_output}\")  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n",
    "        # output = tf.reshape(dense_output, (num_nodes, batch_size, self.output_seq_len))\n",
    "        output = tf.reshape(dense_output, (2*num_nodes, batch_size, 1))\n",
    "        return tf.transpose(output, [1, 2, 0])  \n",
    "        # # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feat = 9\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "out_feat = 10\n",
    "lstm_units = 64\n",
    "graph_conv_params = {\n",
    "    \"aggregation_type\": \"mean\",\n",
    "    \"combination_type\": \"concat\",\n",
    "    \"activation\": None,\n",
    "}\n",
    "\n",
    "st_gcn = LSTMGC(\n",
    "    in_feat,\n",
    "    out_feat,\n",
    "    lstm_units,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    graph,\n",
    "    graph_conv_params,\n",
    ")\n",
    "inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n",
    "outputs = st_gcn(inputs)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "model.summary()\n",
    "model.fit(\n",
    "    test_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
