{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "\n",
    "from extraction.extract import generateNNdataMultiple\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = ICAOTOP10\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2019, 3, 1)\n",
    "end = datetime(2019, 3, 31)\n",
    "timeslotLength = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 62.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "            airports,\n",
    "            timeslotLength,\n",
    "            GNNFormat=True,\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "times = (list(dataDict.values())[0][\"T\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Mean Shape (9,)\n",
      "Y Mean Shape (2,)\n",
      "T x N x F:  Xarray = (720, 10, 9) | Yarray = (720, 20)\n"
     ]
    }
   ],
   "source": [
    "columnsToDrop = [\"weekend\", \"winter\", \"spring\", \"summer\", \"autumn\", \"night\", \"morning\", \"afternoon\"]\n",
    "# print(dataDict[\"EGLL\"][\"X\"].shape)\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1).to_numpy()\n",
    "    Xlist.append(X)\n",
    "    \n",
    "    Y = dataDict[airport][\"Y\"].to_numpy()\n",
    "    Ylist.append(Y)\n",
    "\n",
    "\n",
    "Xlist = np.array(Xlist)\n",
    "Ylist = np.array(Ylist)\n",
    "\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# Reshape to a flat array that goes arrival then departure delay\n",
    "Yarray = np.reshape(Yarray, newshape=[len(times), len(airports)*2], order=\"F\")\n",
    "\n",
    "# print(Yarray.shape)\n",
    "# T x N x F\n",
    "\n",
    "# Normalise over the features\n",
    "Xmean, Xstd = X.mean(axis=0), X.std(axis=0)\n",
    "X = (X - Xmean) / Xstd\n",
    "print(\"X Mean Shape\", Xmean.shape)\n",
    "Ymean, Ystd = Y.mean(axis=0), Y.std(axis=0)\n",
    "Y = (Y - Ymean) / Ystd\n",
    "print(\"Y Mean Shape\", Ymean.shape)\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 503\n"
     ]
    }
   ],
   "source": [
    "train_split, val_split = 0.6, 0.1\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(train_idx, val_idx)\n",
    "\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "\n",
    "\n",
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    target_array,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "    `inputs` is a Tensor\n",
    "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "    the `input_sequence_length` past values of the timeseries for each node.\n",
    "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "    containing the `forecast_horizon`\n",
    "    future values of the timeseries for each node.\n",
    "\n",
    "    Args:\n",
    "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "            timeseries `forecast_horizon` steps ahead (only one value).\n",
    "        batch_size: Number of timeseries samples in each batch.\n",
    "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "        multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        # np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    dataset = inputs\n",
    "    target_offset = (\n",
    "       input_sequence_length\n",
    "       if multi_horizon\n",
    "       else input_sequence_length + forecast_horizon - 1) \n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        target_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,)\n",
    "    \n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "# train_dataset, val_dataset = (\n",
    " #    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
    " #    for data_array in [train_array, val_arrayylist]) \n",
    "\n",
    "# test_dataset = create_tf_dataset(\n",
    "#     Xarray, Yarray,  input_sequence_length,\n",
    "#     forecast_horizon,\n",
    "#     batch_size=test_array.shape[0],\n",
    "#     shuffle=False,\n",
    "#     multi_horizon=multi_horizon,\n",
    "# )\n",
    "\n",
    "train_dataset =  create_tf_dataset(\n",
    "    Xtrain, Ytrain,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "val_dataset =  create_tf_dataset(\n",
    "    Xval, Yval,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "test_dataset =  create_tf_dataset(\n",
    "    Xtest, Ytest,  input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "# Ytrain, Yval, Ytest\n",
    "# print(test_dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "class GraphInfo:\n",
    "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
    "        self.edges = edges\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "adjacency_matrix = distance_weight_adjacency(airports, threshold=100000) # getAdjacencyMatrix(airports)[10]\n",
    "print(adjacency_matrix.shape)\n",
    "node_indices, neighbor_indices = np.where(adjacency_matrix != 0)\n",
    "graph = GraphInfo(\n",
    "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
    "    num_nodes=adjacency_matrix.shape[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adjacency_matrix)\n",
    "plt.xticks(range(len(airports)), airports, fontsize=12, rotation=-90)\n",
    "plt.yticks(range(len(airports)), airports, fontsize=12, rotation=0)\n",
    "plt.show()\n",
    "# print(len(graph.edges[0]))\n",
    "# print(graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphConv(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        graph_info: GraphInfo,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        activation: typing.Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.graph_info = graph_info\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=keras.initializers.glorot_uniform()(\n",
    "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.activation = layers.Activation(activation)\n",
    "\n",
    "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
    "        aggregation_func = {\n",
    "            \"sum\": tf.math.unsorted_segment_sum,\n",
    "            \"mean\": tf.math.unsorted_segment_mean,\n",
    "            \"max\": tf.math.unsorted_segment_max,\n",
    "        }.get(self.aggregation_type)\n",
    "\n",
    "        if aggregation_func:\n",
    "            return aggregation_func(\n",
    "                neighbour_representations,\n",
    "                self.graph_info.edges[0],\n",
    "                num_segments=self.graph_info.num_nodes,\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
    "\n",
    "    def compute_nodes_representation(self, features: tf.Tensor):\n",
    "        \"\"\"Computes each node's representation.\n",
    "\n",
    "        The nodes' representations are obtained by multiplying the features tensor with\n",
    "        `self.weight`. Note that\n",
    "        `self.weight` has shape `(in_feat, out_feat)`.\n",
    "\n",
    "        Args:\n",
    "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        return tf.matmul(features, self.weight)\n",
    "\n",
    "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
    "        neighbour_representations = tf.gather(features, self.graph_info.edges[1])\n",
    "        aggregated_messages = self.aggregate(neighbour_representations)\n",
    "        return tf.matmul(aggregated_messages, self.weight)\n",
    "\n",
    "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
    "        if self.combination_type == \"concat\":\n",
    "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = nodes_representation + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        return self.activation(h)\n",
    "\n",
    "    def call(self, features: tf.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        nodes_representation = self.compute_nodes_representation(features)\n",
    "        aggregated_messages = self.compute_aggregated_messages(features)\n",
    "        return self.update(nodes_representation, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM including graph convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGC(layers.Layer):\n",
    "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        output_seq_len: int,\n",
    "        graph_info: GraphInfo,\n",
    "        graph_conv_params: typing.Optional[dict] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # graph conv layer\n",
    "        if graph_conv_params is None:\n",
    "            graph_conv_params = {\n",
    "                \"aggregation_type\": \"mean\",\n",
    "                \"combination_type\": \"concat\",\n",
    "                \"activation\": None,\n",
    "            }\n",
    "        self.graph_conv = GraphConv(in_feat, out_feat, graph_info, **graph_conv_params)\n",
    "\n",
    "        self.lstm1 = layers.LSTM(lstm_units, return_sequences=True, activation=\"relu\")\n",
    "        self.lstm2 = layers.LSTM(lstm_units, activation=\"relu\")\n",
    "        # self.dense = layers.Dense(output_seq_len)\n",
    "        self.dense = layers.Dense(2)\n",
    "        self.denseThick = layers.Dense(32)\n",
    "\n",
    "        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "\n",
    "        gcn_out = self.graph_conv(\n",
    "            inputs\n",
    "        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n",
    "        # print(f\"The GCN output shape  = {gcn_out}\")\n",
    "        shape = tf.shape(gcn_out)\n",
    "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3],\n",
    "        )\n",
    "\n",
    "        # LSTM takes only 3D tensors as input\n",
    "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
    "        # print(f\"The input shape for the LSTM is {gcn_out}\")\n",
    "        # lstm1_hi = self.lstm1(gcn_out)  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        lstm2_hi = self.lstm2(gcn_out)  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        print(f\"The LSTM output shape  = {lstm2_hi}\")\n",
    "        dense_1 = self.denseThick(lstm2_hi)\n",
    "        dense_output = self.dense(dense_1)\n",
    "        # print(f\"The Dense has output shape {dense_output}\")  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n",
    "        # output = tf.reshape(dense_output, (num_nodes, batch_size, self.output_seq_len))\n",
    "        output = tf.reshape(dense_output, (2*num_nodes, batch_size, 1))\n",
    "        return tf.transpose(output, [1, 2, 0])  \n",
    "        # # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "The LSTM output shape  = Tensor(\"lstmgc_27/lstm_38/strided_slice_3:0\", shape=(None, 64), dtype=float32)\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 12, 10, 9)]       0         \n",
      "                                                                 \n",
      " lstmgc_27 (LSTMGC)          (None, 1, 20)             23996     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,996\n",
      "Trainable params: 23,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "The LSTM output shape  = Tensor(\"model_25/lstmgc_27/lstm_38/strided_slice_3:0\", shape=(None, 64), dtype=float32)\n",
      "The LSTM output shape  = Tensor(\"model_25/lstmgc_27/lstm_38/strided_slice_3:0\", shape=(None, 64), dtype=float32)\n",
      "4/4 [==============================] - ETA: 0s - loss: 43.2890 - acc: 0.0443The LSTM output shape  = Tensor(\"model_25/lstmgc_27/lstm_38/strided_slice_3:0\", shape=(None, 64), dtype=float32)\n",
      "4/4 [==============================] - 2s 188ms/step - loss: 43.2890 - acc: 0.0443 - val_loss: 33.1172 - val_acc: 0.0345\n",
      "Epoch 2/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 30.8887 - acc: 0.0246 - val_loss: 25.4031 - val_acc: 0.0345\n",
      "Epoch 3/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 23.7197 - acc: 0.0394 - val_loss: 20.0631 - val_acc: 0.0394\n",
      "Epoch 4/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 19.1084 - acc: 0.0345 - val_loss: 16.6781 - val_acc: 0.0394\n",
      "Epoch 5/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 15.8883 - acc: 0.0493 - val_loss: 13.8964 - val_acc: 0.0788\n",
      "Epoch 6/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 13.3735 - acc: 0.0739 - val_loss: 12.0320 - val_acc: 0.0739\n",
      "Epoch 7/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 11.6288 - acc: 0.0788 - val_loss: 10.6529 - val_acc: 0.0739\n",
      "Epoch 8/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 10.3738 - acc: 0.0542 - val_loss: 9.7082 - val_acc: 0.0443\n",
      "Epoch 9/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 9.5083 - acc: 0.0591 - val_loss: 8.9704 - val_acc: 0.0591\n",
      "Epoch 10/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 8.7780 - acc: 0.0542 - val_loss: 8.3008 - val_acc: 0.0493\n",
      "Epoch 11/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 8.1732 - acc: 0.0542 - val_loss: 7.8011 - val_acc: 0.0542\n",
      "Epoch 12/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 7.7079 - acc: 0.0542 - val_loss: 7.4683 - val_acc: 0.0591\n",
      "Epoch 13/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 7.4099 - acc: 0.0493 - val_loss: 7.1973 - val_acc: 0.0591\n",
      "Epoch 14/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 7.1354 - acc: 0.0591 - val_loss: 6.9654 - val_acc: 0.0591\n",
      "Epoch 15/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 6.9097 - acc: 0.0591 - val_loss: 6.7571 - val_acc: 0.0542\n",
      "Epoch 16/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 6.7100 - acc: 0.0591 - val_loss: 6.5673 - val_acc: 0.0640\n",
      "Epoch 17/400\n",
      "4/4 [==============================] - 0s 124ms/step - loss: 6.5203 - acc: 0.0591 - val_loss: 6.3882 - val_acc: 0.0690\n",
      "Epoch 18/400\n",
      "4/4 [==============================] - ETA: 0s - loss: 6.3558 - acc: 0.073 - 0s 97ms/step - loss: 6.3558 - acc: 0.0739 - val_loss: 6.2504 - val_acc: 0.0690\n",
      "Epoch 19/400\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 6.2225 - acc: 0.0690 - val_loss: 6.1218 - val_acc: 0.0739\n",
      "Epoch 20/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 6.0973 - acc: 0.0739 - val_loss: 6.0134 - val_acc: 0.0739\n",
      "Epoch 21/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 5.9930 - acc: 0.0739 - val_loss: 5.9232 - val_acc: 0.0739\n",
      "Epoch 22/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 5.9063 - acc: 0.0739 - val_loss: 5.8410 - val_acc: 0.0690\n",
      "Epoch 23/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 5.8253 - acc: 0.0690 - val_loss: 5.7704 - val_acc: 0.0640\n",
      "Epoch 24/400\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 5.7550 - acc: 0.0640 - val_loss: 5.7005 - val_acc: 0.0640\n",
      "Epoch 25/400\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 5.6878 - acc: 0.0640 - val_loss: 5.6369 - val_acc: 0.0690\n",
      "Epoch 26/400\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 5.6248 - acc: 0.0690 - val_loss: 5.5738 - val_acc: 0.0739\n",
      "Epoch 27/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 5.5639 - acc: 0.0739 - val_loss: 5.5130 - val_acc: 0.0690\n",
      "Epoch 28/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 5.5015 - acc: 0.0690 - val_loss: 5.4580 - val_acc: 0.0788\n",
      "Epoch 29/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 5.4487 - acc: 0.0788 - val_loss: 5.4028 - val_acc: 0.0788\n",
      "Epoch 30/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 5.3973 - acc: 0.0788 - val_loss: 5.3632 - val_acc: 0.0837\n",
      "Epoch 31/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 5.3598 - acc: 0.0837 - val_loss: 5.3241 - val_acc: 0.0837\n",
      "Epoch 32/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 5.3172 - acc: 0.0887 - val_loss: 5.2846 - val_acc: 0.0788\n",
      "Epoch 33/400\n",
      "4/4 [==============================] - ETA: 0s - loss: 5.2814 - acc: 0.083 - 0s 87ms/step - loss: 5.2814 - acc: 0.0837 - val_loss: 5.2468 - val_acc: 0.0887\n",
      "Epoch 34/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 5.2438 - acc: 0.0887 - val_loss: 5.2159 - val_acc: 0.0936\n",
      "Epoch 35/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 5.2159 - acc: 0.0936 - val_loss: 5.1929 - val_acc: 0.0936\n",
      "Epoch 36/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 5.1924 - acc: 0.0936 - val_loss: 5.1580 - val_acc: 0.0985\n",
      "Epoch 37/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 5.1572 - acc: 0.0936 - val_loss: 5.1256 - val_acc: 0.0985\n",
      "Epoch 38/400\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 5.1230 - acc: 0.1084 - val_loss: 5.0995 - val_acc: 0.1084\n",
      "Epoch 39/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 5.0987 - acc: 0.1084 - val_loss: 5.0710 - val_acc: 0.1133\n",
      "Epoch 40/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 5.0668 - acc: 0.1133 - val_loss: 5.0420 - val_acc: 0.1182\n",
      "Epoch 41/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 5.0478 - acc: 0.1232 - val_loss: 5.0172 - val_acc: 0.1182\n",
      "Epoch 42/400\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 5.0162 - acc: 0.1182 - val_loss: 4.9789 - val_acc: 0.1281\n",
      "Epoch 43/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.9797 - acc: 0.1281 - val_loss: 4.9545 - val_acc: 0.1330\n",
      "Epoch 44/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 4.9548 - acc: 0.1281 - val_loss: 4.9310 - val_acc: 0.1281\n",
      "Epoch 45/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 4.9380 - acc: 0.1281 - val_loss: 4.9045 - val_acc: 0.1478\n",
      "Epoch 46/400\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 4.9034 - acc: 0.1527 - val_loss: 4.8815 - val_acc: 0.1429\n",
      "Epoch 47/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 4.8843 - acc: 0.1379 - val_loss: 4.8565 - val_acc: 0.1379\n",
      "Epoch 48/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 4.8588 - acc: 0.1429 - val_loss: 4.8365 - val_acc: 0.1429\n",
      "Epoch 49/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.8419 - acc: 0.1379 - val_loss: 4.8049 - val_acc: 0.1478\n",
      "Epoch 50/400\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 4.8080 - acc: 0.1429 - val_loss: 4.7908 - val_acc: 0.1429\n",
      "Epoch 51/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.7942 - acc: 0.1429 - val_loss: 4.7619 - val_acc: 0.1478\n",
      "Epoch 52/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 4.7661 - acc: 0.1429 - val_loss: 4.7449 - val_acc: 0.1429\n",
      "Epoch 53/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 4.7528 - acc: 0.1379 - val_loss: 4.7218 - val_acc: 0.1527\n",
      "Epoch 54/400\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 4.7310 - acc: 0.1478 - val_loss: 4.6950 - val_acc: 0.1330\n",
      "Epoch 55/400\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 4.7021 - acc: 0.1379 - val_loss: 4.6684 - val_acc: 0.1429\n",
      "Epoch 56/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 4.6704 - acc: 0.1429 - val_loss: 4.6532 - val_acc: 0.1478\n",
      "Epoch 57/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 4.6531 - acc: 0.1478 - val_loss: 4.6333 - val_acc: 0.1379\n",
      "Epoch 58/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 4.6453 - acc: 0.1379 - val_loss: 4.6147 - val_acc: 0.1379\n",
      "Epoch 59/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 4.6155 - acc: 0.1379 - val_loss: 4.5923 - val_acc: 0.1478\n",
      "Epoch 60/400\n",
      "4/4 [==============================] - 0s 124ms/step - loss: 4.5990 - acc: 0.1429 - val_loss: 4.5715 - val_acc: 0.1527\n",
      "Epoch 61/400\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 4.5751 - acc: 0.1478 - val_loss: 4.5603 - val_acc: 0.1379\n",
      "Epoch 62/400\n",
      "4/4 [==============================] - 0s 113ms/step - loss: 4.5771 - acc: 0.1429 - val_loss: 4.5422 - val_acc: 0.1527\n",
      "Epoch 63/400\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 4.5506 - acc: 0.1576 - val_loss: 4.5139 - val_acc: 0.1576\n",
      "Epoch 64/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 4.5247 - acc: 0.1527 - val_loss: 4.4984 - val_acc: 0.1527\n",
      "Epoch 65/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 4.5051 - acc: 0.1527 - val_loss: 4.4778 - val_acc: 0.1626\n",
      "Epoch 66/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 4.4817 - acc: 0.1626 - val_loss: 4.4754 - val_acc: 0.1823\n",
      "Epoch 67/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 4.4814 - acc: 0.1773 - val_loss: 4.4638 - val_acc: 0.1576\n",
      "Epoch 68/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.4776 - acc: 0.1626 - val_loss: 4.4402 - val_acc: 0.1724\n",
      "Epoch 69/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 4.4603 - acc: 0.1675 - val_loss: 4.4159 - val_acc: 0.1675\n",
      "Epoch 70/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.4289 - acc: 0.1626 - val_loss: 4.4056 - val_acc: 0.1823\n",
      "Epoch 71/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 4.4172 - acc: 0.1872 - val_loss: 4.3918 - val_acc: 0.1724\n",
      "Epoch 72/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 4.4108 - acc: 0.1626 - val_loss: 4.3777 - val_acc: 0.1921\n",
      "Epoch 73/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.3906 - acc: 0.1921 - val_loss: 4.3666 - val_acc: 0.1823\n",
      "Epoch 74/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 4.3731 - acc: 0.1872 - val_loss: 4.3448 - val_acc: 0.1872\n",
      "Epoch 75/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.3605 - acc: 0.1823 - val_loss: 4.3281 - val_acc: 0.1724\n",
      "Epoch 76/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.3400 - acc: 0.1823 - val_loss: 4.3201 - val_acc: 0.2069\n",
      "Epoch 77/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.3360 - acc: 0.2069 - val_loss: 4.2979 - val_acc: 0.2118\n",
      "Epoch 78/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 4.3196 - acc: 0.2118 - val_loss: 4.2981 - val_acc: 0.2020\n",
      "Epoch 79/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.3262 - acc: 0.2020 - val_loss: 4.2885 - val_acc: 0.1970\n",
      "Epoch 80/400\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 4.3164 - acc: 0.2020 - val_loss: 4.2698 - val_acc: 0.2020\n",
      "Epoch 81/400\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 4.2856 - acc: 0.1921 - val_loss: 4.2460 - val_acc: 0.2118\n",
      "Epoch 82/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 4.2685 - acc: 0.2069 - val_loss: 4.2358 - val_acc: 0.2118\n",
      "Epoch 83/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 4.2487 - acc: 0.2118 - val_loss: 4.2199 - val_acc: 0.2118\n",
      "Epoch 84/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.2346 - acc: 0.1970 - val_loss: 4.2034 - val_acc: 0.2217\n",
      "Epoch 85/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 4.2266 - acc: 0.2069 - val_loss: 4.2004 - val_acc: 0.2167\n",
      "Epoch 86/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 4.2255 - acc: 0.2118 - val_loss: 4.1853 - val_acc: 0.2266\n",
      "Epoch 87/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 4.1987 - acc: 0.2266 - val_loss: 4.1762 - val_acc: 0.2118\n",
      "Epoch 88/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.1921 - acc: 0.2069 - val_loss: 4.1665 - val_acc: 0.2069\n",
      "Epoch 89/400\n",
      "4/4 [==============================] - ETA: 0s - loss: 4.1827 - acc: 0.197 - 0s 84ms/step - loss: 4.1827 - acc: 0.1970 - val_loss: 4.1360 - val_acc: 0.2217\n",
      "Epoch 90/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 4.1444 - acc: 0.2167 - val_loss: 4.1265 - val_acc: 0.2315\n",
      "Epoch 91/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.1411 - acc: 0.2365 - val_loss: 4.1215 - val_acc: 0.2217\n",
      "Epoch 92/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.1330 - acc: 0.2217 - val_loss: 4.1107 - val_acc: 0.2118\n",
      "Epoch 93/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.1361 - acc: 0.2217 - val_loss: 4.0995 - val_acc: 0.2217\n",
      "Epoch 94/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.1045 - acc: 0.2315 - val_loss: 4.0857 - val_acc: 0.2266\n",
      "Epoch 95/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.0927 - acc: 0.2217 - val_loss: 4.0734 - val_acc: 0.2315\n",
      "Epoch 96/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.0904 - acc: 0.2315 - val_loss: 4.0552 - val_acc: 0.2266\n",
      "Epoch 97/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.0700 - acc: 0.2266 - val_loss: 4.0498 - val_acc: 0.2266\n",
      "Epoch 98/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 4.0612 - acc: 0.2167 - val_loss: 4.0333 - val_acc: 0.2365\n",
      "Epoch 99/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 4.0480 - acc: 0.2315 - val_loss: 4.0258 - val_acc: 0.2217\n",
      "Epoch 100/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 4.0421 - acc: 0.2365 - val_loss: 4.0202 - val_acc: 0.2266\n",
      "Epoch 101/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 4.0430 - acc: 0.2266 - val_loss: 4.0152 - val_acc: 0.2315\n",
      "Epoch 102/400\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 4.0359 - acc: 0.2266 - val_loss: 3.9934 - val_acc: 0.2365\n",
      "Epoch 103/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 4.0140 - acc: 0.2365 - val_loss: 3.9912 - val_acc: 0.2266\n",
      "Epoch 104/400\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 4.0034 - acc: 0.2266 - val_loss: 3.9810 - val_acc: 0.2414\n",
      "Epoch 105/400\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 3.9991 - acc: 0.2365 - val_loss: 3.9802 - val_acc: 0.2217\n",
      "Epoch 106/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.9955 - acc: 0.2167 - val_loss: 3.9829 - val_acc: 0.2365\n",
      "Epoch 107/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.9928 - acc: 0.2414 - val_loss: 3.9840 - val_acc: 0.2414\n",
      "Epoch 108/400\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 3.9926 - acc: 0.2315 - val_loss: 3.9574 - val_acc: 0.2266\n",
      "Epoch 109/400\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 3.9703 - acc: 0.2365 - val_loss: 3.9338 - val_acc: 0.2463\n",
      "Epoch 110/400\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 3.9494 - acc: 0.2463 - val_loss: 3.9253 - val_acc: 0.2463\n",
      "Epoch 111/400\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 3.9499 - acc: 0.2414 - val_loss: 3.9222 - val_acc: 0.2414\n",
      "Epoch 112/400\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 3.9391 - acc: 0.2414 - val_loss: 3.8973 - val_acc: 0.2414\n",
      "Epoch 113/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 3.9329 - acc: 0.2217 - val_loss: 3.9063 - val_acc: 0.2463\n",
      "Epoch 114/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.9148 - acc: 0.2709 - val_loss: 3.9203 - val_acc: 0.2512\n",
      "Epoch 115/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.9208 - acc: 0.2414 - val_loss: 3.9280 - val_acc: 0.2365\n",
      "Epoch 116/400\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 3.9424 - acc: 0.2562 - val_loss: 3.8723 - val_acc: 0.2562\n",
      "Epoch 117/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 3.8977 - acc: 0.2562 - val_loss: 3.8557 - val_acc: 0.2562\n",
      "Epoch 118/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.8828 - acc: 0.2463 - val_loss: 3.8623 - val_acc: 0.2611\n",
      "Epoch 119/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.8918 - acc: 0.2660 - val_loss: 3.8779 - val_acc: 0.2562\n",
      "Epoch 120/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 3.9122 - acc: 0.2414 - val_loss: 3.8712 - val_acc: 0.2266\n",
      "Epoch 121/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.8812 - acc: 0.2365 - val_loss: 3.8702 - val_acc: 0.2512\n",
      "Epoch 122/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.9081 - acc: 0.2611 - val_loss: 3.8703 - val_acc: 0.2463\n",
      "Epoch 123/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.9027 - acc: 0.2266 - val_loss: 3.9012 - val_acc: 0.2167\n",
      "Epoch 124/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.9203 - acc: 0.2365 - val_loss: 3.8632 - val_acc: 0.2660\n",
      "Epoch 125/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.8870 - acc: 0.2709 - val_loss: 3.8471 - val_acc: 0.2512\n",
      "Epoch 126/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 3.8465 - acc: 0.2414 - val_loss: 3.8017 - val_acc: 0.2857\n",
      "Epoch 127/400\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 3.8126 - acc: 0.2857 - val_loss: 3.7856 - val_acc: 0.2808\n",
      "Epoch 128/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 3.8005 - acc: 0.2660 - val_loss: 3.7630 - val_acc: 0.2512\n",
      "Epoch 129/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 3.7731 - acc: 0.2660 - val_loss: 3.7529 - val_acc: 0.2611\n",
      "Epoch 130/400\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 3.7713 - acc: 0.2660 - val_loss: 3.7703 - val_acc: 0.2709\n",
      "Epoch 131/400\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 3.7862 - acc: 0.2759 - val_loss: 3.7776 - val_acc: 0.2759\n",
      "Epoch 132/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 3.8013 - acc: 0.2709 - val_loss: 3.7537 - val_acc: 0.2808\n",
      "Epoch 133/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 3.7893 - acc: 0.2660 - val_loss: 3.7391 - val_acc: 0.2808\n",
      "Epoch 134/400\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 3.7653 - acc: 0.2808 - val_loss: 3.7380 - val_acc: 0.2808\n",
      "Epoch 135/400\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 3.7450 - acc: 0.2808 - val_loss: 3.7118 - val_acc: 0.2906\n",
      "Epoch 136/400\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 3.7222 - acc: 0.2956 - val_loss: 3.6826 - val_acc: 0.2857\n",
      "Epoch 137/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 3.7141 - acc: 0.2808 - val_loss: 3.6902 - val_acc: 0.2857\n",
      "Epoch 138/400\n",
      "4/4 [==============================] - 0s 129ms/step - loss: 3.7181 - acc: 0.2808 - val_loss: 3.7077 - val_acc: 0.2906\n",
      "Epoch 139/400\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 3.7176 - acc: 0.2906 - val_loss: 3.7849 - val_acc: 0.2857\n",
      "Epoch 140/400\n",
      "4/4 [==============================] - 0s 129ms/step - loss: 3.7905 - acc: 0.2759 - val_loss: 3.7005 - val_acc: 0.2956\n",
      "Epoch 141/400\n",
      "4/4 [==============================] - 0s 130ms/step - loss: 3.7296 - acc: 0.2857 - val_loss: 3.7381 - val_acc: 0.2759\n",
      "Epoch 142/400\n",
      "4/4 [==============================] - 0s 119ms/step - loss: 3.7122 - acc: 0.2808 - val_loss: 3.6651 - val_acc: 0.2857\n",
      "Epoch 143/400\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 3.6604 - acc: 0.2956 - val_loss: 3.6292 - val_acc: 0.2956\n",
      "Epoch 144/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 3.6476 - acc: 0.2808 - val_loss: 3.6369 - val_acc: 0.2808\n",
      "Epoch 145/400\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 3.6641 - acc: 0.2808 - val_loss: 3.6290 - val_acc: 0.2857\n",
      "Epoch 146/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.6687 - acc: 0.2808 - val_loss: 3.6492 - val_acc: 0.2956\n",
      "Epoch 147/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 3.6597 - acc: 0.3005 - val_loss: 3.6249 - val_acc: 0.2857\n",
      "Epoch 148/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.6483 - acc: 0.2808 - val_loss: 3.6073 - val_acc: 0.2956\n",
      "Epoch 149/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 3.6168 - acc: 0.2857 - val_loss: 3.5956 - val_acc: 0.3005\n",
      "Epoch 150/400\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 3.6242 - acc: 0.2956 - val_loss: 3.5806 - val_acc: 0.2956\n",
      "Epoch 151/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.6086 - acc: 0.3005 - val_loss: 3.6279 - val_acc: 0.2857\n",
      "Epoch 152/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.6425 - acc: 0.2956 - val_loss: 3.6177 - val_acc: 0.3054\n",
      "Epoch 153/400\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 3.6206 - acc: 0.3054 - val_loss: 3.5933 - val_acc: 0.2956\n",
      "Epoch 154/400\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 3.6029 - acc: 0.2906 - val_loss: 3.5417 - val_acc: 0.3005\n",
      "Epoch 155/400\n",
      "4/4 [==============================] - 0s 127ms/step - loss: 3.5810 - acc: 0.3054 - val_loss: 3.5607 - val_acc: 0.3103\n",
      "Epoch 156/400\n",
      "4/4 [==============================] - 0s 121ms/step - loss: 3.5843 - acc: 0.3103 - val_loss: 3.5786 - val_acc: 0.3005\n",
      "Epoch 157/400\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 3.6049 - acc: 0.3005 - val_loss: 3.5759 - val_acc: 0.3103\n",
      "Epoch 158/400\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 3.5799 - acc: 0.3103 - val_loss: 3.5963 - val_acc: 0.3103\n",
      "Epoch 159/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 3.6039 - acc: 0.3054 - val_loss: 3.5173 - val_acc: 0.3153\n",
      "Epoch 160/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 3.5443 - acc: 0.3153 - val_loss: 3.5207 - val_acc: 0.3153\n",
      "Epoch 161/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.5436 - acc: 0.3054 - val_loss: 3.5594 - val_acc: 0.3202\n",
      "Epoch 162/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.5686 - acc: 0.3054 - val_loss: 3.5448 - val_acc: 0.3251\n",
      "Epoch 163/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.5599 - acc: 0.3300 - val_loss: 3.5517 - val_acc: 0.3054\n",
      "Epoch 164/400\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 3.5616 - acc: 0.2808 - val_loss: 3.4926 - val_acc: 0.3202\n",
      "Epoch 165/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.5338 - acc: 0.3202 - val_loss: 3.4949 - val_acc: 0.3202\n",
      "Epoch 166/400\n",
      "4/4 [==============================] - 0s 110ms/step - loss: 3.5260 - acc: 0.3153 - val_loss: 3.5379 - val_acc: 0.3202\n",
      "Epoch 167/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 3.5469 - acc: 0.3251 - val_loss: 3.4885 - val_acc: 0.3251\n",
      "Epoch 168/400\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 3.4946 - acc: 0.3251 - val_loss: 3.4552 - val_acc: 0.3300\n",
      "Epoch 169/400\n",
      "4/4 [==============================] - 0s 128ms/step - loss: 3.4710 - acc: 0.3103 - val_loss: 3.4364 - val_acc: 0.3300\n",
      "Epoch 170/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.4713 - acc: 0.3153 - val_loss: 3.4678 - val_acc: 0.3350\n",
      "Epoch 171/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.5031 - acc: 0.3251 - val_loss: 3.5242 - val_acc: 0.3350\n",
      "Epoch 172/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 3.5361 - acc: 0.3251 - val_loss: 3.4778 - val_acc: 0.3399\n",
      "Epoch 173/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.4936 - acc: 0.3300 - val_loss: 3.4602 - val_acc: 0.3300\n",
      "Epoch 174/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 3.4747 - acc: 0.3251 - val_loss: 3.4357 - val_acc: 0.3251\n",
      "Epoch 175/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 3.4627 - acc: 0.3498 - val_loss: 3.4297 - val_acc: 0.3399\n",
      "Epoch 176/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.4700 - acc: 0.3399 - val_loss: 3.4647 - val_acc: 0.3448\n",
      "Epoch 177/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.4916 - acc: 0.3448 - val_loss: 3.4492 - val_acc: 0.3350\n",
      "Epoch 178/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.4801 - acc: 0.3300 - val_loss: 3.5024 - val_acc: 0.3498\n",
      "Epoch 179/400\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 3.5149 - acc: 0.3300 - val_loss: 3.4408 - val_acc: 0.3300\n",
      "Epoch 180/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 3.4642 - acc: 0.3300 - val_loss: 3.4381 - val_acc: 0.3596\n",
      "Epoch 181/400\n",
      "4/4 [==============================] - 0s 137ms/step - loss: 3.4593 - acc: 0.3448 - val_loss: 3.3980 - val_acc: 0.3448\n",
      "Epoch 182/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 3.4540 - acc: 0.3498 - val_loss: 3.3923 - val_acc: 0.3448\n",
      "Epoch 183/400\n",
      "4/4 [==============================] - 0s 120ms/step - loss: 3.4217 - acc: 0.3547 - val_loss: 3.4397 - val_acc: 0.3744\n",
      "Epoch 184/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.4557 - acc: 0.3645 - val_loss: 3.3984 - val_acc: 0.3350\n",
      "Epoch 185/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.4032 - acc: 0.3300 - val_loss: 3.4000 - val_acc: 0.3448\n",
      "Epoch 186/400\n",
      "4/4 [==============================] - 0s 119ms/step - loss: 3.4019 - acc: 0.3350 - val_loss: 3.3568 - val_acc: 0.3399\n",
      "Epoch 187/400\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 3.3764 - acc: 0.3498 - val_loss: 3.3465 - val_acc: 0.3251\n",
      "Epoch 188/400\n",
      "4/4 [==============================] - 0s 132ms/step - loss: 3.4018 - acc: 0.3300 - val_loss: 3.4035 - val_acc: 0.3645\n",
      "Epoch 189/400\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 3.4416 - acc: 0.3793 - val_loss: 3.4464 - val_acc: 0.3695\n",
      "Epoch 190/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.4910 - acc: 0.3744 - val_loss: 3.5014 - val_acc: 0.3744\n",
      "Epoch 191/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.5098 - acc: 0.3596 - val_loss: 3.4589 - val_acc: 0.3596\n",
      "Epoch 192/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.4587 - acc: 0.3498 - val_loss: 3.4386 - val_acc: 0.3744\n",
      "Epoch 193/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.4191 - acc: 0.3645 - val_loss: 3.3636 - val_acc: 0.3498\n",
      "Epoch 194/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.3689 - acc: 0.3645 - val_loss: 3.3524 - val_acc: 0.3695\n",
      "Epoch 195/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.3795 - acc: 0.3695 - val_loss: 3.3715 - val_acc: 0.3596\n",
      "Epoch 196/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.4076 - acc: 0.3695 - val_loss: 3.3677 - val_acc: 0.3596\n",
      "Epoch 197/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.4389 - acc: 0.3498 - val_loss: 3.4078 - val_acc: 0.3695\n",
      "Epoch 198/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.4627 - acc: 0.3596 - val_loss: 3.3586 - val_acc: 0.3695\n",
      "Epoch 199/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.3887 - acc: 0.3695 - val_loss: 3.3599 - val_acc: 0.3793\n",
      "Epoch 200/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.3595 - acc: 0.3842 - val_loss: 3.3094 - val_acc: 0.3793\n",
      "Epoch 201/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.3071 - acc: 0.3645 - val_loss: 3.3142 - val_acc: 0.3695\n",
      "Epoch 202/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.3197 - acc: 0.3596 - val_loss: 3.3017 - val_acc: 0.3744\n",
      "Epoch 203/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2962 - acc: 0.3941 - val_loss: 3.2758 - val_acc: 0.3941\n",
      "Epoch 204/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.2836 - acc: 0.3892 - val_loss: 3.2699 - val_acc: 0.3842\n",
      "Epoch 205/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.2808 - acc: 0.3842 - val_loss: 3.2544 - val_acc: 0.3596\n",
      "Epoch 206/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.2687 - acc: 0.3547 - val_loss: 3.2685 - val_acc: 0.3793\n",
      "Epoch 207/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2935 - acc: 0.3941 - val_loss: 3.2417 - val_acc: 0.3793\n",
      "Epoch 208/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2982 - acc: 0.3842 - val_loss: 3.2868 - val_acc: 0.3793\n",
      "Epoch 209/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.3027 - acc: 0.3744 - val_loss: 3.2752 - val_acc: 0.3892\n",
      "Epoch 210/400\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 3.2959 - acc: 0.3744 - val_loss: 3.2820 - val_acc: 0.3892\n",
      "Epoch 211/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 3.2924 - acc: 0.3842 - val_loss: 3.2275 - val_acc: 0.3892\n",
      "Epoch 212/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.2565 - acc: 0.3892 - val_loss: 3.2344 - val_acc: 0.3990\n",
      "Epoch 213/400\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 3.2680 - acc: 0.3645 - val_loss: 3.2967 - val_acc: 0.3941\n",
      "Epoch 214/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 3.3027 - acc: 0.3842 - val_loss: 3.2597 - val_acc: 0.4039\n",
      "Epoch 215/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 3.2774 - acc: 0.4089 - val_loss: 3.2477 - val_acc: 0.3941\n",
      "Epoch 216/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2605 - acc: 0.3596 - val_loss: 3.2058 - val_acc: 0.3596\n",
      "Epoch 217/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.2621 - acc: 0.3892 - val_loss: 3.2361 - val_acc: 0.3941\n",
      "Epoch 218/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.3317 - acc: 0.3842 - val_loss: 3.3155 - val_acc: 0.3941\n",
      "Epoch 219/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 3.3451 - acc: 0.3842 - val_loss: 3.3109 - val_acc: 0.4039\n",
      "Epoch 220/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.3149 - acc: 0.4138 - val_loss: 3.2575 - val_acc: 0.4138\n",
      "Epoch 221/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2608 - acc: 0.3941 - val_loss: 3.2070 - val_acc: 0.3990\n",
      "Epoch 222/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.2362 - acc: 0.4039 - val_loss: 3.1754 - val_acc: 0.4138\n",
      "Epoch 223/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.2151 - acc: 0.3990 - val_loss: 3.2187 - val_acc: 0.3990\n",
      "Epoch 224/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.2552 - acc: 0.3941 - val_loss: 3.2093 - val_acc: 0.4236\n",
      "Epoch 225/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.2234 - acc: 0.4089 - val_loss: 3.2208 - val_acc: 0.3941\n",
      "Epoch 226/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.2101 - acc: 0.3842 - val_loss: 3.1582 - val_acc: 0.4089\n",
      "Epoch 227/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.1945 - acc: 0.3990 - val_loss: 3.1250 - val_acc: 0.4236\n",
      "Epoch 228/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.1955 - acc: 0.4089 - val_loss: 3.2084 - val_acc: 0.4089\n",
      "Epoch 229/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.2509 - acc: 0.4089 - val_loss: 3.2542 - val_acc: 0.4138\n",
      "Epoch 230/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.2632 - acc: 0.4089 - val_loss: 3.2553 - val_acc: 0.3892\n",
      "Epoch 231/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.2442 - acc: 0.3744 - val_loss: 3.1377 - val_acc: 0.4138\n",
      "Epoch 232/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.1447 - acc: 0.4089 - val_loss: 3.1375 - val_acc: 0.4286\n",
      "Epoch 233/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.1588 - acc: 0.4089 - val_loss: 3.1554 - val_acc: 0.4089\n",
      "Epoch 234/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 3.1749 - acc: 0.4039 - val_loss: 3.1331 - val_acc: 0.4236\n",
      "Epoch 235/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.1460 - acc: 0.4187 - val_loss: 3.1628 - val_acc: 0.3941\n",
      "Epoch 236/400\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 3.1507 - acc: 0.3941 - val_loss: 3.1275 - val_acc: 0.4138\n",
      "Epoch 237/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.1313 - acc: 0.4236 - val_loss: 3.0942 - val_acc: 0.4236\n",
      "Epoch 238/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 3.1403 - acc: 0.4138 - val_loss: 3.1352 - val_acc: 0.3990\n",
      "Epoch 239/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 3.1889 - acc: 0.3990 - val_loss: 3.1535 - val_acc: 0.4187\n",
      "Epoch 240/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 3.2239 - acc: 0.4138 - val_loss: 3.1891 - val_acc: 0.4236\n",
      "Epoch 241/400\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 3.2093 - acc: 0.4187 - val_loss: 3.1406 - val_acc: 0.4236\n",
      "Epoch 242/400\n",
      "4/4 [==============================] - 0s 129ms/step - loss: 3.1338 - acc: 0.4138 - val_loss: 3.1599 - val_acc: 0.4138\n",
      "Epoch 243/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.1394 - acc: 0.4138 - val_loss: 3.0632 - val_acc: 0.4384\n",
      "Epoch 244/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.0964 - acc: 0.4335 - val_loss: 3.0541 - val_acc: 0.4187\n",
      "Epoch 245/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.1303 - acc: 0.4089 - val_loss: 3.1320 - val_acc: 0.4089\n",
      "Epoch 246/400\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 3.1833 - acc: 0.4089 - val_loss: 3.1404 - val_acc: 0.4138\n",
      "Epoch 247/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 3.1689 - acc: 0.4138 - val_loss: 3.1425 - val_acc: 0.4236\n",
      "Epoch 248/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.1349 - acc: 0.4089 - val_loss: 3.0930 - val_acc: 0.4039\n",
      "Epoch 249/400\n",
      "4/4 [==============================] - 0s 132ms/step - loss: 3.0922 - acc: 0.4138 - val_loss: 3.0481 - val_acc: 0.4286\n",
      "Epoch 250/400\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 3.1040 - acc: 0.3941 - val_loss: 3.0588 - val_acc: 0.4187\n",
      "Epoch 251/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 3.1457 - acc: 0.4335 - val_loss: 3.1099 - val_acc: 0.4384\n",
      "Epoch 252/400\n",
      "4/4 [==============================] - 0s 130ms/step - loss: 3.1590 - acc: 0.4138 - val_loss: 3.1293 - val_acc: 0.4187\n",
      "Epoch 253/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 3.1345 - acc: 0.4089 - val_loss: 3.0561 - val_acc: 0.4039\n",
      "Epoch 254/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.0604 - acc: 0.3941 - val_loss: 3.0433 - val_acc: 0.4335\n",
      "Epoch 255/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 3.0614 - acc: 0.4236 - val_loss: 2.9972 - val_acc: 0.4286\n",
      "Epoch 256/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.0456 - acc: 0.4433 - val_loss: 3.0189 - val_acc: 0.4384\n",
      "Epoch 257/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.0523 - acc: 0.4286 - val_loss: 3.0536 - val_acc: 0.4236\n",
      "Epoch 258/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.0579 - acc: 0.4187 - val_loss: 3.0271 - val_acc: 0.4236\n",
      "Epoch 259/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 3.0215 - acc: 0.4335 - val_loss: 3.0170 - val_acc: 0.4433\n",
      "Epoch 260/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 3.0188 - acc: 0.4384 - val_loss: 2.9582 - val_acc: 0.4483\n",
      "Epoch 261/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.0139 - acc: 0.4384 - val_loss: 2.9602 - val_acc: 0.4335\n",
      "Epoch 262/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 3.0265 - acc: 0.4236 - val_loss: 3.0424 - val_acc: 0.4138\n",
      "Epoch 263/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 3.0380 - acc: 0.4138 - val_loss: 3.0731 - val_acc: 0.4286\n",
      "Epoch 264/400\n",
      "4/4 [==============================] - 0s 112ms/step - loss: 3.0611 - acc: 0.4236 - val_loss: 3.0946 - val_acc: 0.4187\n",
      "Epoch 265/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 3.0845 - acc: 0.4335 - val_loss: 3.0071 - val_acc: 0.4384\n",
      "Epoch 266/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.0430 - acc: 0.4532 - val_loss: 2.9884 - val_acc: 0.4433\n",
      "Epoch 267/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 3.0443 - acc: 0.4236 - val_loss: 3.0046 - val_acc: 0.4187\n",
      "Epoch 268/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.0641 - acc: 0.4187 - val_loss: 2.9819 - val_acc: 0.4335\n",
      "Epoch 269/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 3.0145 - acc: 0.4286 - val_loss: 3.0173 - val_acc: 0.4138\n",
      "Epoch 270/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 3.0131 - acc: 0.4236 - val_loss: 2.9483 - val_acc: 0.4433\n",
      "Epoch 271/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2.9679 - acc: 0.4286 - val_loss: 2.9533 - val_acc: 0.4384\n",
      "Epoch 272/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.9998 - acc: 0.4384 - val_loss: 2.9535 - val_acc: 0.4384\n",
      "Epoch 273/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.9914 - acc: 0.4384 - val_loss: 2.9756 - val_acc: 0.4581\n",
      "Epoch 274/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.9982 - acc: 0.4433 - val_loss: 2.9845 - val_acc: 0.4384\n",
      "Epoch 275/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.9656 - acc: 0.4384 - val_loss: 2.9205 - val_acc: 0.4335\n",
      "Epoch 276/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.9169 - acc: 0.4286 - val_loss: 2.9117 - val_acc: 0.4384\n",
      "Epoch 277/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.9535 - acc: 0.4286 - val_loss: 2.9726 - val_acc: 0.4236\n",
      "Epoch 278/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.0025 - acc: 0.4286 - val_loss: 2.9557 - val_acc: 0.4433\n",
      "Epoch 279/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.9683 - acc: 0.4433 - val_loss: 2.9970 - val_acc: 0.4384\n",
      "Epoch 280/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.9812 - acc: 0.4335 - val_loss: 2.8932 - val_acc: 0.4483\n",
      "Epoch 281/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 2.9045 - acc: 0.4483 - val_loss: 2.8930 - val_acc: 0.4286\n",
      "Epoch 282/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.9500 - acc: 0.4286 - val_loss: 2.9109 - val_acc: 0.4532\n",
      "Epoch 283/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.9690 - acc: 0.4483 - val_loss: 2.9157 - val_acc: 0.4581\n",
      "Epoch 284/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.9558 - acc: 0.4631 - val_loss: 2.9646 - val_acc: 0.4384\n",
      "Epoch 285/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.9498 - acc: 0.4286 - val_loss: 2.9180 - val_acc: 0.4384\n",
      "Epoch 286/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.9053 - acc: 0.4286 - val_loss: 2.8822 - val_acc: 0.4286\n",
      "Epoch 287/400\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 2.9308 - acc: 0.4286 - val_loss: 2.8727 - val_acc: 0.4335\n",
      "Epoch 288/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 2.9507 - acc: 0.4384 - val_loss: 2.9196 - val_acc: 0.4433\n",
      "Epoch 289/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.9409 - acc: 0.4483 - val_loss: 2.9816 - val_acc: 0.4335\n",
      "Epoch 290/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 2.9845 - acc: 0.4236 - val_loss: 2.9097 - val_acc: 0.4236\n",
      "Epoch 291/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 2.9281 - acc: 0.4433 - val_loss: 2.8853 - val_acc: 0.4532\n",
      "Epoch 292/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 2.9039 - acc: 0.4335 - val_loss: 2.8159 - val_acc: 0.4286\n",
      "Epoch 293/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 2.8658 - acc: 0.4384 - val_loss: 2.8251 - val_acc: 0.4384\n",
      "Epoch 294/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.8884 - acc: 0.4286 - val_loss: 2.9188 - val_acc: 0.4483\n",
      "Epoch 295/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 2.9273 - acc: 0.4433 - val_loss: 2.8778 - val_acc: 0.4532\n",
      "Epoch 296/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8769 - acc: 0.4433 - val_loss: 2.8217 - val_acc: 0.4384\n",
      "Epoch 297/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.8616 - acc: 0.4384 - val_loss: 2.8417 - val_acc: 0.4384\n",
      "Epoch 298/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.9142 - acc: 0.4433 - val_loss: 2.8395 - val_acc: 0.4581\n",
      "Epoch 299/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.8743 - acc: 0.4483 - val_loss: 2.8947 - val_acc: 0.4286\n",
      "Epoch 300/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.8839 - acc: 0.4286 - val_loss: 2.8784 - val_acc: 0.4138\n",
      "Epoch 301/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.8661 - acc: 0.4187 - val_loss: 2.8337 - val_acc: 0.4384\n",
      "Epoch 302/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.8735 - acc: 0.4236 - val_loss: 2.7970 - val_acc: 0.4483\n",
      "Epoch 303/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.8765 - acc: 0.4433 - val_loss: 2.7830 - val_acc: 0.4483\n",
      "Epoch 304/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8378 - acc: 0.4433 - val_loss: 2.8247 - val_acc: 0.4631\n",
      "Epoch 305/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.8295 - acc: 0.4532 - val_loss: 2.8007 - val_acc: 0.4483\n",
      "Epoch 306/400\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 2.8011 - acc: 0.4433 - val_loss: 2.8190 - val_acc: 0.4433\n",
      "Epoch 307/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2.8259 - acc: 0.4433 - val_loss: 2.7745 - val_acc: 0.4680\n",
      "Epoch 308/400\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 2.7939 - acc: 0.4631 - val_loss: 2.7977 - val_acc: 0.4483\n",
      "Epoch 309/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.8380 - acc: 0.4680 - val_loss: 2.8637 - val_acc: 0.4483\n",
      "Epoch 310/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8540 - acc: 0.4384 - val_loss: 2.7796 - val_acc: 0.4581\n",
      "Epoch 311/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.7852 - acc: 0.4532 - val_loss: 2.7714 - val_acc: 0.4631\n",
      "Epoch 312/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.7869 - acc: 0.4631 - val_loss: 2.8045 - val_acc: 0.4532\n",
      "Epoch 313/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.8327 - acc: 0.4581 - val_loss: 2.8235 - val_acc: 0.4581\n",
      "Epoch 314/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.8377 - acc: 0.4187 - val_loss: 2.8448 - val_acc: 0.4532\n",
      "Epoch 315/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.8374 - acc: 0.4532 - val_loss: 2.8266 - val_acc: 0.4532\n",
      "Epoch 316/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8183 - acc: 0.4778 - val_loss: 2.7809 - val_acc: 0.4631\n",
      "Epoch 317/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 2.8125 - acc: 0.4483 - val_loss: 2.7871 - val_acc: 0.4483\n",
      "Epoch 318/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.7944 - acc: 0.4433 - val_loss: 2.7507 - val_acc: 0.4581\n",
      "Epoch 319/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.7638 - acc: 0.4631 - val_loss: 2.7325 - val_acc: 0.4532\n",
      "Epoch 320/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 2.7635 - acc: 0.4335 - val_loss: 2.7129 - val_acc: 0.4631\n",
      "Epoch 321/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 2.7436 - acc: 0.4581 - val_loss: 2.6963 - val_acc: 0.4729\n",
      "Epoch 322/400\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 2.7166 - acc: 0.4631 - val_loss: 2.6899 - val_acc: 0.4483\n",
      "Epoch 323/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.7372 - acc: 0.4335 - val_loss: 2.7129 - val_acc: 0.4729\n",
      "Epoch 324/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 2.7382 - acc: 0.4581 - val_loss: 2.7028 - val_acc: 0.4778\n",
      "Epoch 325/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.7146 - acc: 0.4729 - val_loss: 2.6812 - val_acc: 0.4680\n",
      "Epoch 326/400\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 2.7042 - acc: 0.4631 - val_loss: 2.7174 - val_acc: 0.4631\n",
      "Epoch 327/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 2.7769 - acc: 0.4680 - val_loss: 2.8358 - val_acc: 0.4926\n",
      "Epoch 328/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.8273 - acc: 0.4778 - val_loss: 2.8872 - val_acc: 0.4778\n",
      "Epoch 329/400\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 2.8492 - acc: 0.4581 - val_loss: 2.7803 - val_acc: 0.4631\n",
      "Epoch 330/400\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 2.7941 - acc: 0.4581 - val_loss: 2.7392 - val_acc: 0.4631\n",
      "Epoch 331/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.8221 - acc: 0.4483 - val_loss: 2.8111 - val_acc: 0.4631\n",
      "Epoch 332/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 2.8901 - acc: 0.4581 - val_loss: 2.8271 - val_acc: 0.4631\n",
      "Epoch 333/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8511 - acc: 0.4680 - val_loss: 2.7985 - val_acc: 0.4828\n",
      "Epoch 334/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.7885 - acc: 0.4631 - val_loss: 2.7297 - val_acc: 0.4778\n",
      "Epoch 335/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.7376 - acc: 0.4729 - val_loss: 2.7019 - val_acc: 0.4778\n",
      "Epoch 336/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.7646 - acc: 0.4581 - val_loss: 2.7576 - val_acc: 0.4680\n",
      "Epoch 337/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.8343 - acc: 0.4631 - val_loss: 2.8058 - val_acc: 0.4926\n",
      "Epoch 338/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.8048 - acc: 0.4926 - val_loss: 2.9094 - val_acc: 0.4729\n",
      "Epoch 339/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.8875 - acc: 0.4631 - val_loss: 2.8138 - val_acc: 0.4680\n",
      "Epoch 340/400\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 2.8142 - acc: 0.4680 - val_loss: 2.8336 - val_acc: 0.4483\n",
      "Epoch 341/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.8429 - acc: 0.4335 - val_loss: 2.7750 - val_acc: 0.4631\n",
      "Epoch 342/400\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 2.8198 - acc: 0.4680 - val_loss: 2.7307 - val_acc: 0.4828\n",
      "Epoch 343/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 2.7899 - acc: 0.4532 - val_loss: 2.7568 - val_acc: 0.4483\n",
      "Epoch 344/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.8144 - acc: 0.4483 - val_loss: 2.7841 - val_acc: 0.4532\n",
      "Epoch 345/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.8196 - acc: 0.4433 - val_loss: 2.7655 - val_acc: 0.4581\n",
      "Epoch 346/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.7610 - acc: 0.4581 - val_loss: 2.6962 - val_acc: 0.4877\n",
      "Epoch 347/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2.7045 - acc: 0.4975 - val_loss: 2.6578 - val_acc: 0.4778\n",
      "Epoch 348/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.7012 - acc: 0.4778 - val_loss: 2.6694 - val_acc: 0.4532\n",
      "Epoch 349/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.7232 - acc: 0.4532 - val_loss: 2.7474 - val_acc: 0.4877\n",
      "Epoch 350/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.7760 - acc: 0.4532 - val_loss: 2.7633 - val_acc: 0.4532\n",
      "Epoch 351/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.7536 - acc: 0.4532 - val_loss: 2.6685 - val_acc: 0.4828\n",
      "Epoch 352/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 2.6867 - acc: 0.4729 - val_loss: 2.6276 - val_acc: 0.4729\n",
      "Epoch 353/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.6881 - acc: 0.4581 - val_loss: 2.6624 - val_acc: 0.4778\n",
      "Epoch 354/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.6796 - acc: 0.4828 - val_loss: 2.6727 - val_acc: 0.4877\n",
      "Epoch 355/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.6574 - acc: 0.4877 - val_loss: 2.6260 - val_acc: 0.4631\n",
      "Epoch 356/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.6548 - acc: 0.4680 - val_loss: 2.6207 - val_acc: 0.4680\n",
      "Epoch 357/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.6829 - acc: 0.4729 - val_loss: 2.6717 - val_acc: 0.4778\n",
      "Epoch 358/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.6919 - acc: 0.4680 - val_loss: 2.7818 - val_acc: 0.4926\n",
      "Epoch 359/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.7429 - acc: 0.4877 - val_loss: 2.6854 - val_acc: 0.4975\n",
      "Epoch 360/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.6976 - acc: 0.4877 - val_loss: 2.6288 - val_acc: 0.4680\n",
      "Epoch 361/400\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 2.6789 - acc: 0.4680 - val_loss: 2.6380 - val_acc: 0.4926\n",
      "Epoch 362/400\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 2.7347 - acc: 0.4828 - val_loss: 2.6582 - val_acc: 0.4828\n",
      "Epoch 363/400\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 2.7065 - acc: 0.4778 - val_loss: 2.6826 - val_acc: 0.4877\n",
      "Epoch 364/400\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 2.6773 - acc: 0.4778 - val_loss: 2.6357 - val_acc: 0.4828\n",
      "Epoch 365/400\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 2.6272 - acc: 0.4680 - val_loss: 2.6113 - val_acc: 0.4828\n",
      "Epoch 366/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.6558 - acc: 0.4532 - val_loss: 2.6665 - val_acc: 0.4778\n",
      "Epoch 367/400\n",
      "4/4 [==============================] - 0s 133ms/step - loss: 2.7081 - acc: 0.4729 - val_loss: 2.6067 - val_acc: 0.4680\n",
      "Epoch 368/400\n",
      "4/4 [==============================] - 0s 121ms/step - loss: 2.6232 - acc: 0.4729 - val_loss: 2.6341 - val_acc: 0.4729\n",
      "Epoch 369/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.6364 - acc: 0.4729 - val_loss: 2.6097 - val_acc: 0.4631\n",
      "Epoch 370/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 2.6280 - acc: 0.4778 - val_loss: 2.5796 - val_acc: 0.4483\n",
      "Epoch 371/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.6220 - acc: 0.4433 - val_loss: 2.5893 - val_acc: 0.4828\n",
      "Epoch 372/400\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 2.6193 - acc: 0.4828 - val_loss: 2.5828 - val_acc: 0.4729\n",
      "Epoch 373/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.5746 - acc: 0.4828 - val_loss: 2.5638 - val_acc: 0.4729\n",
      "Epoch 374/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.5932 - acc: 0.4532 - val_loss: 2.5631 - val_acc: 0.4778\n",
      "Epoch 375/400\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 2.6106 - acc: 0.4828 - val_loss: 2.5639 - val_acc: 0.4828\n",
      "Epoch 376/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.6067 - acc: 0.4828 - val_loss: 2.6634 - val_acc: 0.4729\n",
      "Epoch 377/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.6453 - acc: 0.4581 - val_loss: 2.6337 - val_acc: 0.4828\n",
      "Epoch 378/400\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 2.6197 - acc: 0.4778 - val_loss: 2.5727 - val_acc: 0.4581\n",
      "Epoch 379/400\n",
      "4/4 [==============================] - 0s 123ms/step - loss: 2.6061 - acc: 0.4433 - val_loss: 2.5435 - val_acc: 0.4926\n",
      "Epoch 380/400\n",
      "4/4 [==============================] - 1s 282ms/step - loss: 2.6101 - acc: 0.4778 - val_loss: 2.6078 - val_acc: 0.4729\n",
      "Epoch 381/400\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 2.6448 - acc: 0.4877 - val_loss: 2.6989 - val_acc: 0.4778\n",
      "Epoch 382/400\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 2.6875 - acc: 0.4680 - val_loss: 2.6386 - val_acc: 0.4828\n",
      "Epoch 383/400\n",
      "4/4 [==============================] - 0s 125ms/step - loss: 2.6216 - acc: 0.4828 - val_loss: 2.5950 - val_acc: 0.4532\n",
      "Epoch 384/400\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 2.6073 - acc: 0.4384 - val_loss: 2.5349 - val_acc: 0.4680\n",
      "Epoch 385/400\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 2.5701 - acc: 0.5025 - val_loss: 2.5676 - val_acc: 0.4828\n",
      "Epoch 386/400\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 2.6314 - acc: 0.4828 - val_loss: 2.6275 - val_acc: 0.4828\n",
      "Epoch 387/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.6316 - acc: 0.4828 - val_loss: 2.6471 - val_acc: 0.4778\n",
      "Epoch 388/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.6225 - acc: 0.4631 - val_loss: 2.5795 - val_acc: 0.4680\n",
      "Epoch 389/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2.5983 - acc: 0.4532 - val_loss: 2.4816 - val_acc: 0.4729\n",
      "Epoch 390/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.5655 - acc: 0.4828 - val_loss: 2.5194 - val_acc: 0.4828\n",
      "Epoch 391/400\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 2.5791 - acc: 0.4828 - val_loss: 2.6354 - val_acc: 0.4729\n",
      "Epoch 392/400\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 2.6802 - acc: 0.4680 - val_loss: 2.6359 - val_acc: 0.4631\n",
      "Epoch 393/400\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 2.6338 - acc: 0.4680 - val_loss: 2.6415 - val_acc: 0.4828\n",
      "Epoch 394/400\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 2.6280 - acc: 0.4581 - val_loss: 2.5955 - val_acc: 0.5025\n",
      "Epoch 395/400\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 2.5794 - acc: 0.5123 - val_loss: 2.5560 - val_acc: 0.4778\n",
      "Epoch 396/400\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 2.5575 - acc: 0.4631 - val_loss: 2.5637 - val_acc: 0.4483\n",
      "Epoch 397/400\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 2.5774 - acc: 0.4680 - val_loss: 2.5422 - val_acc: 0.4729\n",
      "Epoch 398/400\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 2.5496 - acc: 0.4778 - val_loss: 2.5351 - val_acc: 0.4729\n",
      "Epoch 399/400\n",
      "4/4 [==============================] - 0s 133ms/step - loss: 2.5318 - acc: 0.4680 - val_loss: 2.4581 - val_acc: 0.4877\n",
      "Epoch 400/400\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 2.4925 - acc: 0.4828 - val_loss: 2.5014 - val_acc: 0.4778\n"
     ]
    }
   ],
   "source": [
    "in_feat = 9\n",
    "batch_size = 64\n",
    "epochs = 400\n",
    "input_sequence_length = 12\n",
    "forecast_horizon = 3\n",
    "multi_horizon = False\n",
    "out_feat = 10\n",
    "lstm_units = 64\n",
    "graph_conv_params = {\n",
    "    \"aggregation_type\": \"mean\",\n",
    "    \"combination_type\": \"concat\",\n",
    "    \"activation\": None,\n",
    "}\n",
    "\n",
    "st_gcn = LSTMGC(\n",
    "    in_feat,\n",
    "    out_feat,\n",
    "    lstm_units,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    graph,\n",
    "    graph_conv_params,\n",
    ")\n",
    "inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n",
    "outputs = st_gcn(inputs)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n",
    "#     loss=keras.losses.MeanAbsoluteError(),\n",
    "# )\n",
    "learning_rate = 0.0005\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\"),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    test_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=220)],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LSTM output shape  = Tensor(\"model_25/lstmgc_27/lstm_38/strided_slice_3:0\", shape=(None, 64), dtype=float32)\n",
      "4/4 [==============================] - 1s 40ms/step\n",
      "[  0.    8.   -4.8   0.   -3.    4.    0.   -2.   -5.   -3.    3.   28.\n",
      "  -3.    0.   -3.5   0.    0.   10.  -18.   -3. ]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4332/1503344004.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mairportidx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairports\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mplotComparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairportidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# plotComparison(2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4332/1503344004.py\u001b[0m in \u001b[0;36mplotComparison\u001b[1;34m(airport_index, ypredFull, yactualFull)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplotComparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mairport_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypredFull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mypredFull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myactualFull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myactualFull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mypredFull\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mairport_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0myactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myactualFull\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mairport_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mairport_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "# ypredFull = model.predict(test_dataset, verbose=1)\n",
    "# yactualFull = Ytest[0,:]\n",
    "# print(yactualFull)\n",
    "\n",
    "# # airport_index = 0\n",
    "# def plotComparison(airport_index, ypredFull=ypredFull, yactualFull=yactualFull):\n",
    "#     ypred = ypredFull[:,airport_index,:]\n",
    "#     yactual = yactualFull[:,airport_index,:]\n",
    "\n",
    "#     fig, axs = plt.subplots(2, 1, sharex=True, num=airport_index)\n",
    "#     axs[0].plot(ypred[:, 0], label=\"Predicted Arrival Delay\")\n",
    "#     axs[1].plot(ypred[:, 1], label=\"Predicted Departure Delay\")\n",
    "#     axs[0].plot(yactual[:, 0], label=\"Actual Arrival Delay\")\n",
    "#     axs[1].plot(yactual[:, 1], label=\"Actual Departure Delay\")\n",
    "#     axs[0].legend()\n",
    "#     axs[1].legend()\n",
    "#     plt.suptitle(f\"Comparison for airport: {airports[airport_index]}\")\n",
    "\n",
    "# for airportidx in range(0,len(airports)):\n",
    "#     plotComparison(airportidx)\n",
    "# # plotComparison(2)\n",
    "# # plt.show()\n",
    "# # graphData.visualiseGraph(timeSlice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
