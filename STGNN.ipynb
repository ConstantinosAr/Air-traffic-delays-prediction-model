{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogarithmicError\n",
    "\n",
    "from extraction.extract import *\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency\n",
    "# from extraction.adj_data import *\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings control the jupyter notebook. We have generated and stored 2 models, one for the top10 airports and one the top50. The default settings loads the model for the top 50 and displays the results. To try the top10 model, change airportsFull to ICAOTOP10 and runName to \"top10MSE\". To try a custom run, choose an airport list of your liking, change runName to something else, change saveModel to True, and loadModel to False. For reference, top10 took about 15 minutes to train and top50 about 45 minutes. \n",
    "\n",
    "Once your settings are set, you can just click run all in the jupyter interface. In the final plotting cell more settings can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsFull = ICAOTOP50\n",
    "airports = airportsFull[::]\n",
    "airports.remove(\"LTFM\")  # removing the new istanbul airport as it opened mid-2019\n",
    "\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2018, 3, 1)\n",
    "end = datetime(2019, 1, 1)\n",
    "timeslotLength = 30\n",
    "\n",
    "# Run settings\n",
    "batch_size = 64\n",
    "epochs = 1500\n",
    "patience = 100\n",
    "input_sequence_length = 4 * int(60/timeslotLength)\n",
    "forecast_horizon = 10\n",
    "multi_horizon = True\n",
    "\n",
    "learning_rate = 0.0001 \n",
    "# learning_rate = 0.0005\n",
    "\n",
    "\n",
    "runName = \"top50MSE30minsV6\"\n",
    "saveModel = False\n",
    "loadModel = True\n",
    "modelSaveFolder = \"kerasModels/\"\n",
    "modelSaveLocation = modelSaveFolder + runName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:04<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "    airports, timeslotLength, GNNFormat=True, start=start, end=end, disableWeather=True\n",
    ")\n",
    "times = list(dataDict.values())[0][\"T\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the amount of features and normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T x N x F:  Xarray = (5856, 49, 8) | Yarray = (5856, 49, 2)\n"
     ]
    }
   ],
   "source": [
    "# we remove some of the most uncorrelated features\n",
    "columnsToDrop = [\n",
    "    \"weekend\",\n",
    "    \"winter\",\n",
    "    \"spring\",\n",
    "    \"summer\",\n",
    "    \"autumn\",\n",
    "    \"night\",\n",
    "    \"morning\",\n",
    "    \"afternoon\",\n",
    "    \"planes\",\n",
    "]\n",
    "\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler = RobustScaler()\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1).to_numpy()\n",
    "    # X = scaler.fit_transform(X)\n",
    "    Xlist.append(X)\n",
    "\n",
    "    Y = dataDict[airport][\"Y\"].iloc[:, :].to_numpy()\n",
    "    Ylist.append(Y)\n",
    "\n",
    "\n",
    "Xlist = np.stack(Xlist)\n",
    "Ylist = np.stack(Ylist)\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# Reshape to a flat array that goes arrival then departure delay\n",
    "# Yarray = np.reshape(Yarray, newshape=[len(times), len(airports)*2], order=\"F\")\n",
    "# # T x N x F\n",
    "\n",
    "# # Normalise over the features\n",
    "Xmean, Xstd = Xarray.mean(axis=0), Xarray.std(axis=0)\n",
    "Xarray = (Xarray - Xmean) / Xstd\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make the raw data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 0:3513 | Validation split: 3513:4977 | Test split: 4977:5856\n"
     ]
    }
   ],
   "source": [
    "train_split, val_split = 0.6, 0.25\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(\n",
    "    f\"Train split: {0}:{train_idx} | Validation split: {train_idx}:{val_idx} | Test split: {val_idx}:{fullLength}\"\n",
    ")\n",
    "\n",
    "# generate raw splits\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx::]\n",
    "\n",
    "# Save test timeslots for plotting purposes\n",
    "testTimes = times.iloc[val_idx::, 0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensorflow dataset that handles lookback, lookforward and multi-horizon properties. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CacheDataset shapes: ((None, None, 49, 8), (None, None, 49, 2)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    target_array,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "    `inputs` is a Tensor\n",
    "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "    the `input_sequence_length` past values of the timeseries for each node.\n",
    "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "    containing the `forecast_horizon`\n",
    "    future values of the timeseries for each node.\n",
    "\n",
    "    Args:\n",
    "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "            timeseries `forecast_horizon` steps ahead (only one value).\n",
    "        batch_size: Number of timeseries samples in each batch.\n",
    "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "        multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = inputs\n",
    "    target_offset = (\n",
    "        input_sequence_length\n",
    "        if multi_horizon\n",
    "        else input_sequence_length + forecast_horizon - 1\n",
    "    )\n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        target_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_dataset = create_tf_dataset(\n",
    "    Xtrain,\n",
    "    Ytrain,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "val_dataset = create_tf_dataset(\n",
    "    Xval,\n",
    "    Yval,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "test_dataset = create_tf_dataset(\n",
    "    Xtest,\n",
    "    Ytest,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was our original intent to use dynamic graphs for our st-gnn, however due to lack of time we kept it as a weighted average of the distance based distance_weight_adjacency() and an average of the adjancy matrices based on flights getAdjacencyMatrix()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 49)\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "distance_adj = distance_weight_adjacency(airports, threshold=400)\n",
    "# unfortunately we only work with static graphs for now\n",
    "# We take the average of all the dynamic adjacencies from the function above (stored for brevity)\n",
    "flight_adj = np.mean(getAdjacencyMatrix(airports, start, end, timeslotLength), axis=0)\n",
    "\n",
    "# print(flight_adj)\n",
    "\n",
    "# flight_adj = flight_adj_avg\n",
    "\n",
    "adjacency_matrix = distance_adj * 0.4 + (1 - 0.4) * flight_adj\n",
    "\n",
    "node_indices, neighbor_indices = np.where(adjacency_matrix != 0)\n",
    "\n",
    "graph_edges = (node_indices.tolist(), neighbor_indices.tolist())\n",
    "graph_num_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "print(flight_adj.shape)\n",
    "# print(graph_edges)\n",
    "print(graph_num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block shows a heatmap of the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adjacency_matrix)\n",
    "plt.xticks(range(len(airports)), airports, fontsize=12, rotation=-90)\n",
    "plt.yticks(range(len(airports)), airports, fontsize=12, rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our GCN layer. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        # graph_info: GraphInfo,\n",
    "        graph_num_nodes=graph_num_nodes,\n",
    "        graph_edges: tuple = graph_edges,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        activation: typing.Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "\n",
    "        self.graph_edges = graph_edges\n",
    "        self.graph_num_nodes = graph_num_nodes\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=keras.initializers.glorot_uniform()(\n",
    "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.activation = layers.Activation(activation)\n",
    "\n",
    "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
    "        aggregation_func = {\n",
    "            \"sum\": tf.math.unsorted_segment_sum,\n",
    "            \"mean\": tf.math.unsorted_segment_mean,\n",
    "            \"max\": tf.math.unsorted_segment_max,\n",
    "        }.get(self.aggregation_type)\n",
    "\n",
    "        if aggregation_func:\n",
    "            return aggregation_func(\n",
    "                neighbour_representations,\n",
    "                self.graph_edges[0],\n",
    "                num_segments=self.graph_num_nodes,\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
    "\n",
    "    def compute_nodes_representation(self, features: tf.Tensor):\n",
    "        \"\"\"Computes each node's representation.\n",
    "\n",
    "        The nodes' representations are obtained by multiplying the features tensor with\n",
    "        `self.weight`. Note that\n",
    "        `self.weight` has shape `(in_feat, out_feat)`.\n",
    "\n",
    "        Args:\n",
    "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        return tf.matmul(features, self.weight)\n",
    "\n",
    "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
    "        neighbour_representations = tf.gather(features, self.graph_edges[1])\n",
    "        aggregated_messages = self.aggregate(neighbour_representations)\n",
    "        return tf.matmul(aggregated_messages, self.weight)\n",
    "\n",
    "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
    "        if self.combination_type == \"concat\":\n",
    "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = nodes_representation + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        return self.activation(h)\n",
    "\n",
    "    def call(self, features: tf.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        nodes_representation = self.compute_nodes_representation(features)\n",
    "        aggregated_messages = self.compute_aggregated_messages(features)\n",
    "        return self.update(nodes_representation, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM including graph convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our complete model, it is based off a road-traffic example that predicts all features for the next step. We modified it heavily to allow for predictions of 2 labels and multihorizon predictions. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGC(layers.Layer):\n",
    "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        graph_edges: tuple,\n",
    "        graph_num_nodes: int,\n",
    "        graph_conv_params: typing.Optional[dict] = None,\n",
    "        num_labels: int = 2,\n",
    "        multi_horizon: int = True,\n",
    "        forecast_horizon: int = forecast_horizon,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if multi_horizon:\n",
    "            self.forecast_horizon = forecast_horizon\n",
    "        else:\n",
    "            self.forecast_horizon = 1\n",
    "\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.lstm_units = lstm_units\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.graph_edges = (graph_edges,)\n",
    "        self.graph_num_nodes = (graph_num_nodes,)\n",
    "\n",
    "        self.graph_conv_params = graph_conv_params\n",
    "        self.multi_horizon = multi_horizon\n",
    "        self.num_labels = num_labels\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # graph conv layer\n",
    "        if graph_conv_params is None:\n",
    "            graph_conv_params = {\n",
    "                \"aggregation_type\": \"mean\",\n",
    "                \"combination_type\": \"concat\",\n",
    "                \"activation\": None,\n",
    "            }\n",
    "\n",
    "        # Layer definitions\n",
    "        self.graph_conv = GraphConv(in_feat, out_feat, **graph_conv_params)\n",
    "        # self.graph_conv2 = GraphConv(in_feat, out_feat, self.graph_edges, self.graph_num_nodes, **graph_conv_params)\n",
    "        l2_reg = 2.5e-4  # L2 regularization rate\n",
    "\n",
    "        self.lstm1 = layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=True,\n",
    "            activation=\"tanh\",\n",
    "            dropout=0.2,\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            activity_regularizer=l2(l2_reg),\n",
    "            bias_regularizer=l2(l2_reg),\n",
    "        )\n",
    "        self.lstm2 = layers.LSTM(\n",
    "            lstm_units,\n",
    "            activation=\"tanh\",\n",
    "            dropout=0.2,\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            activity_regularizer=l2(l2_reg),\n",
    "            bias_regularizer=l2(l2_reg),\n",
    "        )\n",
    "        # self.dense = layers.Dense(output_seq_len)\n",
    "        self.denseThick = layers.Dense(128)\n",
    "        self.denseThick2 = layers.Dense(64)\n",
    "        self.denseThick3 = layers.Dense(16)\n",
    "        self.dense = layers.Dense(self.forecast_horizon * self.num_labels)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"in_feat\": self.in_feat,\n",
    "                \"out_feat\": self.out_feat,\n",
    "                \"lstm_units\": self.lstm_units,\n",
    "                \"input_seq_len\": self.input_seq_len,\n",
    "                \"graph_conv_params\": self.graph_conv_params,\n",
    "                \"graph_edges\": self.graph_edges,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "        gcn_out = self.graph_conv(inputs)\n",
    "\n",
    "        # print(f\"The GCN output shape  = {gcn_out}\")\n",
    "        shape = tf.shape(gcn_out)\n",
    "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3],\n",
    "        )\n",
    "        # LSTM takes only 3D tensors as input\n",
    "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
    "        # print(f\"The input shape for the LSTM is {gcn_out}\")\n",
    "        lstmLayer1 = self.lstm1(\n",
    "            gcn_out\n",
    "        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        lstmLayer2 = self.lstm2(lstmLayer1)\n",
    "        dense_1 = self.denseThick(lstmLayer2)\n",
    "        dense_2 = self.denseThick2(dense_1)\n",
    "        dense_3 = self.denseThick3(dense_2)\n",
    "\n",
    "        dense_output = self.dense(dense_3)\n",
    "        # dense_output has shape: (batch_size * num_nodes, multi_thing*2)\n",
    "\n",
    "        output = tf.reshape(\n",
    "            dense_output,\n",
    "            (num_nodes, batch_size, self.forecast_horizon, self.num_labels),\n",
    "        )\n",
    "        final = tf.transpose(output, [1, 2, 0, 3])\n",
    "        return final\n",
    "        # # returns Tensor of shape (batch_size, forecast_horizon, num_nodes, nlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8, 49, 8)]        0         \n",
      "                                                                 \n",
      " lstmgc (LSTMGC)             (None, 10, 49, 2)         239260    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 239,260\n",
      "Trainable params: 239,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_feat = Xarray.shape[2]\n",
    "\n",
    "out_feat = 15\n",
    "lstm_units = 128\n",
    "graph_conv_params = {\n",
    "    \"aggregation_type\": \"mean\",\n",
    "    \"combination_type\": \"concat\",\n",
    "    \"activation\": None,\n",
    "}\n",
    "\n",
    "st_gcn = LSTMGC(\n",
    "    in_feat=in_feat,\n",
    "    out_feat=out_feat,\n",
    "    lstm_units=lstm_units,\n",
    "    input_seq_len=input_sequence_length,\n",
    "    graph_edges=graph_edges,\n",
    "    graph_num_nodes=graph_num_nodes,\n",
    "    graph_conv_params=graph_conv_params,\n",
    "    num_labels=2,\n",
    "    multi_horizon=multi_horizon,\n",
    "    forecast_horizon=forecast_horizon,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = layers.Input((input_sequence_length, graph_num_nodes, in_feat))\n",
    "outputs = st_gcn(inputs)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=MeanSquaredError(reduction=\"auto\", name=\"mean_absolute_error\"),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model, if loading the model from a previous run was not requestd. Using tensorboard, training can be followed live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "55/55 [==============================] - 19s 82ms/step - loss: 94.9625 - acc: 0.5175 - val_loss: 70.0986 - val_acc: 0.5063\n",
      "Epoch 2/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 83.2977 - acc: 0.5246 - val_loss: 66.9011 - val_acc: 0.5212\n",
      "Epoch 3/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 76.0431 - acc: 0.5178 - val_loss: 67.0197 - val_acc: 0.5141\n",
      "Epoch 4/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 74.0268 - acc: 0.5369 - val_loss: 65.7865 - val_acc: 0.5374\n",
      "Epoch 5/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 72.9038 - acc: 0.5410 - val_loss: 64.9245 - val_acc: 0.5460\n",
      "Epoch 6/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 72.0247 - acc: 0.5428 - val_loss: 64.2975 - val_acc: 0.5429\n",
      "Epoch 7/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 71.3241 - acc: 0.5433 - val_loss: 63.6860 - val_acc: 0.5435\n",
      "Epoch 8/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 70.8095 - acc: 0.5456 - val_loss: 63.1093 - val_acc: 0.5444\n",
      "Epoch 9/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 70.4272 - acc: 0.5478 - val_loss: 62.6223 - val_acc: 0.5457\n",
      "Epoch 10/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 70.1534 - acc: 0.5498 - val_loss: 62.2382 - val_acc: 0.5491\n",
      "Epoch 11/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 69.9081 - acc: 0.5520 - val_loss: 61.9085 - val_acc: 0.5522\n",
      "Epoch 12/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 69.7122 - acc: 0.5540 - val_loss: 61.6152 - val_acc: 0.5542\n",
      "Epoch 13/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 69.5293 - acc: 0.5562 - val_loss: 61.3468 - val_acc: 0.5585\n",
      "Epoch 14/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 69.3755 - acc: 0.5584 - val_loss: 61.1193 - val_acc: 0.5616\n",
      "Epoch 15/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 69.2159 - acc: 0.5603 - val_loss: 60.9111 - val_acc: 0.5633\n",
      "Epoch 16/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 69.0638 - acc: 0.5621 - val_loss: 60.7092 - val_acc: 0.5640\n",
      "Epoch 17/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 68.9150 - acc: 0.5632 - val_loss: 60.5058 - val_acc: 0.5645\n",
      "Epoch 18/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 68.7899 - acc: 0.5642 - val_loss: 60.3376 - val_acc: 0.5646\n",
      "Epoch 19/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 68.6612 - acc: 0.5649 - val_loss: 60.1730 - val_acc: 0.5651\n",
      "Epoch 20/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 68.5187 - acc: 0.5658 - val_loss: 60.0108 - val_acc: 0.5655\n",
      "Epoch 21/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 68.3919 - acc: 0.5665 - val_loss: 59.8618 - val_acc: 0.5663\n",
      "Epoch 22/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 68.2612 - acc: 0.5671 - val_loss: 59.7084 - val_acc: 0.5671\n",
      "Epoch 23/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 68.1338 - acc: 0.5681 - val_loss: 59.5639 - val_acc: 0.5681\n",
      "Epoch 24/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 68.0406 - acc: 0.5688 - val_loss: 59.4350 - val_acc: 0.5688\n",
      "Epoch 25/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 67.9055 - acc: 0.5697 - val_loss: 59.3006 - val_acc: 0.5697\n",
      "Epoch 26/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 67.7964 - acc: 0.5703 - val_loss: 59.1708 - val_acc: 0.5705\n",
      "Epoch 27/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 67.6823 - acc: 0.5711 - val_loss: 59.0601 - val_acc: 0.5713\n",
      "Epoch 28/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 67.5981 - acc: 0.5717 - val_loss: 58.9520 - val_acc: 0.5722\n",
      "Epoch 29/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 67.4945 - acc: 0.5724 - val_loss: 58.8587 - val_acc: 0.5729\n",
      "Epoch 30/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 67.3975 - acc: 0.5731 - val_loss: 58.7676 - val_acc: 0.5735\n",
      "Epoch 31/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 67.3164 - acc: 0.5738 - val_loss: 58.6817 - val_acc: 0.5743\n",
      "Epoch 32/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 67.2166 - acc: 0.5749 - val_loss: 58.5945 - val_acc: 0.5750\n",
      "Epoch 33/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 67.1345 - acc: 0.5756 - val_loss: 58.5027 - val_acc: 0.5758\n",
      "Epoch 34/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 67.0555 - acc: 0.5767 - val_loss: 58.4379 - val_acc: 0.5766\n",
      "Epoch 35/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 66.9825 - acc: 0.5778 - val_loss: 58.3560 - val_acc: 0.5775\n",
      "Epoch 36/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 66.9308 - acc: 0.5787 - val_loss: 58.3001 - val_acc: 0.5785\n",
      "Epoch 37/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 66.8346 - acc: 0.5799 - val_loss: 58.2317 - val_acc: 0.5793\n",
      "Epoch 38/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 66.7599 - acc: 0.5811 - val_loss: 58.1425 - val_acc: 0.5803\n",
      "Epoch 39/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 66.7169 - acc: 0.5823 - val_loss: 58.0730 - val_acc: 0.5810\n",
      "Epoch 40/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 66.6306 - acc: 0.5836 - val_loss: 58.0273 - val_acc: 0.5818\n",
      "Epoch 41/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 66.5737 - acc: 0.5847 - val_loss: 57.9506 - val_acc: 0.5827\n",
      "Epoch 42/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 66.5010 - acc: 0.5858 - val_loss: 57.8921 - val_acc: 0.5834\n",
      "Epoch 43/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 66.4340 - acc: 0.5873 - val_loss: 57.8350 - val_acc: 0.5842\n",
      "Epoch 44/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 66.3647 - acc: 0.5886 - val_loss: 57.7647 - val_acc: 0.5850\n",
      "Epoch 45/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 66.3084 - acc: 0.5896 - val_loss: 57.7008 - val_acc: 0.5859\n",
      "Epoch 46/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 66.2617 - acc: 0.5909 - val_loss: 57.6441 - val_acc: 0.5865\n",
      "Epoch 47/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 66.1897 - acc: 0.5920 - val_loss: 57.5775 - val_acc: 0.5868\n",
      "Epoch 48/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 66.1207 - acc: 0.5927 - val_loss: 57.4993 - val_acc: 0.5874\n",
      "Epoch 49/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 66.0516 - acc: 0.5936 - val_loss: 57.4418 - val_acc: 0.5877\n",
      "Epoch 50/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 65.9831 - acc: 0.5942 - val_loss: 57.3765 - val_acc: 0.5882\n",
      "Epoch 51/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.9007 - acc: 0.5947 - val_loss: 57.2999 - val_acc: 0.5883\n",
      "Epoch 52/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.8287 - acc: 0.5949 - val_loss: 57.2287 - val_acc: 0.5881\n",
      "Epoch 53/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 65.7888 - acc: 0.5952 - val_loss: 57.1548 - val_acc: 0.5874\n",
      "Epoch 54/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 65.7163 - acc: 0.5949 - val_loss: 57.0933 - val_acc: 0.5862\n",
      "Epoch 55/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.6628 - acc: 0.5946 - val_loss: 57.0266 - val_acc: 0.5856\n",
      "Epoch 56/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.6067 - acc: 0.5942 - val_loss: 56.9776 - val_acc: 0.5844\n",
      "Epoch 57/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.5001 - acc: 0.5931 - val_loss: 56.9088 - val_acc: 0.5827\n",
      "Epoch 58/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 65.4604 - acc: 0.5924 - val_loss: 56.8409 - val_acc: 0.5813\n",
      "Epoch 59/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 65.3963 - acc: 0.5910 - val_loss: 56.7878 - val_acc: 0.5801\n",
      "Epoch 60/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 65.3150 - acc: 0.5902 - val_loss: 56.7183 - val_acc: 0.5787\n",
      "Epoch 61/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.3044 - acc: 0.5890 - val_loss: 56.6731 - val_acc: 0.5780\n",
      "Epoch 62/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.2423 - acc: 0.5883 - val_loss: 56.6004 - val_acc: 0.5770\n",
      "Epoch 63/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.1697 - acc: 0.5874 - val_loss: 56.5484 - val_acc: 0.5762\n",
      "Epoch 64/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.1306 - acc: 0.5861 - val_loss: 56.5110 - val_acc: 0.5744\n",
      "Epoch 65/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 65.0666 - acc: 0.5851 - val_loss: 56.4295 - val_acc: 0.5721\n",
      "Epoch 66/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 65.0065 - acc: 0.5837 - val_loss: 56.3754 - val_acc: 0.5711\n",
      "Epoch 67/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 64.9346 - acc: 0.5827 - val_loss: 56.3343 - val_acc: 0.5697\n",
      "Epoch 68/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.8880 - acc: 0.5816 - val_loss: 56.2781 - val_acc: 0.5693\n",
      "Epoch 69/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 64.8593 - acc: 0.5811 - val_loss: 56.2321 - val_acc: 0.5687\n",
      "Epoch 70/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 64.7914 - acc: 0.5802 - val_loss: 56.1891 - val_acc: 0.5679\n",
      "Epoch 71/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 64.7296 - acc: 0.5801 - val_loss: 56.1339 - val_acc: 0.5679\n",
      "Epoch 72/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 64.6897 - acc: 0.5796 - val_loss: 56.1054 - val_acc: 0.5672\n",
      "Epoch 73/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 64.6669 - acc: 0.5794 - val_loss: 56.0549 - val_acc: 0.5670\n",
      "Epoch 74/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.6271 - acc: 0.5790 - val_loss: 56.0005 - val_acc: 0.5667\n",
      "Epoch 75/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.5557 - acc: 0.5788 - val_loss: 55.9618 - val_acc: 0.5656\n",
      "Epoch 76/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.5229 - acc: 0.5783 - val_loss: 55.9222 - val_acc: 0.5664\n",
      "Epoch 77/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.4762 - acc: 0.5781 - val_loss: 55.8806 - val_acc: 0.5660\n",
      "Epoch 78/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 64.4221 - acc: 0.5780 - val_loss: 55.8367 - val_acc: 0.5661\n",
      "Epoch 79/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 64.3981 - acc: 0.5785 - val_loss: 55.8013 - val_acc: 0.5663\n",
      "Epoch 80/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 64.3168 - acc: 0.5782 - val_loss: 55.7699 - val_acc: 0.5670\n",
      "Epoch 81/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 64.3066 - acc: 0.5787 - val_loss: 55.7360 - val_acc: 0.5665\n",
      "Epoch 82/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.2459 - acc: 0.5783 - val_loss: 55.6871 - val_acc: 0.5659\n",
      "Epoch 83/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.1904 - acc: 0.5777 - val_loss: 55.6561 - val_acc: 0.5665\n",
      "Epoch 84/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 64.1776 - acc: 0.5785 - val_loss: 55.6205 - val_acc: 0.5663\n",
      "Epoch 85/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 64.1604 - acc: 0.5781 - val_loss: 55.6070 - val_acc: 0.5662\n",
      "Epoch 86/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.1351 - acc: 0.5781 - val_loss: 55.5645 - val_acc: 0.5665\n",
      "Epoch 87/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.0700 - acc: 0.5788 - val_loss: 55.5425 - val_acc: 0.5664\n",
      "Epoch 88/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 64.0301 - acc: 0.5786 - val_loss: 55.4984 - val_acc: 0.5659\n",
      "Epoch 89/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 64.0070 - acc: 0.5785 - val_loss: 55.4559 - val_acc: 0.5667\n",
      "Epoch 90/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 63.9351 - acc: 0.5787 - val_loss: 55.4186 - val_acc: 0.5664\n",
      "Epoch 91/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 63.9177 - acc: 0.5796 - val_loss: 55.3970 - val_acc: 0.5664\n",
      "Epoch 92/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.8633 - acc: 0.5792 - val_loss: 55.3711 - val_acc: 0.5671\n",
      "Epoch 93/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 63.7992 - acc: 0.5795 - val_loss: 55.3412 - val_acc: 0.5666\n",
      "Epoch 94/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.7773 - acc: 0.5794 - val_loss: 55.3111 - val_acc: 0.5667\n",
      "Epoch 95/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.7699 - acc: 0.5791 - val_loss: 55.2861 - val_acc: 0.5664\n",
      "Epoch 96/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 63.7304 - acc: 0.5790 - val_loss: 55.2511 - val_acc: 0.5667\n",
      "Epoch 97/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 63.6746 - acc: 0.5799 - val_loss: 55.1958 - val_acc: 0.5671\n",
      "Epoch 98/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.6339 - acc: 0.5796 - val_loss: 55.1839 - val_acc: 0.5670\n",
      "Epoch 99/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.5916 - acc: 0.5802 - val_loss: 55.1661 - val_acc: 0.5674\n",
      "Epoch 100/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.6133 - acc: 0.5792 - val_loss: 55.1163 - val_acc: 0.5668\n",
      "Epoch 101/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.5461 - acc: 0.5796 - val_loss: 55.1168 - val_acc: 0.5667\n",
      "Epoch 102/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.4776 - acc: 0.5805 - val_loss: 55.0779 - val_acc: 0.5671\n",
      "Epoch 103/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 63.4866 - acc: 0.5803 - val_loss: 55.0466 - val_acc: 0.5674\n",
      "Epoch 104/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 63.4594 - acc: 0.5800 - val_loss: 55.0347 - val_acc: 0.5673\n",
      "Epoch 105/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.4429 - acc: 0.5803 - val_loss: 55.0091 - val_acc: 0.5672\n",
      "Epoch 106/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.3593 - acc: 0.5804 - val_loss: 54.9721 - val_acc: 0.5671\n",
      "Epoch 107/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.3319 - acc: 0.5809 - val_loss: 54.9411 - val_acc: 0.5667\n",
      "Epoch 108/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 63.3547 - acc: 0.5804 - val_loss: 54.9086 - val_acc: 0.5677\n",
      "Epoch 109/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 63.3060 - acc: 0.5814 - val_loss: 54.9136 - val_acc: 0.5680\n",
      "Epoch 110/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 63.2507 - acc: 0.5810 - val_loss: 54.8859 - val_acc: 0.5673\n",
      "Epoch 111/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 63.1965 - acc: 0.5817 - val_loss: 54.8372 - val_acc: 0.5670\n",
      "Epoch 112/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.1712 - acc: 0.5814 - val_loss: 54.8238 - val_acc: 0.5676\n",
      "Epoch 113/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 63.1453 - acc: 0.5816 - val_loss: 54.8095 - val_acc: 0.5671\n",
      "Epoch 114/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.1418 - acc: 0.5819 - val_loss: 54.7568 - val_acc: 0.5675\n",
      "Epoch 115/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 63.0950 - acc: 0.5816 - val_loss: 54.7636 - val_acc: 0.5672\n",
      "Epoch 116/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 63.0971 - acc: 0.5812 - val_loss: 54.7073 - val_acc: 0.5672\n",
      "Epoch 117/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.0327 - acc: 0.5815 - val_loss: 54.6904 - val_acc: 0.5673\n",
      "Epoch 118/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.0401 - acc: 0.5817 - val_loss: 54.6701 - val_acc: 0.5677\n",
      "Epoch 119/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.0071 - acc: 0.5817 - val_loss: 54.6612 - val_acc: 0.5673\n",
      "Epoch 120/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 63.0057 - acc: 0.5819 - val_loss: 54.6237 - val_acc: 0.5677\n",
      "Epoch 121/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 62.9617 - acc: 0.5822 - val_loss: 54.6046 - val_acc: 0.5675\n",
      "Epoch 122/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 62.9260 - acc: 0.5820 - val_loss: 54.5763 - val_acc: 0.5673\n",
      "Epoch 123/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.9063 - acc: 0.5819 - val_loss: 54.5487 - val_acc: 0.5669\n",
      "Epoch 124/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.8512 - acc: 0.5825 - val_loss: 54.5387 - val_acc: 0.5680\n",
      "Epoch 125/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.7966 - acc: 0.5823 - val_loss: 54.5121 - val_acc: 0.5684\n",
      "Epoch 126/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.7943 - acc: 0.5822 - val_loss: 54.5023 - val_acc: 0.5683\n",
      "Epoch 127/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 62.7770 - acc: 0.5828 - val_loss: 54.4698 - val_acc: 0.5682\n",
      "Epoch 128/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 62.7692 - acc: 0.5827 - val_loss: 54.4462 - val_acc: 0.5680\n",
      "Epoch 129/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.6903 - acc: 0.5830 - val_loss: 54.4189 - val_acc: 0.5681\n",
      "Epoch 130/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.6829 - acc: 0.5831 - val_loss: 54.4050 - val_acc: 0.5683\n",
      "Epoch 131/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 62.6651 - acc: 0.5829 - val_loss: 54.3604 - val_acc: 0.5682\n",
      "Epoch 132/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.6005 - acc: 0.5834 - val_loss: 54.3681 - val_acc: 0.5687\n",
      "Epoch 133/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 62.6025 - acc: 0.5833 - val_loss: 54.3366 - val_acc: 0.5687\n",
      "Epoch 134/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 62.5685 - acc: 0.5825 - val_loss: 54.3240 - val_acc: 0.5687\n",
      "Epoch 135/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.5703 - acc: 0.5833 - val_loss: 54.2822 - val_acc: 0.5688\n",
      "Epoch 136/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.5812 - acc: 0.5830 - val_loss: 54.2584 - val_acc: 0.5680\n",
      "Epoch 137/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 62.4666 - acc: 0.5835 - val_loss: 54.2533 - val_acc: 0.5690\n",
      "Epoch 138/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 62.4682 - acc: 0.5834 - val_loss: 54.2273 - val_acc: 0.5688\n",
      "Epoch 139/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 62.4291 - acc: 0.5836 - val_loss: 54.2069 - val_acc: 0.5694\n",
      "Epoch 140/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 62.4390 - acc: 0.5843 - val_loss: 54.1889 - val_acc: 0.5698\n",
      "Epoch 141/1500\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 62.3912 - acc: 0.5841 - val_loss: 54.1600 - val_acc: 0.5693\n",
      "Epoch 142/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 62.3849 - acc: 0.5835 - val_loss: 54.1498 - val_acc: 0.5693\n",
      "Epoch 143/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 62.3338 - acc: 0.5842 - val_loss: 54.1313 - val_acc: 0.5697\n",
      "Epoch 144/1500\n",
      "55/55 [==============================] - 3s 58ms/step - loss: 62.3470 - acc: 0.5838 - val_loss: 54.1332 - val_acc: 0.5695\n",
      "Epoch 145/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 62.3256 - acc: 0.5839 - val_loss: 54.0903 - val_acc: 0.5691\n",
      "Epoch 146/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.2820 - acc: 0.5843 - val_loss: 54.0824 - val_acc: 0.5698\n",
      "Epoch 147/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 62.2521 - acc: 0.5841 - val_loss: 54.0404 - val_acc: 0.5695\n",
      "Epoch 148/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.2511 - acc: 0.5847 - val_loss: 54.0506 - val_acc: 0.5697\n",
      "Epoch 149/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 62.1759 - acc: 0.5844 - val_loss: 54.0400 - val_acc: 0.5700\n",
      "Epoch 150/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 62.1865 - acc: 0.5844 - val_loss: 53.9869 - val_acc: 0.5697\n",
      "Epoch 151/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 62.1526 - acc: 0.5853 - val_loss: 53.9663 - val_acc: 0.5698\n",
      "Epoch 152/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 62.1209 - acc: 0.5850 - val_loss: 53.9378 - val_acc: 0.5691\n",
      "Epoch 153/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.1139 - acc: 0.5844 - val_loss: 53.9195 - val_acc: 0.5694\n",
      "Epoch 154/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 62.0785 - acc: 0.5845 - val_loss: 53.9208 - val_acc: 0.5691\n",
      "Epoch 155/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 62.0417 - acc: 0.5848 - val_loss: 53.8771 - val_acc: 0.5692\n",
      "Epoch 156/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 62.0363 - acc: 0.5849 - val_loss: 53.8778 - val_acc: 0.5697\n",
      "Epoch 157/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.0216 - acc: 0.5852 - val_loss: 53.8514 - val_acc: 0.5692\n",
      "Epoch 158/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 62.0136 - acc: 0.5848 - val_loss: 53.8418 - val_acc: 0.5693\n",
      "Epoch 159/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.9714 - acc: 0.5852 - val_loss: 53.8051 - val_acc: 0.5696\n",
      "Epoch 160/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.9467 - acc: 0.5856 - val_loss: 53.8061 - val_acc: 0.5695\n",
      "Epoch 161/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 61.9475 - acc: 0.5857 - val_loss: 53.7689 - val_acc: 0.5693\n",
      "Epoch 162/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 61.8852 - acc: 0.5860 - val_loss: 53.7624 - val_acc: 0.5691\n",
      "Epoch 163/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.9086 - acc: 0.5849 - val_loss: 53.7521 - val_acc: 0.5694\n",
      "Epoch 164/1500\n",
      "55/55 [==============================] - 4s 79ms/step - loss: 61.8533 - acc: 0.5854 - val_loss: 53.7329 - val_acc: 0.5690\n",
      "Epoch 165/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 61.8443 - acc: 0.5855 - val_loss: 53.7093 - val_acc: 0.5697\n",
      "Epoch 166/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 61.8327 - acc: 0.5863 - val_loss: 53.7028 - val_acc: 0.5692\n",
      "Epoch 167/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 61.7734 - acc: 0.5853 - val_loss: 53.6694 - val_acc: 0.5696\n",
      "Epoch 168/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.7627 - acc: 0.5856 - val_loss: 53.6530 - val_acc: 0.5694\n",
      "Epoch 169/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.7784 - acc: 0.5854 - val_loss: 53.6445 - val_acc: 0.5695\n",
      "Epoch 170/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 61.7525 - acc: 0.5867 - val_loss: 53.6295 - val_acc: 0.5698\n",
      "Epoch 171/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.6921 - acc: 0.5869 - val_loss: 53.6144 - val_acc: 0.5701\n",
      "Epoch 172/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 61.6849 - acc: 0.5865 - val_loss: 53.6181 - val_acc: 0.5696\n",
      "Epoch 173/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 61.6750 - acc: 0.5867 - val_loss: 53.5692 - val_acc: 0.5698\n",
      "Epoch 174/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 61.6559 - acc: 0.5865 - val_loss: 53.5526 - val_acc: 0.5697\n",
      "Epoch 175/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.6320 - acc: 0.5862 - val_loss: 53.5521 - val_acc: 0.5698\n",
      "Epoch 176/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.5602 - acc: 0.5872 - val_loss: 53.5160 - val_acc: 0.5700\n",
      "Epoch 177/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.5743 - acc: 0.5867 - val_loss: 53.5152 - val_acc: 0.5696\n",
      "Epoch 178/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 61.5817 - acc: 0.5865 - val_loss: 53.4862 - val_acc: 0.5697\n",
      "Epoch 179/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 61.5623 - acc: 0.5864 - val_loss: 53.4687 - val_acc: 0.5695\n",
      "Epoch 180/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.5489 - acc: 0.5870 - val_loss: 53.4488 - val_acc: 0.5701\n",
      "Epoch 181/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.4862 - acc: 0.5873 - val_loss: 53.4319 - val_acc: 0.5698\n",
      "Epoch 182/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.4409 - acc: 0.5869 - val_loss: 53.3995 - val_acc: 0.5696\n",
      "Epoch 183/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.4394 - acc: 0.5872 - val_loss: 53.3868 - val_acc: 0.5702\n",
      "Epoch 184/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 61.4468 - acc: 0.5874 - val_loss: 53.3884 - val_acc: 0.5702\n",
      "Epoch 185/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 61.4436 - acc: 0.5877 - val_loss: 53.3627 - val_acc: 0.5703\n",
      "Epoch 186/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.3812 - acc: 0.5875 - val_loss: 53.3502 - val_acc: 0.5703\n",
      "Epoch 187/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.3542 - acc: 0.5880 - val_loss: 53.3106 - val_acc: 0.5705\n",
      "Epoch 188/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 61.3581 - acc: 0.5881 - val_loss: 53.3070 - val_acc: 0.5707\n",
      "Epoch 189/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.3211 - acc: 0.5884 - val_loss: 53.2782 - val_acc: 0.5704\n",
      "Epoch 190/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 61.2911 - acc: 0.5881 - val_loss: 53.2586 - val_acc: 0.5706\n",
      "Epoch 191/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 61.2505 - acc: 0.5880 - val_loss: 53.2311 - val_acc: 0.5707\n",
      "Epoch 192/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.2573 - acc: 0.5882 - val_loss: 53.2308 - val_acc: 0.5712\n",
      "Epoch 193/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.2613 - acc: 0.5884 - val_loss: 53.2161 - val_acc: 0.5713\n",
      "Epoch 194/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.2138 - acc: 0.5884 - val_loss: 53.2085 - val_acc: 0.5720\n",
      "Epoch 195/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.1977 - acc: 0.5890 - val_loss: 53.1896 - val_acc: 0.5715\n",
      "Epoch 196/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 61.1898 - acc: 0.5891 - val_loss: 53.1695 - val_acc: 0.5720\n",
      "Epoch 197/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 61.1367 - acc: 0.5892 - val_loss: 53.1501 - val_acc: 0.5718\n",
      "Epoch 198/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.1289 - acc: 0.5892 - val_loss: 53.1391 - val_acc: 0.5722\n",
      "Epoch 199/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.0937 - acc: 0.5888 - val_loss: 53.1251 - val_acc: 0.5721\n",
      "Epoch 200/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.1243 - acc: 0.5895 - val_loss: 53.0852 - val_acc: 0.5717\n",
      "Epoch 201/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 61.0841 - acc: 0.5896 - val_loss: 53.0588 - val_acc: 0.5723\n",
      "Epoch 202/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 61.0544 - acc: 0.5889 - val_loss: 53.0764 - val_acc: 0.5717\n",
      "Epoch 203/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 61.0650 - acc: 0.5891 - val_loss: 53.0520 - val_acc: 0.5716\n",
      "Epoch 204/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 61.0062 - acc: 0.5898 - val_loss: 53.0452 - val_acc: 0.5723\n",
      "Epoch 205/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 61.0129 - acc: 0.5893 - val_loss: 53.0016 - val_acc: 0.5720\n",
      "Epoch 206/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.9317 - acc: 0.5895 - val_loss: 53.0103 - val_acc: 0.5724\n",
      "Epoch 207/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.9489 - acc: 0.5896 - val_loss: 52.9950 - val_acc: 0.5725\n",
      "Epoch 208/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.9225 - acc: 0.5902 - val_loss: 52.9739 - val_acc: 0.5729\n",
      "Epoch 209/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 60.9475 - acc: 0.5903 - val_loss: 52.9441 - val_acc: 0.5726\n",
      "Epoch 210/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.9085 - acc: 0.5899 - val_loss: 52.9257 - val_acc: 0.5732\n",
      "Epoch 211/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.8952 - acc: 0.5898 - val_loss: 52.9337 - val_acc: 0.5729\n",
      "Epoch 212/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.8486 - acc: 0.5904 - val_loss: 52.9247 - val_acc: 0.5730\n",
      "Epoch 213/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 60.8602 - acc: 0.5908 - val_loss: 52.8993 - val_acc: 0.5730\n",
      "Epoch 214/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.8301 - acc: 0.5908 - val_loss: 52.8440 - val_acc: 0.5730\n",
      "Epoch 215/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.7787 - acc: 0.5907 - val_loss: 52.8590 - val_acc: 0.5733\n",
      "Epoch 216/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.8045 - acc: 0.5913 - val_loss: 52.8271 - val_acc: 0.5737\n",
      "Epoch 217/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.7999 - acc: 0.5908 - val_loss: 52.8337 - val_acc: 0.5742\n",
      "Epoch 218/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.7427 - acc: 0.5913 - val_loss: 52.8102 - val_acc: 0.5742\n",
      "Epoch 219/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 60.7210 - acc: 0.5911 - val_loss: 52.7860 - val_acc: 0.5742\n",
      "Epoch 220/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.6871 - acc: 0.5909 - val_loss: 52.7611 - val_acc: 0.5736\n",
      "Epoch 221/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.6997 - acc: 0.5914 - val_loss: 52.7822 - val_acc: 0.5744\n",
      "Epoch 222/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.6801 - acc: 0.5915 - val_loss: 52.7570 - val_acc: 0.5742\n",
      "Epoch 223/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.6391 - acc: 0.5916 - val_loss: 52.7273 - val_acc: 0.5738\n",
      "Epoch 224/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.6275 - acc: 0.5915 - val_loss: 52.7302 - val_acc: 0.5745\n",
      "Epoch 225/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 60.6066 - acc: 0.5923 - val_loss: 52.7133 - val_acc: 0.5748\n",
      "Epoch 226/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 60.6140 - acc: 0.5920 - val_loss: 52.6976 - val_acc: 0.5745\n",
      "Epoch 227/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 60.5777 - acc: 0.5922 - val_loss: 52.6743 - val_acc: 0.5745\n",
      "Epoch 228/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.5375 - acc: 0.5922 - val_loss: 52.6632 - val_acc: 0.5751\n",
      "Epoch 229/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.5468 - acc: 0.5919 - val_loss: 52.6438 - val_acc: 0.5746\n",
      "Epoch 230/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.4841 - acc: 0.5921 - val_loss: 52.6320 - val_acc: 0.5748\n",
      "Epoch 231/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 60.5171 - acc: 0.5929 - val_loss: 52.6187 - val_acc: 0.5757\n",
      "Epoch 232/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.4670 - acc: 0.5928 - val_loss: 52.5961 - val_acc: 0.5758\n",
      "Epoch 233/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.4337 - acc: 0.5927 - val_loss: 52.5884 - val_acc: 0.5749\n",
      "Epoch 234/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.3989 - acc: 0.5927 - val_loss: 52.5598 - val_acc: 0.5761\n",
      "Epoch 235/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.4087 - acc: 0.5928 - val_loss: 52.5393 - val_acc: 0.5759\n",
      "Epoch 236/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.4062 - acc: 0.5931 - val_loss: 52.5334 - val_acc: 0.5762\n",
      "Epoch 237/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.4024 - acc: 0.5932 - val_loss: 52.5046 - val_acc: 0.5758\n",
      "Epoch 238/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 60.3507 - acc: 0.5932 - val_loss: 52.5225 - val_acc: 0.5765\n",
      "Epoch 239/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.3653 - acc: 0.5933 - val_loss: 52.4987 - val_acc: 0.5764\n",
      "Epoch 240/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.3662 - acc: 0.5935 - val_loss: 52.4907 - val_acc: 0.5764\n",
      "Epoch 241/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.2588 - acc: 0.5939 - val_loss: 52.4649 - val_acc: 0.5774\n",
      "Epoch 242/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.2970 - acc: 0.5939 - val_loss: 52.4562 - val_acc: 0.5767\n",
      "Epoch 243/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.2837 - acc: 0.5938 - val_loss: 52.4522 - val_acc: 0.5771\n",
      "Epoch 244/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 60.2470 - acc: 0.5939 - val_loss: 52.4237 - val_acc: 0.5772\n",
      "Epoch 245/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.2320 - acc: 0.5937 - val_loss: 52.4157 - val_acc: 0.5771\n",
      "Epoch 246/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 60.2377 - acc: 0.5941 - val_loss: 52.3742 - val_acc: 0.5776\n",
      "Epoch 247/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.1898 - acc: 0.5942 - val_loss: 52.3859 - val_acc: 0.5772\n",
      "Epoch 248/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.1776 - acc: 0.5937 - val_loss: 52.3835 - val_acc: 0.5775\n",
      "Epoch 249/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.1512 - acc: 0.5946 - val_loss: 52.3669 - val_acc: 0.5776\n",
      "Epoch 250/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 60.1353 - acc: 0.5945 - val_loss: 52.3378 - val_acc: 0.5782\n",
      "Epoch 251/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.1007 - acc: 0.5944 - val_loss: 52.3347 - val_acc: 0.5781\n",
      "Epoch 252/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.1437 - acc: 0.5946 - val_loss: 52.3052 - val_acc: 0.5780\n",
      "Epoch 253/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.1074 - acc: 0.5950 - val_loss: 52.2867 - val_acc: 0.5782\n",
      "Epoch 254/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 60.0716 - acc: 0.5948 - val_loss: 52.3075 - val_acc: 0.5789\n",
      "Epoch 255/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 60.0446 - acc: 0.5949 - val_loss: 52.3089 - val_acc: 0.5786\n",
      "Epoch 256/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 60.0446 - acc: 0.5952 - val_loss: 52.2813 - val_acc: 0.5782\n",
      "Epoch 257/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 59.9875 - acc: 0.5949 - val_loss: 52.2508 - val_acc: 0.5790\n",
      "Epoch 258/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 60.0021 - acc: 0.5957 - val_loss: 52.2325 - val_acc: 0.5788\n",
      "Epoch 259/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 59.9886 - acc: 0.5956 - val_loss: 52.2231 - val_acc: 0.5795\n",
      "Epoch 260/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 59.9711 - acc: 0.5960 - val_loss: 52.2155 - val_acc: 0.5798\n",
      "Epoch 261/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 59.9368 - acc: 0.5958 - val_loss: 52.1738 - val_acc: 0.5795\n",
      "Epoch 262/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 59.9418 - acc: 0.5958 - val_loss: 52.1797 - val_acc: 0.5801\n",
      "Epoch 263/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 59.9284 - acc: 0.5961 - val_loss: 52.1821 - val_acc: 0.5790\n",
      "Epoch 264/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 59.8681 - acc: 0.5959 - val_loss: 52.1656 - val_acc: 0.5801\n",
      "Epoch 265/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 59.8868 - acc: 0.5962 - val_loss: 52.1503 - val_acc: 0.5796\n",
      "Epoch 266/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.8727 - acc: 0.5962 - val_loss: 52.1274 - val_acc: 0.5800\n",
      "Epoch 267/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 59.8196 - acc: 0.5965 - val_loss: 52.1194 - val_acc: 0.5805\n",
      "Epoch 268/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.8316 - acc: 0.5968 - val_loss: 52.0984 - val_acc: 0.5802\n",
      "Epoch 269/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.7718 - acc: 0.5967 - val_loss: 52.1009 - val_acc: 0.5807\n",
      "Epoch 270/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.7863 - acc: 0.5967 - val_loss: 52.0709 - val_acc: 0.5804\n",
      "Epoch 271/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.7859 - acc: 0.5962 - val_loss: 52.0743 - val_acc: 0.5810\n",
      "Epoch 272/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 59.7500 - acc: 0.5966 - val_loss: 52.0549 - val_acc: 0.5808\n",
      "Epoch 273/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.7619 - acc: 0.5960 - val_loss: 52.0486 - val_acc: 0.5810\n",
      "Epoch 274/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.6884 - acc: 0.5969 - val_loss: 52.0498 - val_acc: 0.5811\n",
      "Epoch 275/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.6772 - acc: 0.5973 - val_loss: 52.0090 - val_acc: 0.5813\n",
      "Epoch 276/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.6950 - acc: 0.5972 - val_loss: 52.0121 - val_acc: 0.5813\n",
      "Epoch 277/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.6581 - acc: 0.5972 - val_loss: 51.9781 - val_acc: 0.5817\n",
      "Epoch 278/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 59.6351 - acc: 0.5974 - val_loss: 51.9911 - val_acc: 0.5816\n",
      "Epoch 279/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.6492 - acc: 0.5971 - val_loss: 51.9820 - val_acc: 0.5819\n",
      "Epoch 280/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.5787 - acc: 0.5974 - val_loss: 51.9646 - val_acc: 0.5819\n",
      "Epoch 281/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.5579 - acc: 0.5972 - val_loss: 51.9399 - val_acc: 0.5822\n",
      "Epoch 282/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.5306 - acc: 0.5978 - val_loss: 51.9570 - val_acc: 0.5825\n",
      "Epoch 283/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 59.5573 - acc: 0.5982 - val_loss: 51.9290 - val_acc: 0.5826\n",
      "Epoch 284/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 59.5418 - acc: 0.5975 - val_loss: 51.8867 - val_acc: 0.5824\n",
      "Epoch 285/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.5476 - acc: 0.5980 - val_loss: 51.9061 - val_acc: 0.5822\n",
      "Epoch 286/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.5474 - acc: 0.5978 - val_loss: 51.8731 - val_acc: 0.5826\n",
      "Epoch 287/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.4958 - acc: 0.5980 - val_loss: 51.8829 - val_acc: 0.5829\n",
      "Epoch 288/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.4970 - acc: 0.5980 - val_loss: 51.8883 - val_acc: 0.5832\n",
      "Epoch 289/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 59.4094 - acc: 0.5984 - val_loss: 51.8602 - val_acc: 0.5832\n",
      "Epoch 290/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.4470 - acc: 0.5983 - val_loss: 51.8463 - val_acc: 0.5834\n",
      "Epoch 291/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.4394 - acc: 0.5991 - val_loss: 51.8549 - val_acc: 0.5835\n",
      "Epoch 292/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.3791 - acc: 0.5987 - val_loss: 51.8195 - val_acc: 0.5837\n",
      "Epoch 293/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.3839 - acc: 0.5989 - val_loss: 51.8020 - val_acc: 0.5834\n",
      "Epoch 294/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 59.3504 - acc: 0.5980 - val_loss: 51.8255 - val_acc: 0.5834\n",
      "Epoch 295/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 59.3354 - acc: 0.5988 - val_loss: 51.7859 - val_acc: 0.5836\n",
      "Epoch 296/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.3047 - acc: 0.5986 - val_loss: 51.7859 - val_acc: 0.5836\n",
      "Epoch 297/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.3104 - acc: 0.5991 - val_loss: 51.7905 - val_acc: 0.5844\n",
      "Epoch 298/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.3176 - acc: 0.5994 - val_loss: 51.7560 - val_acc: 0.5839\n",
      "Epoch 299/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.2891 - acc: 0.5982 - val_loss: 51.7579 - val_acc: 0.5839\n",
      "Epoch 300/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 59.2485 - acc: 0.5997 - val_loss: 51.7309 - val_acc: 0.5843\n",
      "Epoch 301/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.2676 - acc: 0.5992 - val_loss: 51.7351 - val_acc: 0.5839\n",
      "Epoch 302/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.2251 - acc: 0.5995 - val_loss: 51.7155 - val_acc: 0.5847\n",
      "Epoch 303/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.1769 - acc: 0.5999 - val_loss: 51.7028 - val_acc: 0.5849\n",
      "Epoch 304/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.1852 - acc: 0.5997 - val_loss: 51.6926 - val_acc: 0.5847\n",
      "Epoch 305/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 59.1648 - acc: 0.5995 - val_loss: 51.6942 - val_acc: 0.5847\n",
      "Epoch 306/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 59.1200 - acc: 0.6001 - val_loss: 51.6856 - val_acc: 0.5845\n",
      "Epoch 307/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.1359 - acc: 0.5998 - val_loss: 51.6517 - val_acc: 0.5852\n",
      "Epoch 308/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.1561 - acc: 0.6000 - val_loss: 51.6602 - val_acc: 0.5845\n",
      "Epoch 309/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.1073 - acc: 0.6004 - val_loss: 51.6302 - val_acc: 0.5847\n",
      "Epoch 310/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 59.0649 - acc: 0.6000 - val_loss: 51.6294 - val_acc: 0.5847\n",
      "Epoch 311/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 59.0361 - acc: 0.6001 - val_loss: 51.6245 - val_acc: 0.5851\n",
      "Epoch 312/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 59.0823 - acc: 0.5996 - val_loss: 51.6070 - val_acc: 0.5851\n",
      "Epoch 313/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 59.0289 - acc: 0.6009 - val_loss: 51.5957 - val_acc: 0.5850\n",
      "Epoch 314/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 59.0286 - acc: 0.6003 - val_loss: 51.5978 - val_acc: 0.5856\n",
      "Epoch 315/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.9734 - acc: 0.6000 - val_loss: 51.5807 - val_acc: 0.5852\n",
      "Epoch 316/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 58.9781 - acc: 0.5998 - val_loss: 51.5854 - val_acc: 0.5857\n",
      "Epoch 317/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 58.9241 - acc: 0.6001 - val_loss: 51.5446 - val_acc: 0.5852\n",
      "Epoch 318/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.9716 - acc: 0.6011 - val_loss: 51.5401 - val_acc: 0.5852\n",
      "Epoch 319/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.9162 - acc: 0.6005 - val_loss: 51.5152 - val_acc: 0.5851\n",
      "Epoch 320/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.8950 - acc: 0.6008 - val_loss: 51.5131 - val_acc: 0.5853\n",
      "Epoch 321/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 58.8754 - acc: 0.6000 - val_loss: 51.5271 - val_acc: 0.5858\n",
      "Epoch 322/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 58.8864 - acc: 0.6007 - val_loss: 51.5040 - val_acc: 0.5856\n",
      "Epoch 323/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.8445 - acc: 0.6009 - val_loss: 51.4962 - val_acc: 0.5866\n",
      "Epoch 324/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.8672 - acc: 0.6012 - val_loss: 51.5030 - val_acc: 0.5857\n",
      "Epoch 325/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.8372 - acc: 0.6005 - val_loss: 51.4908 - val_acc: 0.5864\n",
      "Epoch 326/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.8119 - acc: 0.6008 - val_loss: 51.4774 - val_acc: 0.5861\n",
      "Epoch 327/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 58.7980 - acc: 0.6010 - val_loss: 51.4598 - val_acc: 0.5862\n",
      "Epoch 328/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 58.8101 - acc: 0.6010 - val_loss: 51.4561 - val_acc: 0.5862\n",
      "Epoch 329/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.7027 - acc: 0.6005 - val_loss: 51.4382 - val_acc: 0.5859\n",
      "Epoch 330/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.7382 - acc: 0.6012 - val_loss: 51.4134 - val_acc: 0.5869\n",
      "Epoch 331/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.7532 - acc: 0.6015 - val_loss: 51.4153 - val_acc: 0.5865\n",
      "Epoch 332/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.7093 - acc: 0.6017 - val_loss: 51.4126 - val_acc: 0.5861\n",
      "Epoch 333/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 58.7165 - acc: 0.6014 - val_loss: 51.4230 - val_acc: 0.5871\n",
      "Epoch 334/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.6471 - acc: 0.6017 - val_loss: 51.3964 - val_acc: 0.5865\n",
      "Epoch 335/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.6701 - acc: 0.6015 - val_loss: 51.3866 - val_acc: 0.5868\n",
      "Epoch 336/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 58.6241 - acc: 0.6021 - val_loss: 51.3515 - val_acc: 0.5867\n",
      "Epoch 337/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.6372 - acc: 0.6018 - val_loss: 51.3433 - val_acc: 0.5866\n",
      "Epoch 338/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 58.5814 - acc: 0.6017 - val_loss: 51.3569 - val_acc: 0.5866\n",
      "Epoch 339/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 58.5958 - acc: 0.6020 - val_loss: 51.3602 - val_acc: 0.5872\n",
      "Epoch 340/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.5870 - acc: 0.6017 - val_loss: 51.3487 - val_acc: 0.5871\n",
      "Epoch 341/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.6138 - acc: 0.6020 - val_loss: 51.3384 - val_acc: 0.5873\n",
      "Epoch 342/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.5498 - acc: 0.6028 - val_loss: 51.3223 - val_acc: 0.5877\n",
      "Epoch 343/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.5206 - acc: 0.6025 - val_loss: 51.3060 - val_acc: 0.5873\n",
      "Epoch 344/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.4999 - acc: 0.6026 - val_loss: 51.3037 - val_acc: 0.5872\n",
      "Epoch 345/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 58.5014 - acc: 0.6031 - val_loss: 51.2926 - val_acc: 0.5877\n",
      "Epoch 346/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.4865 - acc: 0.6033 - val_loss: 51.2644 - val_acc: 0.5876\n",
      "Epoch 347/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.4746 - acc: 0.6032 - val_loss: 51.2735 - val_acc: 0.5882\n",
      "Epoch 348/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.4378 - acc: 0.6031 - val_loss: 51.2797 - val_acc: 0.5872\n",
      "Epoch 349/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.4469 - acc: 0.6026 - val_loss: 51.2413 - val_acc: 0.5869\n",
      "Epoch 350/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.4190 - acc: 0.6027 - val_loss: 51.2310 - val_acc: 0.5874\n",
      "Epoch 351/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 58.3825 - acc: 0.6032 - val_loss: 51.2275 - val_acc: 0.5877\n",
      "Epoch 352/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.3858 - acc: 0.6029 - val_loss: 51.2274 - val_acc: 0.5870\n",
      "Epoch 353/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.3634 - acc: 0.6026 - val_loss: 51.2233 - val_acc: 0.5874\n",
      "Epoch 354/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.3144 - acc: 0.6028 - val_loss: 51.2014 - val_acc: 0.5878\n",
      "Epoch 355/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.3898 - acc: 0.6031 - val_loss: 51.1933 - val_acc: 0.5885\n",
      "Epoch 356/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 58.3514 - acc: 0.6032 - val_loss: 51.1884 - val_acc: 0.5878\n",
      "Epoch 357/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 58.3139 - acc: 0.6031 - val_loss: 51.1730 - val_acc: 0.5880\n",
      "Epoch 358/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.2896 - acc: 0.6032 - val_loss: 51.1572 - val_acc: 0.5885\n",
      "Epoch 359/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.2429 - acc: 0.6036 - val_loss: 51.1569 - val_acc: 0.5880\n",
      "Epoch 360/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 58.2193 - acc: 0.6028 - val_loss: 51.1384 - val_acc: 0.5881\n",
      "Epoch 361/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 58.2202 - acc: 0.6034 - val_loss: 51.1160 - val_acc: 0.5887\n",
      "Epoch 362/1500\n",
      "55/55 [==============================] - 3s 59ms/step - loss: 58.2307 - acc: 0.6033 - val_loss: 51.1297 - val_acc: 0.5882\n",
      "Epoch 363/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 58.2403 - acc: 0.6035 - val_loss: 51.1250 - val_acc: 0.5885\n",
      "Epoch 364/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.1800 - acc: 0.6036 - val_loss: 51.0995 - val_acc: 0.5885\n",
      "Epoch 365/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.1822 - acc: 0.6029 - val_loss: 51.1175 - val_acc: 0.5890\n",
      "Epoch 366/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 58.1705 - acc: 0.6043 - val_loss: 51.1102 - val_acc: 0.5891\n",
      "Epoch 367/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 58.1421 - acc: 0.6038 - val_loss: 51.0944 - val_acc: 0.5886\n",
      "Epoch 368/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 58.1213 - acc: 0.6041 - val_loss: 51.0721 - val_acc: 0.5884\n",
      "Epoch 369/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.1536 - acc: 0.6030 - val_loss: 51.0749 - val_acc: 0.5883\n",
      "Epoch 370/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 58.1213 - acc: 0.6038 - val_loss: 51.0935 - val_acc: 0.5886\n",
      "Epoch 371/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.0904 - acc: 0.6040 - val_loss: 51.0414 - val_acc: 0.5880\n",
      "Epoch 372/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 58.0780 - acc: 0.6040 - val_loss: 51.0671 - val_acc: 0.5888\n",
      "Epoch 373/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 58.0385 - acc: 0.6044 - val_loss: 51.0296 - val_acc: 0.5887\n",
      "Epoch 374/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 58.0201 - acc: 0.6042 - val_loss: 51.0259 - val_acc: 0.5893\n",
      "Epoch 375/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 58.0113 - acc: 0.6044 - val_loss: 51.0180 - val_acc: 0.5887\n",
      "Epoch 376/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.9822 - acc: 0.6043 - val_loss: 51.0139 - val_acc: 0.5888\n",
      "Epoch 377/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 58.0120 - acc: 0.6040 - val_loss: 51.0020 - val_acc: 0.5890\n",
      "Epoch 378/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.9642 - acc: 0.6048 - val_loss: 51.0042 - val_acc: 0.5887\n",
      "Epoch 379/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.9705 - acc: 0.6041 - val_loss: 50.9803 - val_acc: 0.5886\n",
      "Epoch 380/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.9449 - acc: 0.6043 - val_loss: 50.9755 - val_acc: 0.5894\n",
      "Epoch 381/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.9322 - acc: 0.6048 - val_loss: 50.9899 - val_acc: 0.5895\n",
      "Epoch 382/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.9118 - acc: 0.6042 - val_loss: 50.9740 - val_acc: 0.5897\n",
      "Epoch 383/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.9194 - acc: 0.6047 - val_loss: 50.9703 - val_acc: 0.5898\n",
      "Epoch 384/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.8642 - acc: 0.6050 - val_loss: 50.9575 - val_acc: 0.5898\n",
      "Epoch 385/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.8759 - acc: 0.6047 - val_loss: 50.9595 - val_acc: 0.5899\n",
      "Epoch 386/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 57.8521 - acc: 0.6049 - val_loss: 50.9453 - val_acc: 0.5895\n",
      "Epoch 387/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.8447 - acc: 0.6044 - val_loss: 50.9311 - val_acc: 0.5895\n",
      "Epoch 388/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.8531 - acc: 0.6052 - val_loss: 50.9081 - val_acc: 0.5892\n",
      "Epoch 389/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.8396 - acc: 0.6047 - val_loss: 50.9200 - val_acc: 0.5908\n",
      "Epoch 390/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.8001 - acc: 0.6059 - val_loss: 50.9232 - val_acc: 0.5892\n",
      "Epoch 391/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.7827 - acc: 0.6043 - val_loss: 50.9050 - val_acc: 0.5897\n",
      "Epoch 392/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.7446 - acc: 0.6051 - val_loss: 50.8975 - val_acc: 0.5897\n",
      "Epoch 393/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.6993 - acc: 0.6057 - val_loss: 50.8767 - val_acc: 0.5899\n",
      "Epoch 394/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.7636 - acc: 0.6052 - val_loss: 50.8794 - val_acc: 0.5894\n",
      "Epoch 395/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.7209 - acc: 0.6045 - val_loss: 50.8715 - val_acc: 0.5898\n",
      "Epoch 396/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 57.7264 - acc: 0.6050 - val_loss: 50.8824 - val_acc: 0.5904\n",
      "Epoch 397/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 57.6240 - acc: 0.6059 - val_loss: 50.8463 - val_acc: 0.5907\n",
      "Epoch 398/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.6940 - acc: 0.6057 - val_loss: 50.8441 - val_acc: 0.5905\n",
      "Epoch 399/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.6688 - acc: 0.6058 - val_loss: 50.8239 - val_acc: 0.5902\n",
      "Epoch 400/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.6300 - acc: 0.6060 - val_loss: 50.8478 - val_acc: 0.5904\n",
      "Epoch 401/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.5918 - acc: 0.6056 - val_loss: 50.8561 - val_acc: 0.5906\n",
      "Epoch 402/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 57.5738 - acc: 0.6055 - val_loss: 50.8190 - val_acc: 0.5908\n",
      "Epoch 403/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.6318 - acc: 0.6063 - val_loss: 50.8256 - val_acc: 0.5907\n",
      "Epoch 404/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.5942 - acc: 0.6053 - val_loss: 50.8121 - val_acc: 0.5902\n",
      "Epoch 405/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.6216 - acc: 0.6062 - val_loss: 50.8133 - val_acc: 0.5908\n",
      "Epoch 406/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.5710 - acc: 0.6061 - val_loss: 50.7965 - val_acc: 0.5903\n",
      "Epoch 407/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.5494 - acc: 0.6061 - val_loss: 50.7990 - val_acc: 0.5902\n",
      "Epoch 408/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.5327 - acc: 0.6059 - val_loss: 50.7854 - val_acc: 0.5902\n",
      "Epoch 409/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.5007 - acc: 0.6063 - val_loss: 50.7765 - val_acc: 0.5896\n",
      "Epoch 410/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.5051 - acc: 0.6057 - val_loss: 50.7679 - val_acc: 0.5901\n",
      "Epoch 411/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.4503 - acc: 0.6065 - val_loss: 50.7566 - val_acc: 0.5903\n",
      "Epoch 412/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.4534 - acc: 0.6060 - val_loss: 50.7634 - val_acc: 0.5905\n",
      "Epoch 413/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 57.4540 - acc: 0.6061 - val_loss: 50.7455 - val_acc: 0.5902\n",
      "Epoch 414/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.4075 - acc: 0.6061 - val_loss: 50.7223 - val_acc: 0.5909\n",
      "Epoch 415/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.4416 - acc: 0.6065 - val_loss: 50.7293 - val_acc: 0.5904\n",
      "Epoch 416/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.3910 - acc: 0.6068 - val_loss: 50.7167 - val_acc: 0.5907\n",
      "Epoch 417/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.4234 - acc: 0.6060 - val_loss: 50.7293 - val_acc: 0.5908\n",
      "Epoch 418/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.3935 - acc: 0.6064 - val_loss: 50.7174 - val_acc: 0.5912\n",
      "Epoch 419/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.4064 - acc: 0.6066 - val_loss: 50.7255 - val_acc: 0.5908\n",
      "Epoch 420/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 57.3357 - acc: 0.6066 - val_loss: 50.7178 - val_acc: 0.5912\n",
      "Epoch 421/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.3604 - acc: 0.6065 - val_loss: 50.6932 - val_acc: 0.5908\n",
      "Epoch 422/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.3017 - acc: 0.6066 - val_loss: 50.6835 - val_acc: 0.5910\n",
      "Epoch 423/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.3395 - acc: 0.6072 - val_loss: 50.6813 - val_acc: 0.5907\n",
      "Epoch 424/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.3287 - acc: 0.6065 - val_loss: 50.6737 - val_acc: 0.5906\n",
      "Epoch 425/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.3369 - acc: 0.6070 - val_loss: 50.6581 - val_acc: 0.5914\n",
      "Epoch 426/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.2904 - acc: 0.6069 - val_loss: 50.6708 - val_acc: 0.5911\n",
      "Epoch 427/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.2610 - acc: 0.6073 - val_loss: 50.6730 - val_acc: 0.5915\n",
      "Epoch 428/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.2390 - acc: 0.6075 - val_loss: 50.6535 - val_acc: 0.5911\n",
      "Epoch 429/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.2341 - acc: 0.6071 - val_loss: 50.6436 - val_acc: 0.5924\n",
      "Epoch 430/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.2418 - acc: 0.6068 - val_loss: 50.6402 - val_acc: 0.5910\n",
      "Epoch 431/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.2308 - acc: 0.6070 - val_loss: 50.6450 - val_acc: 0.5912\n",
      "Epoch 432/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.2166 - acc: 0.6076 - val_loss: 50.6319 - val_acc: 0.5908\n",
      "Epoch 433/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.1962 - acc: 0.6075 - val_loss: 50.6285 - val_acc: 0.5910\n",
      "Epoch 434/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.1615 - acc: 0.6070 - val_loss: 50.6167 - val_acc: 0.5915\n",
      "Epoch 435/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.1631 - acc: 0.6073 - val_loss: 50.6360 - val_acc: 0.5915\n",
      "Epoch 436/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.1956 - acc: 0.6069 - val_loss: 50.6170 - val_acc: 0.5909\n",
      "Epoch 437/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.1520 - acc: 0.6076 - val_loss: 50.6265 - val_acc: 0.5913\n",
      "Epoch 438/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 57.0706 - acc: 0.6076 - val_loss: 50.5873 - val_acc: 0.5913\n",
      "Epoch 439/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.1139 - acc: 0.6075 - val_loss: 50.5712 - val_acc: 0.5916\n",
      "Epoch 440/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.0771 - acc: 0.6077 - val_loss: 50.5711 - val_acc: 0.5910\n",
      "Epoch 441/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.1071 - acc: 0.6076 - val_loss: 50.5918 - val_acc: 0.5922\n",
      "Epoch 442/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 57.0562 - acc: 0.6076 - val_loss: 50.5683 - val_acc: 0.5910\n",
      "Epoch 443/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 57.0868 - acc: 0.6076 - val_loss: 50.5730 - val_acc: 0.5917\n",
      "Epoch 444/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 57.0184 - acc: 0.6082 - val_loss: 50.5569 - val_acc: 0.5920\n",
      "Epoch 445/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 57.0654 - acc: 0.6077 - val_loss: 50.5479 - val_acc: 0.5918\n",
      "Epoch 446/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 57.0477 - acc: 0.6078 - val_loss: 50.5545 - val_acc: 0.5917\n",
      "Epoch 447/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.9926 - acc: 0.6081 - val_loss: 50.5430 - val_acc: 0.5924\n",
      "Epoch 448/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.9891 - acc: 0.6080 - val_loss: 50.5236 - val_acc: 0.5915\n",
      "Epoch 449/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.9747 - acc: 0.6078 - val_loss: 50.5062 - val_acc: 0.5925\n",
      "Epoch 450/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.9670 - acc: 0.6084 - val_loss: 50.5023 - val_acc: 0.5922\n",
      "Epoch 451/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.9757 - acc: 0.6079 - val_loss: 50.5100 - val_acc: 0.5914\n",
      "Epoch 452/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.9421 - acc: 0.6078 - val_loss: 50.5026 - val_acc: 0.5921\n",
      "Epoch 453/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.8729 - acc: 0.6088 - val_loss: 50.4939 - val_acc: 0.5928\n",
      "Epoch 454/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.8968 - acc: 0.6094 - val_loss: 50.4889 - val_acc: 0.5917\n",
      "Epoch 455/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.9087 - acc: 0.6081 - val_loss: 50.4791 - val_acc: 0.5927\n",
      "Epoch 456/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.8948 - acc: 0.6087 - val_loss: 50.4912 - val_acc: 0.5929\n",
      "Epoch 457/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.8577 - acc: 0.6091 - val_loss: 50.4622 - val_acc: 0.5924\n",
      "Epoch 458/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.8544 - acc: 0.6083 - val_loss: 50.4503 - val_acc: 0.5923\n",
      "Epoch 459/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.8438 - acc: 0.6092 - val_loss: 50.4786 - val_acc: 0.5923\n",
      "Epoch 460/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.8103 - acc: 0.6084 - val_loss: 50.5008 - val_acc: 0.5922\n",
      "Epoch 461/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 56.8416 - acc: 0.6082 - val_loss: 50.4420 - val_acc: 0.5921\n",
      "Epoch 462/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.8439 - acc: 0.6084 - val_loss: 50.4518 - val_acc: 0.5926\n",
      "Epoch 463/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.8172 - acc: 0.6091 - val_loss: 50.4687 - val_acc: 0.5921\n",
      "Epoch 464/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.7681 - acc: 0.6083 - val_loss: 50.4408 - val_acc: 0.5923\n",
      "Epoch 465/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.7636 - acc: 0.6088 - val_loss: 50.4353 - val_acc: 0.5921\n",
      "Epoch 466/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 56.7410 - acc: 0.6085 - val_loss: 50.4299 - val_acc: 0.5921\n",
      "Epoch 467/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.7556 - acc: 0.6084 - val_loss: 50.4256 - val_acc: 0.5922\n",
      "Epoch 468/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.7050 - acc: 0.6090 - val_loss: 50.4296 - val_acc: 0.5926\n",
      "Epoch 469/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.7068 - acc: 0.6090 - val_loss: 50.4141 - val_acc: 0.5927\n",
      "Epoch 470/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.6837 - acc: 0.6088 - val_loss: 50.4210 - val_acc: 0.5931\n",
      "Epoch 471/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.7206 - acc: 0.6095 - val_loss: 50.4153 - val_acc: 0.5928\n",
      "Epoch 472/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.6531 - acc: 0.6093 - val_loss: 50.3847 - val_acc: 0.5930\n",
      "Epoch 473/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 56.6844 - acc: 0.6095 - val_loss: 50.4003 - val_acc: 0.5929\n",
      "Epoch 474/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.6899 - acc: 0.6091 - val_loss: 50.3974 - val_acc: 0.5928\n",
      "Epoch 475/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.6589 - acc: 0.6092 - val_loss: 50.3981 - val_acc: 0.5927\n",
      "Epoch 476/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.6488 - acc: 0.6091 - val_loss: 50.3992 - val_acc: 0.5931\n",
      "Epoch 477/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.6064 - acc: 0.6092 - val_loss: 50.3766 - val_acc: 0.5925\n",
      "Epoch 478/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 56.6235 - acc: 0.6098 - val_loss: 50.3729 - val_acc: 0.5933\n",
      "Epoch 479/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.6117 - acc: 0.6096 - val_loss: 50.3745 - val_acc: 0.5934\n",
      "Epoch 480/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.5917 - acc: 0.6097 - val_loss: 50.3715 - val_acc: 0.5933\n",
      "Epoch 481/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.5162 - acc: 0.6099 - val_loss: 50.3577 - val_acc: 0.5932\n",
      "Epoch 482/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.5672 - acc: 0.6096 - val_loss: 50.3531 - val_acc: 0.5934\n",
      "Epoch 483/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.5485 - acc: 0.6096 - val_loss: 50.3461 - val_acc: 0.5928\n",
      "Epoch 484/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.5346 - acc: 0.6097 - val_loss: 50.3450 - val_acc: 0.5931\n",
      "Epoch 485/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.5448 - acc: 0.6097 - val_loss: 50.3411 - val_acc: 0.5926\n",
      "Epoch 486/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.5382 - acc: 0.6095 - val_loss: 50.3508 - val_acc: 0.5931\n",
      "Epoch 487/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.5033 - acc: 0.6095 - val_loss: 50.3465 - val_acc: 0.5928\n",
      "Epoch 488/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.4553 - acc: 0.6104 - val_loss: 50.3288 - val_acc: 0.5935\n",
      "Epoch 489/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.4665 - acc: 0.6102 - val_loss: 50.3203 - val_acc: 0.5930\n",
      "Epoch 490/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 56.4636 - acc: 0.6100 - val_loss: 50.3302 - val_acc: 0.5928\n",
      "Epoch 491/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.4254 - acc: 0.6102 - val_loss: 50.3045 - val_acc: 0.5937\n",
      "Epoch 492/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.4317 - acc: 0.6103 - val_loss: 50.3086 - val_acc: 0.5933\n",
      "Epoch 493/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.4723 - acc: 0.6102 - val_loss: 50.3020 - val_acc: 0.5936\n",
      "Epoch 494/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.4431 - acc: 0.6099 - val_loss: 50.2909 - val_acc: 0.5940\n",
      "Epoch 495/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.4042 - acc: 0.6100 - val_loss: 50.2905 - val_acc: 0.5937\n",
      "Epoch 496/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.4104 - acc: 0.6103 - val_loss: 50.2695 - val_acc: 0.5932\n",
      "Epoch 497/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.3996 - acc: 0.6105 - val_loss: 50.3035 - val_acc: 0.5932\n",
      "Epoch 498/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.3869 - acc: 0.6107 - val_loss: 50.2745 - val_acc: 0.5934\n",
      "Epoch 499/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.3976 - acc: 0.6104 - val_loss: 50.2721 - val_acc: 0.5931\n",
      "Epoch 500/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.3740 - acc: 0.6104 - val_loss: 50.2704 - val_acc: 0.5934\n",
      "Epoch 501/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.3090 - acc: 0.6101 - val_loss: 50.2798 - val_acc: 0.5934\n",
      "Epoch 502/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.2935 - acc: 0.6107 - val_loss: 50.2751 - val_acc: 0.5937\n",
      "Epoch 503/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.3067 - acc: 0.6105 - val_loss: 50.2940 - val_acc: 0.5939\n",
      "Epoch 504/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.2805 - acc: 0.6105 - val_loss: 50.2582 - val_acc: 0.5938\n",
      "Epoch 505/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.2867 - acc: 0.6108 - val_loss: 50.2350 - val_acc: 0.5930\n",
      "Epoch 506/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.2677 - acc: 0.6103 - val_loss: 50.2408 - val_acc: 0.5939\n",
      "Epoch 507/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.2568 - acc: 0.6107 - val_loss: 50.2592 - val_acc: 0.5937\n",
      "Epoch 508/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.2442 - acc: 0.6107 - val_loss: 50.2528 - val_acc: 0.5942\n",
      "Epoch 509/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.2259 - acc: 0.6110 - val_loss: 50.2244 - val_acc: 0.5939\n",
      "Epoch 510/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 56.2375 - acc: 0.6107 - val_loss: 50.2347 - val_acc: 0.5935\n",
      "Epoch 511/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 56.2094 - acc: 0.6105 - val_loss: 50.2080 - val_acc: 0.5940\n",
      "Epoch 512/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.2165 - acc: 0.6109 - val_loss: 50.2035 - val_acc: 0.5941\n",
      "Epoch 513/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.1925 - acc: 0.6108 - val_loss: 50.2217 - val_acc: 0.5940\n",
      "Epoch 514/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 56.2196 - acc: 0.6104 - val_loss: 50.2026 - val_acc: 0.5941\n",
      "Epoch 515/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.2117 - acc: 0.6109 - val_loss: 50.2042 - val_acc: 0.5936\n",
      "Epoch 516/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.1782 - acc: 0.6103 - val_loss: 50.1899 - val_acc: 0.5940\n",
      "Epoch 517/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.1872 - acc: 0.6111 - val_loss: 50.2212 - val_acc: 0.5936\n",
      "Epoch 518/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.1413 - acc: 0.6114 - val_loss: 50.1744 - val_acc: 0.5941\n",
      "Epoch 519/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.1386 - acc: 0.6113 - val_loss: 50.1853 - val_acc: 0.5941\n",
      "Epoch 520/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.1082 - acc: 0.6107 - val_loss: 50.1726 - val_acc: 0.5939\n",
      "Epoch 521/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.1236 - acc: 0.6116 - val_loss: 50.1817 - val_acc: 0.5938\n",
      "Epoch 522/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.0939 - acc: 0.6120 - val_loss: 50.1783 - val_acc: 0.5942\n",
      "Epoch 523/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.0583 - acc: 0.6114 - val_loss: 50.1711 - val_acc: 0.5937\n",
      "Epoch 524/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.0594 - acc: 0.6119 - val_loss: 50.1800 - val_acc: 0.5941\n",
      "Epoch 525/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 56.0257 - acc: 0.6113 - val_loss: 50.1633 - val_acc: 0.5940\n",
      "Epoch 526/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 56.0454 - acc: 0.6111 - val_loss: 50.1616 - val_acc: 0.5941\n",
      "Epoch 527/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 56.0515 - acc: 0.6115 - val_loss: 50.1493 - val_acc: 0.5941\n",
      "Epoch 528/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 56.0243 - acc: 0.6116 - val_loss: 50.1533 - val_acc: 0.5940\n",
      "Epoch 529/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 56.0130 - acc: 0.6113 - val_loss: 50.1390 - val_acc: 0.5944\n",
      "Epoch 530/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 56.0522 - acc: 0.6119 - val_loss: 50.1383 - val_acc: 0.5939\n",
      "Epoch 531/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.9890 - acc: 0.6116 - val_loss: 50.1486 - val_acc: 0.5941\n",
      "Epoch 532/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 56.0189 - acc: 0.6119 - val_loss: 50.1426 - val_acc: 0.5941\n",
      "Epoch 533/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 56.0162 - acc: 0.6114 - val_loss: 50.1106 - val_acc: 0.5946\n",
      "Epoch 534/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 55.9671 - acc: 0.6122 - val_loss: 50.1015 - val_acc: 0.5944\n",
      "Epoch 535/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 55.9453 - acc: 0.6124 - val_loss: 50.1325 - val_acc: 0.5952\n",
      "Epoch 536/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.9705 - acc: 0.6122 - val_loss: 50.0913 - val_acc: 0.5940\n",
      "Epoch 537/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 55.9249 - acc: 0.6122 - val_loss: 50.1165 - val_acc: 0.5944\n",
      "Epoch 538/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 55.9169 - acc: 0.6118 - val_loss: 50.0963 - val_acc: 0.5941\n",
      "Epoch 539/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.9224 - acc: 0.6121 - val_loss: 50.0916 - val_acc: 0.5944\n",
      "Epoch 540/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 55.8973 - acc: 0.6122 - val_loss: 50.1150 - val_acc: 0.5949\n",
      "Epoch 541/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 55.8812 - acc: 0.6125 - val_loss: 50.1176 - val_acc: 0.5943\n",
      "Epoch 542/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.8562 - acc: 0.6125 - val_loss: 50.1015 - val_acc: 0.5951\n",
      "Epoch 543/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.8921 - acc: 0.6121 - val_loss: 50.1005 - val_acc: 0.5947\n",
      "Epoch 544/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.8483 - acc: 0.6118 - val_loss: 50.0883 - val_acc: 0.5949\n",
      "Epoch 545/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.8660 - acc: 0.6125 - val_loss: 50.0697 - val_acc: 0.5946\n",
      "Epoch 546/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.8304 - acc: 0.6124 - val_loss: 50.0847 - val_acc: 0.5951\n",
      "Epoch 547/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.7897 - acc: 0.6122 - val_loss: 50.0639 - val_acc: 0.5946\n",
      "Epoch 548/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.7919 - acc: 0.6124 - val_loss: 50.0746 - val_acc: 0.5942\n",
      "Epoch 549/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.7999 - acc: 0.6125 - val_loss: 50.0656 - val_acc: 0.5945\n",
      "Epoch 550/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.7890 - acc: 0.6127 - val_loss: 50.0615 - val_acc: 0.5938\n",
      "Epoch 551/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.7746 - acc: 0.6125 - val_loss: 50.0616 - val_acc: 0.5948\n",
      "Epoch 552/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.7997 - acc: 0.6126 - val_loss: 50.0678 - val_acc: 0.5952\n",
      "Epoch 553/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.7454 - acc: 0.6128 - val_loss: 50.0750 - val_acc: 0.5945\n",
      "Epoch 554/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.7463 - acc: 0.6123 - val_loss: 50.0518 - val_acc: 0.5946\n",
      "Epoch 555/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.7399 - acc: 0.6129 - val_loss: 50.0657 - val_acc: 0.5952\n",
      "Epoch 556/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.7236 - acc: 0.6126 - val_loss: 50.0615 - val_acc: 0.5950\n",
      "Epoch 557/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.7321 - acc: 0.6128 - val_loss: 50.0594 - val_acc: 0.5952\n",
      "Epoch 558/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.6557 - acc: 0.6135 - val_loss: 50.0290 - val_acc: 0.5948\n",
      "Epoch 559/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.7021 - acc: 0.6128 - val_loss: 50.0195 - val_acc: 0.5949\n",
      "Epoch 560/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.6891 - acc: 0.6130 - val_loss: 50.0313 - val_acc: 0.5942\n",
      "Epoch 561/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.7136 - acc: 0.6131 - val_loss: 50.0250 - val_acc: 0.5953\n",
      "Epoch 562/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.6761 - acc: 0.6135 - val_loss: 50.0154 - val_acc: 0.5946\n",
      "Epoch 563/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.6308 - acc: 0.6134 - val_loss: 50.0103 - val_acc: 0.5958\n",
      "Epoch 564/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.6307 - acc: 0.6136 - val_loss: 50.0132 - val_acc: 0.5954\n",
      "Epoch 565/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.6372 - acc: 0.6129 - val_loss: 50.0225 - val_acc: 0.5951\n",
      "Epoch 566/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.5807 - acc: 0.6132 - val_loss: 50.0258 - val_acc: 0.5948\n",
      "Epoch 567/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.6043 - acc: 0.6134 - val_loss: 50.0128 - val_acc: 0.5954\n",
      "Epoch 568/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.5951 - acc: 0.6128 - val_loss: 49.9932 - val_acc: 0.5959\n",
      "Epoch 569/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.5913 - acc: 0.6134 - val_loss: 49.9900 - val_acc: 0.5954\n",
      "Epoch 570/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.6023 - acc: 0.6139 - val_loss: 49.9887 - val_acc: 0.5952\n",
      "Epoch 571/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.5367 - acc: 0.6141 - val_loss: 49.9991 - val_acc: 0.5952\n",
      "Epoch 572/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.5617 - acc: 0.6128 - val_loss: 49.9692 - val_acc: 0.5951\n",
      "Epoch 573/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 55.5441 - acc: 0.6137 - val_loss: 49.9791 - val_acc: 0.5944\n",
      "Epoch 574/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.5379 - acc: 0.6128 - val_loss: 49.9733 - val_acc: 0.5952\n",
      "Epoch 575/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.5270 - acc: 0.6138 - val_loss: 49.9894 - val_acc: 0.5958\n",
      "Epoch 576/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.5212 - acc: 0.6143 - val_loss: 49.9608 - val_acc: 0.5953\n",
      "Epoch 577/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.5183 - acc: 0.6140 - val_loss: 49.9606 - val_acc: 0.5954\n",
      "Epoch 578/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.5193 - acc: 0.6140 - val_loss: 49.9556 - val_acc: 0.5956\n",
      "Epoch 579/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.4796 - acc: 0.6137 - val_loss: 49.9472 - val_acc: 0.5952\n",
      "Epoch 580/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.4268 - acc: 0.6137 - val_loss: 49.9583 - val_acc: 0.5953\n",
      "Epoch 581/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.4413 - acc: 0.6134 - val_loss: 49.9562 - val_acc: 0.5957\n",
      "Epoch 582/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.4553 - acc: 0.6140 - val_loss: 49.9448 - val_acc: 0.5950\n",
      "Epoch 583/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.4833 - acc: 0.6139 - val_loss: 49.9543 - val_acc: 0.5947\n",
      "Epoch 584/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.4878 - acc: 0.6142 - val_loss: 49.9382 - val_acc: 0.5950\n",
      "Epoch 585/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.4547 - acc: 0.6142 - val_loss: 49.9620 - val_acc: 0.5960\n",
      "Epoch 586/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.4107 - acc: 0.6136 - val_loss: 49.9366 - val_acc: 0.5956\n",
      "Epoch 587/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.3697 - acc: 0.6140 - val_loss: 49.9592 - val_acc: 0.5952\n",
      "Epoch 588/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.4150 - acc: 0.6138 - val_loss: 49.9459 - val_acc: 0.5955\n",
      "Epoch 589/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.3764 - acc: 0.6138 - val_loss: 49.9247 - val_acc: 0.5962\n",
      "Epoch 590/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.3893 - acc: 0.6143 - val_loss: 49.9345 - val_acc: 0.5962\n",
      "Epoch 591/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.3916 - acc: 0.6139 - val_loss: 49.9261 - val_acc: 0.5963\n",
      "Epoch 592/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.3687 - acc: 0.6146 - val_loss: 49.9094 - val_acc: 0.5956\n",
      "Epoch 593/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.3728 - acc: 0.6143 - val_loss: 49.9063 - val_acc: 0.5963\n",
      "Epoch 594/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.3544 - acc: 0.6144 - val_loss: 49.8969 - val_acc: 0.5960\n",
      "Epoch 595/1500\n",
      "55/55 [==============================] - 3s 56ms/step - loss: 55.3532 - acc: 0.6150 - val_loss: 49.9006 - val_acc: 0.5956\n",
      "Epoch 596/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.3299 - acc: 0.6145 - val_loss: 49.9119 - val_acc: 0.5956\n",
      "Epoch 597/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 55.3164 - acc: 0.6143 - val_loss: 49.8652 - val_acc: 0.5953\n",
      "Epoch 598/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 55.2906 - acc: 0.6147 - val_loss: 49.8862 - val_acc: 0.5961\n",
      "Epoch 599/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.2683 - acc: 0.6147 - val_loss: 49.9081 - val_acc: 0.5963\n",
      "Epoch 600/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.2628 - acc: 0.6142 - val_loss: 49.8698 - val_acc: 0.5954\n",
      "Epoch 601/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.2504 - acc: 0.6138 - val_loss: 49.8921 - val_acc: 0.5963\n",
      "Epoch 602/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.2901 - acc: 0.6143 - val_loss: 49.8710 - val_acc: 0.5963\n",
      "Epoch 603/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.2777 - acc: 0.6149 - val_loss: 49.8849 - val_acc: 0.5959\n",
      "Epoch 604/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.2621 - acc: 0.6147 - val_loss: 49.8897 - val_acc: 0.5961\n",
      "Epoch 605/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.2062 - acc: 0.6150 - val_loss: 49.8638 - val_acc: 0.5962\n",
      "Epoch 606/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.2421 - acc: 0.6148 - val_loss: 49.8827 - val_acc: 0.5959\n",
      "Epoch 607/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 55.2421 - acc: 0.6147 - val_loss: 49.8802 - val_acc: 0.5963\n",
      "Epoch 608/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 55.2085 - acc: 0.6152 - val_loss: 49.8787 - val_acc: 0.5965\n",
      "Epoch 609/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.1903 - acc: 0.6148 - val_loss: 49.8712 - val_acc: 0.5963\n",
      "Epoch 610/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.1377 - acc: 0.6152 - val_loss: 49.8589 - val_acc: 0.5958\n",
      "Epoch 611/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 55.2081 - acc: 0.6144 - val_loss: 49.8670 - val_acc: 0.5957\n",
      "Epoch 612/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.1147 - acc: 0.6151 - val_loss: 49.8633 - val_acc: 0.5965\n",
      "Epoch 613/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 55.1530 - acc: 0.6152 - val_loss: 49.8599 - val_acc: 0.5965\n",
      "Epoch 614/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 55.1166 - acc: 0.6152 - val_loss: 49.8503 - val_acc: 0.5969\n",
      "Epoch 615/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.1310 - acc: 0.6155 - val_loss: 49.8455 - val_acc: 0.5967\n",
      "Epoch 616/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.1275 - acc: 0.6155 - val_loss: 49.8546 - val_acc: 0.5966\n",
      "Epoch 617/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.1044 - acc: 0.6150 - val_loss: 49.8462 - val_acc: 0.5969\n",
      "Epoch 618/1500\n",
      "55/55 [==============================] - 3s 56ms/step - loss: 55.1112 - acc: 0.6157 - val_loss: 49.8259 - val_acc: 0.5958\n",
      "Epoch 619/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.0872 - acc: 0.6151 - val_loss: 49.8289 - val_acc: 0.5967\n",
      "Epoch 620/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 55.0786 - acc: 0.6151 - val_loss: 49.8262 - val_acc: 0.5969\n",
      "Epoch 621/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.0686 - acc: 0.6154 - val_loss: 49.8276 - val_acc: 0.5962\n",
      "Epoch 622/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 55.0856 - acc: 0.6146 - val_loss: 49.8101 - val_acc: 0.5962\n",
      "Epoch 623/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 55.0654 - acc: 0.6159 - val_loss: 49.7971 - val_acc: 0.5965\n",
      "Epoch 624/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 55.0421 - acc: 0.6157 - val_loss: 49.8028 - val_acc: 0.5970\n",
      "Epoch 625/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 55.0495 - acc: 0.6155 - val_loss: 49.8012 - val_acc: 0.5965\n",
      "Epoch 626/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.0515 - acc: 0.6152 - val_loss: 49.8009 - val_acc: 0.5962\n",
      "Epoch 627/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 55.0128 - acc: 0.6155 - val_loss: 49.8244 - val_acc: 0.5968\n",
      "Epoch 628/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 55.0175 - acc: 0.6157 - val_loss: 49.8078 - val_acc: 0.5967\n",
      "Epoch 629/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 55.0138 - acc: 0.6159 - val_loss: 49.8145 - val_acc: 0.5969\n",
      "Epoch 630/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.9819 - acc: 0.6157 - val_loss: 49.8134 - val_acc: 0.5974\n",
      "Epoch 631/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.9456 - acc: 0.6158 - val_loss: 49.8073 - val_acc: 0.5972\n",
      "Epoch 632/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.9933 - acc: 0.6158 - val_loss: 49.7759 - val_acc: 0.5965\n",
      "Epoch 633/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.9633 - acc: 0.6159 - val_loss: 49.7982 - val_acc: 0.5971\n",
      "Epoch 634/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 54.9883 - acc: 0.6157 - val_loss: 49.7989 - val_acc: 0.5966\n",
      "Epoch 635/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 54.9652 - acc: 0.6155 - val_loss: 49.8189 - val_acc: 0.5968\n",
      "Epoch 636/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.9558 - acc: 0.6163 - val_loss: 49.7694 - val_acc: 0.5970\n",
      "Epoch 637/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.9352 - acc: 0.6160 - val_loss: 49.7758 - val_acc: 0.5966\n",
      "Epoch 638/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.9219 - acc: 0.6162 - val_loss: 49.7759 - val_acc: 0.5970\n",
      "Epoch 639/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.9181 - acc: 0.6159 - val_loss: 49.7741 - val_acc: 0.5976\n",
      "Epoch 640/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.8662 - acc: 0.6166 - val_loss: 49.7668 - val_acc: 0.5976\n",
      "Epoch 641/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 54.8772 - acc: 0.6163 - val_loss: 49.7578 - val_acc: 0.5971\n",
      "Epoch 642/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 54.8890 - acc: 0.6159 - val_loss: 49.7824 - val_acc: 0.5965\n",
      "Epoch 643/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.9011 - acc: 0.6165 - val_loss: 49.7731 - val_acc: 0.5975\n",
      "Epoch 644/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.8722 - acc: 0.6162 - val_loss: 49.7751 - val_acc: 0.5977\n",
      "Epoch 645/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.9049 - acc: 0.6157 - val_loss: 49.7559 - val_acc: 0.5974\n",
      "Epoch 646/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.8289 - acc: 0.6163 - val_loss: 49.7641 - val_acc: 0.5973\n",
      "Epoch 647/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.8495 - acc: 0.6166 - val_loss: 49.7491 - val_acc: 0.5977\n",
      "Epoch 648/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.8225 - acc: 0.6163 - val_loss: 49.7542 - val_acc: 0.5974\n",
      "Epoch 649/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.8226 - acc: 0.6162 - val_loss: 49.7552 - val_acc: 0.5967\n",
      "Epoch 650/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.7935 - acc: 0.6164 - val_loss: 49.7757 - val_acc: 0.5968\n",
      "Epoch 651/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.7609 - acc: 0.6161 - val_loss: 49.7831 - val_acc: 0.5967\n",
      "Epoch 652/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 54.8126 - acc: 0.6162 - val_loss: 49.7674 - val_acc: 0.5968\n",
      "Epoch 653/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.7915 - acc: 0.6163 - val_loss: 49.7465 - val_acc: 0.5971\n",
      "Epoch 654/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.7789 - acc: 0.6166 - val_loss: 49.7588 - val_acc: 0.5969\n",
      "Epoch 655/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.7349 - acc: 0.6169 - val_loss: 49.7469 - val_acc: 0.5974\n",
      "Epoch 656/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.7826 - acc: 0.6164 - val_loss: 49.7305 - val_acc: 0.5969\n",
      "Epoch 657/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.7315 - acc: 0.6164 - val_loss: 49.7203 - val_acc: 0.5976\n",
      "Epoch 658/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 54.7397 - acc: 0.6165 - val_loss: 49.7282 - val_acc: 0.5967\n",
      "Epoch 659/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.7109 - acc: 0.6174 - val_loss: 49.7279 - val_acc: 0.5984\n",
      "Epoch 660/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.7014 - acc: 0.6171 - val_loss: 49.7427 - val_acc: 0.5974\n",
      "Epoch 661/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.7167 - acc: 0.6168 - val_loss: 49.7257 - val_acc: 0.5980\n",
      "Epoch 662/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.6714 - acc: 0.6172 - val_loss: 49.7386 - val_acc: 0.5985\n",
      "Epoch 663/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.7101 - acc: 0.6169 - val_loss: 49.7286 - val_acc: 0.5973\n",
      "Epoch 664/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 54.6725 - acc: 0.6173 - val_loss: 49.7232 - val_acc: 0.5975\n",
      "Epoch 665/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.6663 - acc: 0.6167 - val_loss: 49.7303 - val_acc: 0.5972\n",
      "Epoch 666/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.6665 - acc: 0.6172 - val_loss: 49.6989 - val_acc: 0.5971\n",
      "Epoch 667/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 54.7007 - acc: 0.6168 - val_loss: 49.7238 - val_acc: 0.5974\n",
      "Epoch 668/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.6476 - acc: 0.6174 - val_loss: 49.7117 - val_acc: 0.5976\n",
      "Epoch 669/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.6395 - acc: 0.6171 - val_loss: 49.7355 - val_acc: 0.5977\n",
      "Epoch 670/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 54.6422 - acc: 0.6175 - val_loss: 49.7153 - val_acc: 0.5970\n",
      "Epoch 671/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.6326 - acc: 0.6167 - val_loss: 49.7224 - val_acc: 0.5967\n",
      "Epoch 672/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.5711 - acc: 0.6170 - val_loss: 49.6980 - val_acc: 0.5972\n",
      "Epoch 673/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.5953 - acc: 0.6173 - val_loss: 49.7028 - val_acc: 0.5974\n",
      "Epoch 674/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.5843 - acc: 0.6172 - val_loss: 49.7000 - val_acc: 0.5971\n",
      "Epoch 675/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.5644 - acc: 0.6169 - val_loss: 49.6961 - val_acc: 0.5972\n",
      "Epoch 676/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 54.5937 - acc: 0.6168 - val_loss: 49.6788 - val_acc: 0.5974loss: 53.\n",
      "Epoch 677/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 54.5789 - acc: 0.6173 - val_loss: 49.6890 - val_acc: 0.5980\n",
      "Epoch 678/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.5386 - acc: 0.6175 - val_loss: 49.7014 - val_acc: 0.5978\n",
      "Epoch 679/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 54.5420 - acc: 0.6169 - val_loss: 49.6831 - val_acc: 0.5969\n",
      "Epoch 680/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.5360 - acc: 0.6174 - val_loss: 49.7029 - val_acc: 0.5976\n",
      "Epoch 681/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 54.5989 - acc: 0.6174 - val_loss: 49.6824 - val_acc: 0.5981\n",
      "Epoch 682/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.5443 - acc: 0.6173 - val_loss: 49.6950 - val_acc: 0.5976\n",
      "Epoch 683/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.5093 - acc: 0.6172 - val_loss: 49.6869 - val_acc: 0.5979\n",
      "Epoch 684/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.5183 - acc: 0.6178 - val_loss: 49.6839 - val_acc: 0.5971\n",
      "Epoch 685/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.4868 - acc: 0.6171 - val_loss: 49.6866 - val_acc: 0.5982\n",
      "Epoch 686/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 54.4726 - acc: 0.6175 - val_loss: 49.6941 - val_acc: 0.5976\n",
      "Epoch 687/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.4995 - acc: 0.6175 - val_loss: 49.6719 - val_acc: 0.5976\n",
      "Epoch 688/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.5022 - acc: 0.6177 - val_loss: 49.6642 - val_acc: 0.5977\n",
      "Epoch 689/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.4445 - acc: 0.6181 - val_loss: 49.6740 - val_acc: 0.5971\n",
      "Epoch 690/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.4291 - acc: 0.6176 - val_loss: 49.6534 - val_acc: 0.5975\n",
      "Epoch 691/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.4670 - acc: 0.6180 - val_loss: 49.6743 - val_acc: 0.5976\n",
      "Epoch 692/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.4351 - acc: 0.6181 - val_loss: 49.6713 - val_acc: 0.5968\n",
      "Epoch 693/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.4512 - acc: 0.6175 - val_loss: 49.6635 - val_acc: 0.5974\n",
      "Epoch 694/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.4351 - acc: 0.6178 - val_loss: 49.6539 - val_acc: 0.5978\n",
      "Epoch 695/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.4556 - acc: 0.6177 - val_loss: 49.6556 - val_acc: 0.5982\n",
      "Epoch 696/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.4044 - acc: 0.6182 - val_loss: 49.6657 - val_acc: 0.5975\n",
      "Epoch 697/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.4188 - acc: 0.6178 - val_loss: 49.6612 - val_acc: 0.5977\n",
      "Epoch 698/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.3644 - acc: 0.6182 - val_loss: 49.6683 - val_acc: 0.5986\n",
      "Epoch 699/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 54.3815 - acc: 0.6180 - val_loss: 49.6564 - val_acc: 0.5985\n",
      "Epoch 700/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.3917 - acc: 0.6184 - val_loss: 49.6722 - val_acc: 0.5983\n",
      "Epoch 701/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.3622 - acc: 0.6184 - val_loss: 49.6311 - val_acc: 0.5983\n",
      "Epoch 702/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.3562 - acc: 0.6182 - val_loss: 49.6408 - val_acc: 0.5983\n",
      "Epoch 703/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.3468 - acc: 0.6183 - val_loss: 49.6453 - val_acc: 0.5981\n",
      "Epoch 704/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.3635 - acc: 0.6182 - val_loss: 49.6382 - val_acc: 0.5981\n",
      "Epoch 705/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.3799 - acc: 0.6182 - val_loss: 49.6329 - val_acc: 0.5986\n",
      "Epoch 706/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.3483 - acc: 0.6187 - val_loss: 49.6469 - val_acc: 0.5983\n",
      "Epoch 707/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.3370 - acc: 0.6184 - val_loss: 49.6296 - val_acc: 0.5976\n",
      "Epoch 708/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.2946 - acc: 0.6183 - val_loss: 49.6379 - val_acc: 0.5981\n",
      "Epoch 709/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.2877 - acc: 0.6185 - val_loss: 49.6438 - val_acc: 0.5982\n",
      "Epoch 710/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.3154 - acc: 0.6191 - val_loss: 49.6297 - val_acc: 0.5982\n",
      "Epoch 711/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.2888 - acc: 0.6181 - val_loss: 49.6516 - val_acc: 0.5985\n",
      "Epoch 712/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.2674 - acc: 0.6183 - val_loss: 49.6460 - val_acc: 0.5981\n",
      "Epoch 713/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.2860 - acc: 0.6183 - val_loss: 49.6465 - val_acc: 0.5987\n",
      "Epoch 714/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.2300 - acc: 0.6186 - val_loss: 49.6279 - val_acc: 0.5981\n",
      "Epoch 715/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.2396 - acc: 0.6179 - val_loss: 49.6345 - val_acc: 0.5991\n",
      "Epoch 716/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.2534 - acc: 0.6187 - val_loss: 49.6228 - val_acc: 0.5983\n",
      "Epoch 717/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 54.2307 - acc: 0.6183 - val_loss: 49.6261 - val_acc: 0.5986\n",
      "Epoch 718/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.2306 - acc: 0.6191 - val_loss: 49.6353 - val_acc: 0.5987\n",
      "Epoch 719/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.2205 - acc: 0.6184 - val_loss: 49.6125 - val_acc: 0.5983\n",
      "Epoch 720/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.2304 - acc: 0.6187 - val_loss: 49.6173 - val_acc: 0.5986\n",
      "Epoch 721/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.1975 - acc: 0.6188 - val_loss: 49.6464 - val_acc: 0.5982\n",
      "Epoch 722/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.1650 - acc: 0.6189 - val_loss: 49.6272 - val_acc: 0.5983\n",
      "Epoch 723/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 54.1652 - acc: 0.6190 - val_loss: 49.6297 - val_acc: 0.5987\n",
      "Epoch 724/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.1795 - acc: 0.6188 - val_loss: 49.6234 - val_acc: 0.5980\n",
      "Epoch 725/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.1170 - acc: 0.6187 - val_loss: 49.6078 - val_acc: 0.5984\n",
      "Epoch 726/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.1517 - acc: 0.6187 - val_loss: 49.6252 - val_acc: 0.5986\n",
      "Epoch 727/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.1169 - acc: 0.6187 - val_loss: 49.6046 - val_acc: 0.5986\n",
      "Epoch 728/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.1360 - acc: 0.6191 - val_loss: 49.5958 - val_acc: 0.5981\n",
      "Epoch 729/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.1772 - acc: 0.6185 - val_loss: 49.6028 - val_acc: 0.5981\n",
      "Epoch 730/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 54.1197 - acc: 0.6190 - val_loss: 49.6156 - val_acc: 0.5987\n",
      "Epoch 731/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.1212 - acc: 0.6190 - val_loss: 49.6209 - val_acc: 0.5982\n",
      "Epoch 732/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.1205 - acc: 0.6186 - val_loss: 49.5963 - val_acc: 0.5981\n",
      "Epoch 733/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.0727 - acc: 0.6188 - val_loss: 49.5825 - val_acc: 0.5994\n",
      "Epoch 734/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.0743 - acc: 0.6197 - val_loss: 49.5883 - val_acc: 0.5983\n",
      "Epoch 735/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 54.0779 - acc: 0.6190 - val_loss: 49.6187 - val_acc: 0.5993\n",
      "Epoch 736/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 54.0704 - acc: 0.6192 - val_loss: 49.5907 - val_acc: 0.5981\n",
      "Epoch 737/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.0657 - acc: 0.6195 - val_loss: 49.6102 - val_acc: 0.5983\n",
      "Epoch 738/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.0739 - acc: 0.6192 - val_loss: 49.5929 - val_acc: 0.5983\n",
      "Epoch 739/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 54.0698 - acc: 0.6197 - val_loss: 49.6271 - val_acc: 0.5984\n",
      "Epoch 740/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.0775 - acc: 0.6190 - val_loss: 49.5947 - val_acc: 0.5986\n",
      "Epoch 741/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 54.0072 - acc: 0.6193 - val_loss: 49.6278 - val_acc: 0.5989\n",
      "Epoch 742/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 54.0385 - acc: 0.6195 - val_loss: 49.6080 - val_acc: 0.5988\n",
      "Epoch 743/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.0372 - acc: 0.6193 - val_loss: 49.6049 - val_acc: 0.5984\n",
      "Epoch 744/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.9984 - acc: 0.6197 - val_loss: 49.5863 - val_acc: 0.5982\n",
      "Epoch 745/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.0406 - acc: 0.6188 - val_loss: 49.5999 - val_acc: 0.5985\n",
      "Epoch 746/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 54.0150 - acc: 0.6190 - val_loss: 49.6000 - val_acc: 0.5982\n",
      "Epoch 747/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.9758 - acc: 0.6188 - val_loss: 49.5759 - val_acc: 0.5989\n",
      "Epoch 748/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.9562 - acc: 0.6195 - val_loss: 49.5801 - val_acc: 0.5991\n",
      "Epoch 749/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 54.0005 - acc: 0.6199 - val_loss: 49.5778 - val_acc: 0.5988\n",
      "Epoch 750/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.9689 - acc: 0.6199 - val_loss: 49.5834 - val_acc: 0.5989\n",
      "Epoch 751/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.9603 - acc: 0.6197 - val_loss: 49.5884 - val_acc: 0.5988\n",
      "Epoch 752/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.9395 - acc: 0.6197 - val_loss: 49.5845 - val_acc: 0.5992\n",
      "Epoch 753/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.9709 - acc: 0.6194 - val_loss: 49.5913 - val_acc: 0.5986\n",
      "Epoch 754/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.9443 - acc: 0.6195 - val_loss: 49.5808 - val_acc: 0.5987\n",
      "Epoch 755/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.9416 - acc: 0.6201 - val_loss: 49.5636 - val_acc: 0.5996\n",
      "Epoch 756/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.8850 - acc: 0.6195 - val_loss: 49.5840 - val_acc: 0.5990\n",
      "Epoch 757/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.9391 - acc: 0.6196 - val_loss: 49.5874 - val_acc: 0.5999\n",
      "Epoch 758/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.9070 - acc: 0.6204 - val_loss: 49.5707 - val_acc: 0.5992\n",
      "Epoch 759/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8912 - acc: 0.6205 - val_loss: 49.5810 - val_acc: 0.5985\n",
      "Epoch 760/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.8847 - acc: 0.6195 - val_loss: 49.5866 - val_acc: 0.5995\n",
      "Epoch 761/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.8814 - acc: 0.6202 - val_loss: 49.5575 - val_acc: 0.5996\n",
      "Epoch 762/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.8644 - acc: 0.6200 - val_loss: 49.5922 - val_acc: 0.5993\n",
      "Epoch 763/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8394 - acc: 0.6203 - val_loss: 49.5537 - val_acc: 0.5994\n",
      "Epoch 764/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8138 - acc: 0.6203 - val_loss: 49.5825 - val_acc: 0.5991\n",
      "Epoch 765/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.8668 - acc: 0.6200 - val_loss: 49.5744 - val_acc: 0.5993\n",
      "Epoch 766/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.8801 - acc: 0.6202 - val_loss: 49.5869 - val_acc: 0.5999\n",
      "Epoch 767/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.8309 - acc: 0.6200 - val_loss: 49.5509 - val_acc: 0.5995\n",
      "Epoch 768/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.8602 - acc: 0.6206 - val_loss: 49.5737 - val_acc: 0.5998\n",
      "Epoch 769/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8059 - acc: 0.6206 - val_loss: 49.5628 - val_acc: 0.5996\n",
      "Epoch 770/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8318 - acc: 0.6203 - val_loss: 49.5696 - val_acc: 0.6000\n",
      "Epoch 771/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.7673 - acc: 0.6204 - val_loss: 49.5499 - val_acc: 0.5990\n",
      "Epoch 772/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 53.7514 - acc: 0.6204 - val_loss: 49.5451 - val_acc: 0.5989\n",
      "Epoch 773/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.7750 - acc: 0.6206 - val_loss: 49.5509 - val_acc: 0.5997\n",
      "Epoch 774/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.7747 - acc: 0.6205 - val_loss: 49.5380 - val_acc: 0.5997\n",
      "Epoch 775/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.7962 - acc: 0.6208 - val_loss: 49.5568 - val_acc: 0.5995\n",
      "Epoch 776/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.7715 - acc: 0.6207 - val_loss: 49.5452 - val_acc: 0.5995\n",
      "Epoch 777/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 53.7458 - acc: 0.6207 - val_loss: 49.5557 - val_acc: 0.5994\n",
      "Epoch 778/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.7130 - acc: 0.6205 - val_loss: 49.5303 - val_acc: 0.6002\n",
      "Epoch 779/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.8003 - acc: 0.6209 - val_loss: 49.5432 - val_acc: 0.5994\n",
      "Epoch 780/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.7579 - acc: 0.6209 - val_loss: 49.5244 - val_acc: 0.5988\n",
      "Epoch 781/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 53.7551 - acc: 0.6203 - val_loss: 49.5569 - val_acc: 0.5998\n",
      "Epoch 782/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.7163 - acc: 0.6206 - val_loss: 49.5474 - val_acc: 0.6003\n",
      "Epoch 783/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.7056 - acc: 0.6211 - val_loss: 49.5204 - val_acc: 0.5995\n",
      "Epoch 784/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.6886 - acc: 0.6206 - val_loss: 49.5444 - val_acc: 0.5995\n",
      "Epoch 785/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.6807 - acc: 0.6209 - val_loss: 49.5728 - val_acc: 0.5996\n",
      "Epoch 786/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.6761 - acc: 0.6207 - val_loss: 49.5315 - val_acc: 0.5999\n",
      "Epoch 787/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.6654 - acc: 0.6212 - val_loss: 49.5157 - val_acc: 0.5995\n",
      "Epoch 788/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.6905 - acc: 0.6209 - val_loss: 49.5325 - val_acc: 0.5990\n",
      "Epoch 789/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.6425 - acc: 0.6211 - val_loss: 49.5824 - val_acc: 0.6000\n",
      "Epoch 790/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.5979 - acc: 0.6208 - val_loss: 49.5319 - val_acc: 0.5997\n",
      "Epoch 791/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.6570 - acc: 0.6213 - val_loss: 49.5385 - val_acc: 0.5999\n",
      "Epoch 792/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.6134 - acc: 0.6211 - val_loss: 49.5196 - val_acc: 0.6005\n",
      "Epoch 793/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.6412 - acc: 0.6207 - val_loss: 49.5234 - val_acc: 0.5996\n",
      "Epoch 794/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.6431 - acc: 0.6212 - val_loss: 49.5470 - val_acc: 0.5996\n",
      "Epoch 795/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.6423 - acc: 0.6206 - val_loss: 49.5394 - val_acc: 0.5997\n",
      "Epoch 796/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.6186 - acc: 0.6210 - val_loss: 49.5492 - val_acc: 0.6004\n",
      "Epoch 797/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.6269 - acc: 0.6209 - val_loss: 49.5739 - val_acc: 0.6006\n",
      "Epoch 798/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 53.6205 - acc: 0.6216 - val_loss: 49.5137 - val_acc: 0.5997\n",
      "Epoch 799/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.5847 - acc: 0.6212 - val_loss: 49.5103 - val_acc: 0.5997\n",
      "Epoch 800/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.6175 - acc: 0.6210 - val_loss: 49.5387 - val_acc: 0.6003\n",
      "Epoch 801/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.5928 - acc: 0.6211 - val_loss: 49.5155 - val_acc: 0.6001\n",
      "Epoch 802/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.5391 - acc: 0.6214 - val_loss: 49.5401 - val_acc: 0.6005\n",
      "Epoch 803/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.5603 - acc: 0.6216 - val_loss: 49.5440 - val_acc: 0.6002\n",
      "Epoch 804/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.5517 - acc: 0.6217 - val_loss: 49.5328 - val_acc: 0.6005\n",
      "Epoch 805/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 53.5514 - acc: 0.6211 - val_loss: 49.5310 - val_acc: 0.6006\n",
      "Epoch 806/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 53.5528 - acc: 0.6213 - val_loss: 49.5211 - val_acc: 0.6001\n",
      "Epoch 807/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.5635 - acc: 0.6214 - val_loss: 49.5037 - val_acc: 0.6001\n",
      "Epoch 808/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 53.5354 - acc: 0.6212 - val_loss: 49.5307 - val_acc: 0.6007\n",
      "Epoch 809/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.5599 - acc: 0.6214 - val_loss: 49.5254 - val_acc: 0.6000\n",
      "Epoch 810/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.5585 - acc: 0.6217 - val_loss: 49.5264 - val_acc: 0.5998\n",
      "Epoch 811/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.4874 - acc: 0.6215 - val_loss: 49.4989 - val_acc: 0.6004\n",
      "Epoch 812/1500\n",
      "55/55 [==============================] - 3s 56ms/step - loss: 53.4725 - acc: 0.6216 - val_loss: 49.5281 - val_acc: 0.6005\n",
      "Epoch 813/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.4789 - acc: 0.6208 - val_loss: 49.5324 - val_acc: 0.5999\n",
      "Epoch 814/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.4899 - acc: 0.6216 - val_loss: 49.5120 - val_acc: 0.5997\n",
      "Epoch 815/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.4911 - acc: 0.6219 - val_loss: 49.5101 - val_acc: 0.6004\n",
      "Epoch 816/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 53.4850 - acc: 0.6216 - val_loss: 49.5347 - val_acc: 0.5992\n",
      "Epoch 817/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.4342 - acc: 0.6212 - val_loss: 49.5150 - val_acc: 0.6004\n",
      "Epoch 818/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.4448 - acc: 0.6216 - val_loss: 49.5316 - val_acc: 0.6000\n",
      "Epoch 819/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.4368 - acc: 0.6216 - val_loss: 49.5315 - val_acc: 0.6002\n",
      "Epoch 820/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 53.4388 - acc: 0.6212 - val_loss: 49.5443 - val_acc: 0.6002\n",
      "Epoch 821/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.4249 - acc: 0.6217 - val_loss: 49.5200 - val_acc: 0.5999\n",
      "Epoch 822/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.3828 - acc: 0.6216 - val_loss: 49.5107 - val_acc: 0.6004\n",
      "Epoch 823/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.3820 - acc: 0.6215 - val_loss: 49.5005 - val_acc: 0.5999\n",
      "Epoch 824/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.4247 - acc: 0.6215 - val_loss: 49.5197 - val_acc: 0.6004\n",
      "Epoch 825/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.3986 - acc: 0.6219 - val_loss: 49.4961 - val_acc: 0.6004\n",
      "Epoch 826/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.3847 - acc: 0.6211 - val_loss: 49.5210 - val_acc: 0.6007\n",
      "Epoch 827/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.4256 - acc: 0.6222 - val_loss: 49.5387 - val_acc: 0.6004\n",
      "Epoch 828/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 53.4080 - acc: 0.6222 - val_loss: 49.5253 - val_acc: 0.5998\n",
      "Epoch 829/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3912 - acc: 0.6217 - val_loss: 49.4960 - val_acc: 0.6007\n",
      "Epoch 830/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.3238 - acc: 0.6216 - val_loss: 49.5343 - val_acc: 0.6008\n",
      "Epoch 831/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3606 - acc: 0.6219 - val_loss: 49.5150 - val_acc: 0.6012\n",
      "Epoch 832/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.3242 - acc: 0.6220 - val_loss: 49.5192 - val_acc: 0.6005\n",
      "Epoch 833/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.3712 - acc: 0.6219 - val_loss: 49.5116 - val_acc: 0.6004\n",
      "Epoch 834/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3640 - acc: 0.6221 - val_loss: 49.5002 - val_acc: 0.6004\n",
      "Epoch 835/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 53.3607 - acc: 0.6219 - val_loss: 49.5098 - val_acc: 0.5998\n",
      "Epoch 836/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3218 - acc: 0.6220 - val_loss: 49.5065 - val_acc: 0.5997\n",
      "Epoch 837/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3158 - acc: 0.6219 - val_loss: 49.5149 - val_acc: 0.6010\n",
      "Epoch 838/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.2846 - acc: 0.6224 - val_loss: 49.4953 - val_acc: 0.6000\n",
      "Epoch 839/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.3287 - acc: 0.6220 - val_loss: 49.5345 - val_acc: 0.6005\n",
      "Epoch 840/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.3441 - acc: 0.6223 - val_loss: 49.5012 - val_acc: 0.6006\n",
      "Epoch 841/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 53.3033 - acc: 0.6227 - val_loss: 49.5454 - val_acc: 0.6007\n",
      "Epoch 842/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.2426 - acc: 0.6226 - val_loss: 49.4920 - val_acc: 0.5999\n",
      "Epoch 843/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.2403 - acc: 0.6218 - val_loss: 49.4852 - val_acc: 0.6004\n",
      "Epoch 844/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.2884 - acc: 0.6222 - val_loss: 49.4928 - val_acc: 0.6007\n",
      "Epoch 845/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.2360 - acc: 0.6227 - val_loss: 49.4900 - val_acc: 0.6010\n",
      "Epoch 846/1500\n",
      "55/55 [==============================] - 3s 60ms/step - loss: 53.2756 - acc: 0.6218 - val_loss: 49.5215 - val_acc: 0.6009\n",
      "Epoch 847/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.2420 - acc: 0.6225 - val_loss: 49.5066 - val_acc: 0.6007\n",
      "Epoch 848/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.2154 - acc: 0.6223 - val_loss: 49.5170 - val_acc: 0.6006\n",
      "Epoch 849/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 53.2366 - acc: 0.6223 - val_loss: 49.5177 - val_acc: 0.6008\n",
      "Epoch 850/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.2130 - acc: 0.6223 - val_loss: 49.5074 - val_acc: 0.6010\n",
      "Epoch 851/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.2284 - acc: 0.6223 - val_loss: 49.5116 - val_acc: 0.6009\n",
      "Epoch 852/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.2208 - acc: 0.6229 - val_loss: 49.5099 - val_acc: 0.6013\n",
      "Epoch 853/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.1975 - acc: 0.6234 - val_loss: 49.4981 - val_acc: 0.6013\n",
      "Epoch 854/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.1753 - acc: 0.6229 - val_loss: 49.4778 - val_acc: 0.6001\n",
      "Epoch 855/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.2013 - acc: 0.6222 - val_loss: 49.4964 - val_acc: 0.6005\n",
      "Epoch 856/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.1568 - acc: 0.6222 - val_loss: 49.5175 - val_acc: 0.6011\n",
      "Epoch 857/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.2125 - acc: 0.6226 - val_loss: 49.5007 - val_acc: 0.6005\n",
      "Epoch 858/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.2039 - acc: 0.6222 - val_loss: 49.5165 - val_acc: 0.6013\n",
      "Epoch 859/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.1410 - acc: 0.6226 - val_loss: 49.4860 - val_acc: 0.6009\n",
      "Epoch 860/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.1905 - acc: 0.6227 - val_loss: 49.4939 - val_acc: 0.6007\n",
      "Epoch 861/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.1465 - acc: 0.6229 - val_loss: 49.5246 - val_acc: 0.6007\n",
      "Epoch 862/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.1551 - acc: 0.6226 - val_loss: 49.4969 - val_acc: 0.6014\n",
      "Epoch 863/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.1816 - acc: 0.6225 - val_loss: 49.4790 - val_acc: 0.6008\n",
      "Epoch 864/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.1346 - acc: 0.6229 - val_loss: 49.5082 - val_acc: 0.6014\n",
      "Epoch 865/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.1228 - acc: 0.6228 - val_loss: 49.4861 - val_acc: 0.6010\n",
      "Epoch 866/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.0938 - acc: 0.6237 - val_loss: 49.5065 - val_acc: 0.6003\n",
      "Epoch 867/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.1034 - acc: 0.6229 - val_loss: 49.4915 - val_acc: 0.6006\n",
      "Epoch 868/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.0999 - acc: 0.6229 - val_loss: 49.4970 - val_acc: 0.6014\n",
      "Epoch 869/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 53.1022 - acc: 0.6236 - val_loss: 49.4843 - val_acc: 0.6004\n",
      "Epoch 870/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.0871 - acc: 0.6231 - val_loss: 49.5030 - val_acc: 0.6008\n",
      "Epoch 871/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.0572 - acc: 0.6230 - val_loss: 49.4834 - val_acc: 0.6005\n",
      "Epoch 872/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.0830 - acc: 0.6228 - val_loss: 49.4720 - val_acc: 0.6008\n",
      "Epoch 873/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.0938 - acc: 0.6226 - val_loss: 49.5063 - val_acc: 0.6012\n",
      "Epoch 874/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.0818 - acc: 0.6233 - val_loss: 49.4999 - val_acc: 0.6008\n",
      "Epoch 875/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 53.0401 - acc: 0.6238 - val_loss: 49.4969 - val_acc: 0.6014\n",
      "Epoch 876/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 53.0879 - acc: 0.6231 - val_loss: 49.4835 - val_acc: 0.6010\n",
      "Epoch 877/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.0097 - acc: 0.6234 - val_loss: 49.5089 - val_acc: 0.6010\n",
      "Epoch 878/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.0459 - acc: 0.6225 - val_loss: 49.4652 - val_acc: 0.6006\n",
      "Epoch 879/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 53.0027 - acc: 0.6230 - val_loss: 49.4398 - val_acc: 0.6011\n",
      "Epoch 880/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 53.0320 - acc: 0.6232 - val_loss: 49.4545 - val_acc: 0.6014\n",
      "Epoch 881/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 53.0005 - acc: 0.6230 - val_loss: 49.4659 - val_acc: 0.6004\n",
      "Epoch 882/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 53.0106 - acc: 0.6230 - val_loss: 49.4628 - val_acc: 0.6007\n",
      "Epoch 883/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 53.0203 - acc: 0.6231 - val_loss: 49.4928 - val_acc: 0.6013\n",
      "Epoch 884/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.9815 - acc: 0.6232 - val_loss: 49.4779 - val_acc: 0.6015\n",
      "Epoch 885/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.9741 - acc: 0.6236 - val_loss: 49.4664 - val_acc: 0.6010\n",
      "Epoch 886/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 53.0580 - acc: 0.6234 - val_loss: 49.4647 - val_acc: 0.6008\n",
      "Epoch 887/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.9702 - acc: 0.6240 - val_loss: 49.4809 - val_acc: 0.6006\n",
      "Epoch 888/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.9808 - acc: 0.6227 - val_loss: 49.4645 - val_acc: 0.6010\n",
      "Epoch 889/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.9542 - acc: 0.6234 - val_loss: 49.4695 - val_acc: 0.6008\n",
      "Epoch 890/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.9613 - acc: 0.6231 - val_loss: 49.4735 - val_acc: 0.6014\n",
      "Epoch 891/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.9541 - acc: 0.6238 - val_loss: 49.4950 - val_acc: 0.6011\n",
      "Epoch 892/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.9800 - acc: 0.6233 - val_loss: 49.4580 - val_acc: 0.6016\n",
      "Epoch 893/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.9663 - acc: 0.6235 - val_loss: 49.4724 - val_acc: 0.6017\n",
      "Epoch 894/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.9461 - acc: 0.6233 - val_loss: 49.4939 - val_acc: 0.6014\n",
      "Epoch 895/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.8953 - acc: 0.6236 - val_loss: 49.4848 - val_acc: 0.6009\n",
      "Epoch 896/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.9166 - acc: 0.6235 - val_loss: 49.4863 - val_acc: 0.6013\n",
      "Epoch 897/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.9358 - acc: 0.6239 - val_loss: 49.4812 - val_acc: 0.6014\n",
      "Epoch 898/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 52.8965 - acc: 0.6238 - val_loss: 49.4785 - val_acc: 0.6016\n",
      "Epoch 899/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.8510 - acc: 0.6240 - val_loss: 49.4488 - val_acc: 0.6013\n",
      "Epoch 900/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.9137 - acc: 0.6241 - val_loss: 49.4668 - val_acc: 0.6006\n",
      "Epoch 901/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.8972 - acc: 0.6229 - val_loss: 49.4913 - val_acc: 0.6012\n",
      "Epoch 902/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.8559 - acc: 0.6240 - val_loss: 49.4680 - val_acc: 0.6011\n",
      "Epoch 903/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.8674 - acc: 0.6236 - val_loss: 49.4648 - val_acc: 0.6014\n",
      "Epoch 904/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.8586 - acc: 0.6240 - val_loss: 49.4731 - val_acc: 0.6013\n",
      "Epoch 905/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.8669 - acc: 0.6239 - val_loss: 49.4529 - val_acc: 0.6017\n",
      "Epoch 906/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.8288 - acc: 0.6238 - val_loss: 49.4897 - val_acc: 0.6015\n",
      "Epoch 907/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.8414 - acc: 0.6238 - val_loss: 49.4764 - val_acc: 0.6009\n",
      "Epoch 908/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.8537 - acc: 0.6242 - val_loss: 49.4778 - val_acc: 0.6018\n",
      "Epoch 909/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.8358 - acc: 0.6241 - val_loss: 49.4635 - val_acc: 0.6004\n",
      "Epoch 910/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.8398 - acc: 0.6240 - val_loss: 49.4600 - val_acc: 0.6011\n",
      "Epoch 911/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.8188 - acc: 0.6242 - val_loss: 49.4682 - val_acc: 0.6020\n",
      "Epoch 912/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7944 - acc: 0.6240 - val_loss: 49.4630 - val_acc: 0.6015\n",
      "Epoch 913/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.8154 - acc: 0.6237 - val_loss: 49.4996 - val_acc: 0.6013\n",
      "Epoch 914/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7879 - acc: 0.6235 - val_loss: 49.4624 - val_acc: 0.6018\n",
      "Epoch 915/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.8166 - acc: 0.6239 - val_loss: 49.4614 - val_acc: 0.6011\n",
      "Epoch 916/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.8336 - acc: 0.6241 - val_loss: 49.4637 - val_acc: 0.6004\n",
      "Epoch 917/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.7889 - acc: 0.6233 - val_loss: 49.4548 - val_acc: 0.6009\n",
      "Epoch 918/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.7639 - acc: 0.6241 - val_loss: 49.4689 - val_acc: 0.6017\n",
      "Epoch 919/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.7468 - acc: 0.6241 - val_loss: 49.4794 - val_acc: 0.6014\n",
      "Epoch 920/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.7694 - acc: 0.6240 - val_loss: 49.4559 - val_acc: 0.6013\n",
      "Epoch 921/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.7446 - acc: 0.6246 - val_loss: 49.4758 - val_acc: 0.6012\n",
      "Epoch 922/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7544 - acc: 0.6240 - val_loss: 49.4550 - val_acc: 0.6011\n",
      "Epoch 923/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.7215 - acc: 0.6235 - val_loss: 49.4755 - val_acc: 0.6012\n",
      "Epoch 924/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7645 - acc: 0.6244 - val_loss: 49.4766 - val_acc: 0.6015\n",
      "Epoch 925/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.7272 - acc: 0.6235 - val_loss: 49.4548 - val_acc: 0.6016\n",
      "Epoch 926/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7177 - acc: 0.6242 - val_loss: 49.4576 - val_acc: 0.6019\n",
      "Epoch 927/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.7316 - acc: 0.6239 - val_loss: 49.4497 - val_acc: 0.6019\n",
      "Epoch 928/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.7076 - acc: 0.6246 - val_loss: 49.4610 - val_acc: 0.6019\n",
      "Epoch 929/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.7310 - acc: 0.6240 - val_loss: 49.4749 - val_acc: 0.6011\n",
      "Epoch 930/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6953 - acc: 0.6235 - val_loss: 49.4470 - val_acc: 0.6016\n",
      "Epoch 931/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.6947 - acc: 0.6250 - val_loss: 49.4789 - val_acc: 0.6017\n",
      "Epoch 932/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.6369 - acc: 0.6246 - val_loss: 49.4581 - val_acc: 0.6017\n",
      "Epoch 933/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 52.6679 - acc: 0.6245 - val_loss: 49.4510 - val_acc: 0.6015\n",
      "Epoch 934/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6872 - acc: 0.6243 - val_loss: 49.4513 - val_acc: 0.6015\n",
      "Epoch 935/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6780 - acc: 0.6244 - val_loss: 49.4472 - val_acc: 0.6013\n",
      "Epoch 936/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6394 - acc: 0.6247 - val_loss: 49.4869 - val_acc: 0.6005\n",
      "Epoch 937/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6415 - acc: 0.6245 - val_loss: 49.4695 - val_acc: 0.6020\n",
      "Epoch 938/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.6051 - acc: 0.6244 - val_loss: 49.4604 - val_acc: 0.6023\n",
      "Epoch 939/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.6495 - acc: 0.6247 - val_loss: 49.4319 - val_acc: 0.6013\n",
      "Epoch 940/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6072 - acc: 0.6243 - val_loss: 49.4417 - val_acc: 0.6014\n",
      "Epoch 941/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.6602 - acc: 0.6244 - val_loss: 49.4736 - val_acc: 0.6020.\n",
      "Epoch 942/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.6183 - acc: 0.6246 - val_loss: 49.4652 - val_acc: 0.6016\n",
      "Epoch 943/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.5978 - acc: 0.6249 - val_loss: 49.4368 - val_acc: 0.6015\n",
      "Epoch 944/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.6231 - acc: 0.6246 - val_loss: 49.4476 - val_acc: 0.6018\n",
      "Epoch 945/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.6106 - acc: 0.6249 - val_loss: 49.4337 - val_acc: 0.6017\n",
      "Epoch 946/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.6132 - acc: 0.6252 - val_loss: 49.4347 - val_acc: 0.6021\n",
      "Epoch 947/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.5846 - acc: 0.6248 - val_loss: 49.4094 - val_acc: 0.6016\n",
      "Epoch 948/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.5639 - acc: 0.6248 - val_loss: 49.4607 - val_acc: 0.6013\n",
      "Epoch 949/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.5909 - acc: 0.6244 - val_loss: 49.4654 - val_acc: 0.6020\n",
      "Epoch 950/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.5742 - acc: 0.6253 - val_loss: 49.4442 - val_acc: 0.6022\n",
      "Epoch 951/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.5637 - acc: 0.6250 - val_loss: 49.4688 - val_acc: 0.6017\n",
      "Epoch 952/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.5849 - acc: 0.6251 - val_loss: 49.4497 - val_acc: 0.6021\n",
      "Epoch 953/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.5398 - acc: 0.6247 - val_loss: 49.4529 - val_acc: 0.6009\n",
      "Epoch 954/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.5494 - acc: 0.6236 - val_loss: 49.4785 - val_acc: 0.6016\n",
      "Epoch 955/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.5504 - acc: 0.6244 - val_loss: 49.4361 - val_acc: 0.6020\n",
      "Epoch 956/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.5289 - acc: 0.6249 - val_loss: 49.4585 - val_acc: 0.6014\n",
      "Epoch 957/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.5460 - acc: 0.6251 - val_loss: 49.4492 - val_acc: 0.6018\n",
      "Epoch 958/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.5513 - acc: 0.6248 - val_loss: 49.4502 - val_acc: 0.6019\n",
      "Epoch 959/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.5384 - acc: 0.6251 - val_loss: 49.4848 - val_acc: 0.6018\n",
      "Epoch 960/1500\n",
      "55/55 [==============================] - 3s 57ms/step - loss: 52.5212 - acc: 0.6246 - val_loss: 49.4584 - val_acc: 0.6017\n",
      "Epoch 961/1500\n",
      "55/55 [==============================] - 3s 57ms/step - loss: 52.4788 - acc: 0.6252 - val_loss: 49.4403 - val_acc: 0.6019\n",
      "Epoch 962/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 52.4691 - acc: 0.6257 - val_loss: 49.4362 - val_acc: 0.6020\n",
      "Epoch 963/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.4896 - acc: 0.6253 - val_loss: 49.4179 - val_acc: 0.6012\n",
      "Epoch 964/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4485 - acc: 0.6251 - val_loss: 49.4725 - val_acc: 0.6013\n",
      "Epoch 965/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4442 - acc: 0.6250 - val_loss: 49.4559 - val_acc: 0.6021\n",
      "Epoch 966/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.4569 - acc: 0.6255 - val_loss: 49.4294 - val_acc: 0.6015\n",
      "Epoch 967/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.4862 - acc: 0.6253 - val_loss: 49.4396 - val_acc: 0.6014\n",
      "Epoch 968/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.4760 - acc: 0.6248 - val_loss: 49.4448 - val_acc: 0.6014\n",
      "Epoch 969/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4755 - acc: 0.6243 - val_loss: 49.4244 - val_acc: 0.6010\n",
      "Epoch 970/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4533 - acc: 0.6252 - val_loss: 49.4708 - val_acc: 0.6015\n",
      "Epoch 971/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4416 - acc: 0.6252 - val_loss: 49.4346 - val_acc: 0.6017\n",
      "Epoch 972/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4387 - acc: 0.6251 - val_loss: 49.4314 - val_acc: 0.6019\n",
      "Epoch 973/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.4538 - acc: 0.6250 - val_loss: 49.4322 - val_acc: 0.6021\n",
      "Epoch 974/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.4287 - acc: 0.6255 - val_loss: 49.4491 - val_acc: 0.6023\n",
      "Epoch 975/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.3771 - acc: 0.6254 - val_loss: 49.4603 - val_acc: 0.6019\n",
      "Epoch 976/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.4451 - acc: 0.6256 - val_loss: 49.4344 - val_acc: 0.6023\n",
      "Epoch 977/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.3887 - acc: 0.6254 - val_loss: 49.4312 - val_acc: 0.6018\n",
      "Epoch 978/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.4032 - acc: 0.6251 - val_loss: 49.4517 - val_acc: 0.6021\n",
      "Epoch 979/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 52.4402 - acc: 0.6254 - val_loss: 49.4603 - val_acc: 0.6017\n",
      "Epoch 980/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.4458 - acc: 0.6252 - val_loss: 49.4166 - val_acc: 0.6020\n",
      "Epoch 981/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.3783 - acc: 0.6254 - val_loss: 49.4532 - val_acc: 0.6021\n",
      "Epoch 982/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.3670 - acc: 0.6256 - val_loss: 49.4387 - val_acc: 0.6016\n",
      "Epoch 983/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.3848 - acc: 0.6259 - val_loss: 49.4199 - val_acc: 0.6019\n",
      "Epoch 984/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.3557 - acc: 0.6253 - val_loss: 49.4264 - val_acc: 0.6018\n",
      "Epoch 985/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.3558 - acc: 0.6256 - val_loss: 49.4293 - val_acc: 0.6022\n",
      "Epoch 986/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.3636 - acc: 0.6250 - val_loss: 49.4420 - val_acc: 0.6016\n",
      "Epoch 987/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.3605 - acc: 0.6256 - val_loss: 49.4294 - val_acc: 0.6024\n",
      "Epoch 988/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 52.3315 - acc: 0.6254 - val_loss: 49.4112 - val_acc: 0.6024\n",
      "Epoch 989/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 52.3484 - acc: 0.6259 - val_loss: 49.4542 - val_acc: 0.6025\n",
      "Epoch 990/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.3357 - acc: 0.6255 - val_loss: 49.4751 - val_acc: 0.6022\n",
      "Epoch 991/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 52.3258 - acc: 0.6258 - val_loss: 49.4604 - val_acc: 0.6020\n",
      "Epoch 992/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 52.3257 - acc: 0.6254 - val_loss: 49.4574 - val_acc: 0.6017\n",
      "Epoch 993/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 52.3211 - acc: 0.6256 - val_loss: 49.4585 - val_acc: 0.6021\n",
      "Epoch 994/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.2735 - acc: 0.6261 - val_loss: 49.4502 - val_acc: 0.6028\n",
      "Epoch 995/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.2957 - acc: 0.6259 - val_loss: 49.4781 - val_acc: 0.6021\n",
      "Epoch 996/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.2904 - acc: 0.6251 - val_loss: 49.4844 - val_acc: 0.6028\n",
      "Epoch 997/1500\n",
      "55/55 [==============================] - 3s 60ms/step - loss: 52.2870 - acc: 0.6256 - val_loss: 49.4556 - val_acc: 0.6020\n",
      "Epoch 998/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.2604 - acc: 0.6258 - val_loss: 49.4524 - val_acc: 0.6024\n",
      "Epoch 999/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2841 - acc: 0.6256 - val_loss: 49.4377 - val_acc: 0.6024\n",
      "Epoch 1000/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.2834 - acc: 0.6258 - val_loss: 49.4389 - val_acc: 0.6019\n",
      "Epoch 1001/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.2336 - acc: 0.6262 - val_loss: 49.4403 - val_acc: 0.6023\n",
      "Epoch 1002/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 52.2393 - acc: 0.6265 - val_loss: 49.4565 - val_acc: 0.6022\n",
      "Epoch 1003/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.2747 - acc: 0.6257 - val_loss: 49.4383 - val_acc: 0.6023\n",
      "Epoch 1004/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2489 - acc: 0.6261 - val_loss: 49.4403 - val_acc: 0.6020\n",
      "Epoch 1005/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.2640 - acc: 0.6259 - val_loss: 49.4483 - val_acc: 0.6021\n",
      "Epoch 1006/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2768 - acc: 0.6257 - val_loss: 49.4387 - val_acc: 0.6014\n",
      "Epoch 1007/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2195 - acc: 0.6259 - val_loss: 49.4377 - val_acc: 0.6017\n",
      "Epoch 1008/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.2298 - acc: 0.6258 - val_loss: 49.4590 - val_acc: 0.6021\n",
      "Epoch 1009/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.2259 - acc: 0.6254 - val_loss: 49.4437 - val_acc: 0.6020\n",
      "Epoch 1010/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2099 - acc: 0.6262 - val_loss: 49.4346 - val_acc: 0.6024\n",
      "Epoch 1011/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 52.2378 - acc: 0.6260 - val_loss: 49.4554 - val_acc: 0.6023\n",
      "Epoch 1012/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.1692 - acc: 0.6259 - val_loss: 49.4420 - val_acc: 0.6021\n",
      "Epoch 1013/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.2177 - acc: 0.6264 - val_loss: 49.4371 - val_acc: 0.6020\n",
      "Epoch 1014/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 52.1513 - acc: 0.6261 - val_loss: 49.4407 - val_acc: 0.6020\n",
      "Epoch 1015/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.1899 - acc: 0.6261 - val_loss: 49.4734 - val_acc: 0.6019\n",
      "Epoch 1016/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.2023 - acc: 0.6264 - val_loss: 49.4450 - val_acc: 0.6019\n",
      "Epoch 1017/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.1519 - acc: 0.6264 - val_loss: 49.4541 - val_acc: 0.6015\n",
      "Epoch 1018/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.1625 - acc: 0.6263 - val_loss: 49.4366 - val_acc: 0.6023\n",
      "Epoch 1019/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 52.1524 - acc: 0.6264 - val_loss: 49.4755 - val_acc: 0.6025\n",
      "Epoch 1020/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.1481 - acc: 0.6264 - val_loss: 49.4438 - val_acc: 0.6030\n",
      "Epoch 1021/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.1597 - acc: 0.6263 - val_loss: 49.4136 - val_acc: 0.6027\n",
      "Epoch 1022/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.1198 - acc: 0.6264 - val_loss: 49.4311 - val_acc: 0.6030\n",
      "Epoch 1023/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.1416 - acc: 0.6265 - val_loss: 49.4355 - val_acc: 0.6026\n",
      "Epoch 1024/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.1432 - acc: 0.6268 - val_loss: 49.4146 - val_acc: 0.6024\n",
      "Epoch 1025/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.0755 - acc: 0.6268 - val_loss: 49.4197 - val_acc: 0.6020\n",
      "Epoch 1026/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.1327 - acc: 0.6259 - val_loss: 49.4636 - val_acc: 0.6020\n",
      "Epoch 1027/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 52.0953 - acc: 0.6262 - val_loss: 49.4343 - val_acc: 0.6018\n",
      "Epoch 1028/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.1069 - acc: 0.6261 - val_loss: 49.4763 - val_acc: 0.6026\n",
      "Epoch 1029/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0997 - acc: 0.6265 - val_loss: 49.4489 - val_acc: 0.6023\n",
      "Epoch 1030/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0898 - acc: 0.6266 - val_loss: 49.4289 - val_acc: 0.6023\n",
      "Epoch 1031/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.0688 - acc: 0.6267 - val_loss: 49.4465 - val_acc: 0.6027\n",
      "Epoch 1032/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 52.0704 - acc: 0.6263 - val_loss: 49.4641 - val_acc: 0.6033\n",
      "Epoch 1033/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.0810 - acc: 0.6264 - val_loss: 49.4753 - val_acc: 0.6022\n",
      "Epoch 1034/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.1090 - acc: 0.6262 - val_loss: 49.4360 - val_acc: 0.6028\n",
      "Epoch 1035/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.1008 - acc: 0.6262 - val_loss: 49.4466 - val_acc: 0.6024\n",
      "Epoch 1036/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0822 - acc: 0.6264 - val_loss: 49.4529 - val_acc: 0.6021\n",
      "Epoch 1037/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.0415 - acc: 0.6266 - val_loss: 49.4352 - val_acc: 0.6025\n",
      "Epoch 1038/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 52.0516 - acc: 0.6263 - val_loss: 49.4595 - val_acc: 0.6029\n",
      "Epoch 1039/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.0574 - acc: 0.6268 - val_loss: 49.4380 - val_acc: 0.6030\n",
      "Epoch 1040/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 52.0388 - acc: 0.6262 - val_loss: 49.4425 - val_acc: 0.6031\n",
      "Epoch 1041/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0234 - acc: 0.6266 - val_loss: 49.4340 - val_acc: 0.6029\n",
      "Epoch 1042/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 52.0185 - acc: 0.6274 - val_loss: 49.4361 - val_acc: 0.6023\n",
      "Epoch 1043/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.0377 - acc: 0.6269 - val_loss: 49.4412 - val_acc: 0.6025\n",
      "Epoch 1044/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 52.0209 - acc: 0.6267 - val_loss: 49.4275 - val_acc: 0.6033\n",
      "Epoch 1045/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0180 - acc: 0.6264 - val_loss: 49.4049 - val_acc: 0.6030\n",
      "Epoch 1046/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 52.0080 - acc: 0.6272 - val_loss: 49.4289 - val_acc: 0.6029\n",
      "Epoch 1047/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0152 - acc: 0.6273 - val_loss: 49.4249 - val_acc: 0.6014\n",
      "Epoch 1048/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 52.0026 - acc: 0.6269 - val_loss: 49.4099 - val_acc: 0.6016\n",
      "Epoch 1049/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 52.0154 - acc: 0.6263 - val_loss: 49.4249 - val_acc: 0.6023\n",
      "Epoch 1050/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.9762 - acc: 0.6266 - val_loss: 49.4085 - val_acc: 0.6030\n",
      "Epoch 1051/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.9638 - acc: 0.6262 - val_loss: 49.4506 - val_acc: 0.6025\n",
      "Epoch 1052/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.9818 - acc: 0.6280 - val_loss: 49.4470 - val_acc: 0.6017\n",
      "Epoch 1053/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.9679 - acc: 0.6268 - val_loss: 49.4401 - val_acc: 0.6025\n",
      "Epoch 1054/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.9703 - acc: 0.6271 - val_loss: 49.4190 - val_acc: 0.6026\n",
      "Epoch 1055/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.9493 - acc: 0.6269 - val_loss: 49.4349 - val_acc: 0.6026\n",
      "Epoch 1056/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.9204 - acc: 0.6271 - val_loss: 49.4391 - val_acc: 0.6026\n",
      "Epoch 1057/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.9409 - acc: 0.6266 - val_loss: 49.4398 - val_acc: 0.6025\n",
      "Epoch 1058/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.9549 - acc: 0.6276 - val_loss: 49.4447 - val_acc: 0.6028\n",
      "Epoch 1059/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.9300 - acc: 0.6266 - val_loss: 49.4712 - val_acc: 0.6022\n",
      "Epoch 1060/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.9691 - acc: 0.6270 - val_loss: 49.4208 - val_acc: 0.6023\n",
      "Epoch 1061/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.9081 - acc: 0.6275 - val_loss: 49.4425 - val_acc: 0.6026\n",
      "Epoch 1062/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.9238 - acc: 0.6274 - val_loss: 49.4447 - val_acc: 0.6023\n",
      "Epoch 1063/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.9169 - acc: 0.6265 - val_loss: 49.4440 - val_acc: 0.6028\n",
      "Epoch 1064/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8973 - acc: 0.6275 - val_loss: 49.4395 - val_acc: 0.6029\n",
      "Epoch 1065/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.9046 - acc: 0.6274 - val_loss: 49.4912 - val_acc: 0.6026\n",
      "Epoch 1066/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.9347 - acc: 0.6268 - val_loss: 49.4375 - val_acc: 0.6025\n",
      "Epoch 1067/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.8764 - acc: 0.6270 - val_loss: 49.4468 - val_acc: 0.6024\n",
      "Epoch 1068/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.8806 - acc: 0.6274 - val_loss: 49.4500 - val_acc: 0.6021\n",
      "Epoch 1069/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8873 - acc: 0.6270 - val_loss: 49.4626 - val_acc: 0.6019\n",
      "Epoch 1070/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.8707 - acc: 0.6273 - val_loss: 49.4377 - val_acc: 0.6028\n",
      "Epoch 1071/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8864 - acc: 0.6269 - val_loss: 49.4557 - val_acc: 0.6026\n",
      "Epoch 1072/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.8687 - acc: 0.6271 - val_loss: 49.4318 - val_acc: 0.6030\n",
      "Epoch 1073/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.8806 - acc: 0.6269 - val_loss: 49.4239 - val_acc: 0.6030\n",
      "Epoch 1074/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.8229 - acc: 0.6278 - val_loss: 49.4515 - val_acc: 0.6028\n",
      "Epoch 1075/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.8620 - acc: 0.6275 - val_loss: 49.4048 - val_acc: 0.6032\n",
      "Epoch 1076/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8473 - acc: 0.6274 - val_loss: 49.4153 - val_acc: 0.6027\n",
      "Epoch 1077/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8586 - acc: 0.6277 - val_loss: 49.4278 - val_acc: 0.6026\n",
      "Epoch 1078/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.8326 - acc: 0.6277 - val_loss: 49.4256 - val_acc: 0.6025\n",
      "Epoch 1079/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.8229 - acc: 0.6274 - val_loss: 49.4471 - val_acc: 0.6023\n",
      "Epoch 1080/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8106 - acc: 0.6275 - val_loss: 49.4552 - val_acc: 0.6029\n",
      "Epoch 1081/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.8104 - acc: 0.6273 - val_loss: 49.4544 - val_acc: 0.6030\n",
      "Epoch 1082/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.8043 - acc: 0.6273 - val_loss: 49.4418 - val_acc: 0.6029\n",
      "Epoch 1083/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.7873 - acc: 0.6280 - val_loss: 49.4452 - val_acc: 0.6032\n",
      "Epoch 1084/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.8298 - acc: 0.6276 - val_loss: 49.4125 - val_acc: 0.6032\n",
      "Epoch 1085/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.7689 - acc: 0.6276 - val_loss: 49.4455 - val_acc: 0.6029\n",
      "Epoch 1086/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.7534 - acc: 0.6273 - val_loss: 49.4018 - val_acc: 0.6019\n",
      "Epoch 1087/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8102 - acc: 0.6272 - val_loss: 49.4361 - val_acc: 0.6025\n",
      "Epoch 1088/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.8024 - acc: 0.6280 - val_loss: 49.4288 - val_acc: 0.6028\n",
      "Epoch 1089/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.7494 - acc: 0.6278 - val_loss: 49.4275 - val_acc: 0.6030\n",
      "Epoch 1090/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.7715 - acc: 0.6272 - val_loss: 49.4512 - val_acc: 0.6021\n",
      "Epoch 1091/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.7474 - acc: 0.6273 - val_loss: 49.4144 - val_acc: 0.6031\n",
      "Epoch 1092/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.7881 - acc: 0.6276 - val_loss: 49.4267 - val_acc: 0.6031\n",
      "Epoch 1093/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.6971 - acc: 0.6278 - val_loss: 49.4537 - val_acc: 0.6031\n",
      "Epoch 1094/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.7133 - acc: 0.6274 - val_loss: 49.4378 - val_acc: 0.6024\n",
      "Epoch 1095/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.7575 - acc: 0.6277 - val_loss: 49.4171 - val_acc: 0.6027\n",
      "Epoch 1096/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.7381 - acc: 0.6277 - val_loss: 49.4634 - val_acc: 0.6039\n",
      "Epoch 1097/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.7461 - acc: 0.6275 - val_loss: 49.4455 - val_acc: 0.6034\n",
      "Epoch 1098/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.7287 - acc: 0.6282 - val_loss: 49.4351 - val_acc: 0.6030\n",
      "Epoch 1099/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.7079 - acc: 0.6284 - val_loss: 49.4452 - val_acc: 0.6039\n",
      "Epoch 1100/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.7261 - acc: 0.6277 - val_loss: 49.4385 - val_acc: 0.6032\n",
      "Epoch 1101/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.7312 - acc: 0.6283 - val_loss: 49.4479 - val_acc: 0.6023\n",
      "Epoch 1102/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.6898 - acc: 0.6276 - val_loss: 49.4673 - val_acc: 0.6031\n",
      "Epoch 1103/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.7030 - acc: 0.6278 - val_loss: 49.4431 - val_acc: 0.6032\n",
      "Epoch 1104/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.6704 - acc: 0.6278 - val_loss: 49.4279 - val_acc: 0.6027\n",
      "Epoch 1105/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.6523 - acc: 0.6288 - val_loss: 49.4494 - val_acc: 0.6031\n",
      "Epoch 1106/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.7227 - acc: 0.6278 - val_loss: 49.4382 - val_acc: 0.6037\n",
      "Epoch 1107/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.6583 - acc: 0.6281 - val_loss: 49.4599 - val_acc: 0.6033\n",
      "Epoch 1108/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.6786 - acc: 0.6282 - val_loss: 49.4511 - val_acc: 0.6031\n",
      "Epoch 1109/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.6330 - acc: 0.6281 - val_loss: 49.4171 - val_acc: 0.6033\n",
      "Epoch 1110/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.6462 - acc: 0.6277 - val_loss: 49.4295 - val_acc: 0.6035\n",
      "Epoch 1111/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.6542 - acc: 0.6277 - val_loss: 49.4356 - val_acc: 0.6026\n",
      "Epoch 1112/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.6664 - acc: 0.6281 - val_loss: 49.4377 - val_acc: 0.6031\n",
      "Epoch 1113/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.6532 - acc: 0.6277 - val_loss: 49.4545 - val_acc: 0.6025\n",
      "Epoch 1114/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.6358 - acc: 0.6278 - val_loss: 49.4531 - val_acc: 0.6030\n",
      "Epoch 1115/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.6598 - acc: 0.6281 - val_loss: 49.4648 - val_acc: 0.6029\n",
      "Epoch 1116/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.5950 - acc: 0.6282 - val_loss: 49.4590 - val_acc: 0.6037\n",
      "Epoch 1117/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.6537 - acc: 0.6285 - val_loss: 49.4296 - val_acc: 0.6025\n",
      "Epoch 1118/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.5645 - acc: 0.6281 - val_loss: 49.4396 - val_acc: 0.6025\n",
      "Epoch 1119/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.6516 - acc: 0.6279 - val_loss: 49.4444 - val_acc: 0.6027\n",
      "Epoch 1120/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.5871 - acc: 0.6277 - val_loss: 49.4330 - val_acc: 0.6030\n",
      "Epoch 1121/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.6130 - acc: 0.6282 - val_loss: 49.4371 - val_acc: 0.6030\n",
      "Epoch 1122/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.5921 - acc: 0.6285 - val_loss: 49.4478 - val_acc: 0.6030\n",
      "Epoch 1123/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.5956 - acc: 0.6279 - val_loss: 49.4388 - val_acc: 0.6026\n",
      "Epoch 1124/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.5938 - acc: 0.6282 - val_loss: 49.4589 - val_acc: 0.6027\n",
      "Epoch 1125/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.5552 - acc: 0.6281 - val_loss: 49.4457 - val_acc: 0.6033\n",
      "Epoch 1126/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.5567 - acc: 0.6284 - val_loss: 49.4531 - val_acc: 0.6037\n",
      "Epoch 1127/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.5481 - acc: 0.6283 - val_loss: 49.4584 - val_acc: 0.6032\n",
      "Epoch 1128/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.5552 - acc: 0.6282 - val_loss: 49.4345 - val_acc: 0.6033\n",
      "Epoch 1129/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 51.5455 - acc: 0.6284 - val_loss: 49.4507 - val_acc: 0.6031\n",
      "Epoch 1130/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.5426 - acc: 0.6286 - val_loss: 49.4544 - val_acc: 0.6033\n",
      "Epoch 1131/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 51.5444 - acc: 0.6283 - val_loss: 49.4306 - val_acc: 0.6027\n",
      "Epoch 1132/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.5284 - acc: 0.6282 - val_loss: 49.4327 - val_acc: 0.6035\n",
      "Epoch 1133/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.5326 - acc: 0.6288 - val_loss: 49.4746 - val_acc: 0.6033\n",
      "Epoch 1134/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.5609 - acc: 0.6286 - val_loss: 49.4353 - val_acc: 0.6031\n",
      "Epoch 1135/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.5045 - acc: 0.6283 - val_loss: 49.4367 - val_acc: 0.6041\n",
      "Epoch 1136/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.5458 - acc: 0.6281 - val_loss: 49.4351 - val_acc: 0.6040\n",
      "Epoch 1137/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.5569 - acc: 0.6283 - val_loss: 49.4547 - val_acc: 0.6031\n",
      "Epoch 1138/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.4983 - acc: 0.6286 - val_loss: 49.4677 - val_acc: 0.6030\n",
      "Epoch 1139/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.4903 - acc: 0.6288 - val_loss: 49.4362 - val_acc: 0.6033\n",
      "Epoch 1140/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.5206 - acc: 0.6281 - val_loss: 49.4373 - val_acc: 0.6033\n",
      "Epoch 1141/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.4760 - acc: 0.6284 - val_loss: 49.4468 - val_acc: 0.6029\n",
      "Epoch 1142/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.5028 - acc: 0.6285 - val_loss: 49.4157 - val_acc: 0.6024\n",
      "Epoch 1143/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.4935 - acc: 0.6285 - val_loss: 49.4635 - val_acc: 0.6030\n",
      "Epoch 1144/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.5003 - acc: 0.6285 - val_loss: 49.4385 - val_acc: 0.6025\n",
      "Epoch 1145/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.4726 - acc: 0.6282 - val_loss: 49.4319 - val_acc: 0.6029\n",
      "Epoch 1146/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.4538 - acc: 0.6283 - val_loss: 49.4426 - val_acc: 0.6032\n",
      "Epoch 1147/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.4839 - acc: 0.6286 - val_loss: 49.4545 - val_acc: 0.6039\n",
      "Epoch 1148/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 51.4780 - acc: 0.6288 - val_loss: 49.4427 - val_acc: 0.6026\n",
      "Epoch 1149/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 51.4347 - acc: 0.6282 - val_loss: 49.3959 - val_acc: 0.6032\n",
      "Epoch 1150/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.4610 - acc: 0.6290 - val_loss: 49.4257 - val_acc: 0.6031\n",
      "Epoch 1151/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.4639 - acc: 0.6289 - val_loss: 49.4618 - val_acc: 0.6033\n",
      "Epoch 1152/1500\n",
      "55/55 [==============================] - 2s 42ms/step - loss: 51.4752 - acc: 0.6284 - val_loss: 49.4547 - val_acc: 0.6025\n",
      "Epoch 1153/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.4447 - acc: 0.6285 - val_loss: 49.4313 - val_acc: 0.6026\n",
      "Epoch 1154/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.4498 - acc: 0.6286 - val_loss: 49.4296 - val_acc: 0.6037\n",
      "Epoch 1155/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.4245 - acc: 0.6286 - val_loss: 49.4609 - val_acc: 0.6033\n",
      "Epoch 1156/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.4197 - acc: 0.6291 - val_loss: 49.4492 - val_acc: 0.6029\n",
      "Epoch 1157/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.4336 - acc: 0.6289 - val_loss: 49.4331 - val_acc: 0.6026\n",
      "Epoch 1158/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.4234 - acc: 0.6290 - val_loss: 49.4421 - val_acc: 0.6033\n",
      "Epoch 1159/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3978 - acc: 0.6289 - val_loss: 49.4926 - val_acc: 0.6029\n",
      "Epoch 1160/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.4301 - acc: 0.6284 - val_loss: 49.4686 - val_acc: 0.6034\n",
      "Epoch 1161/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.3669 - acc: 0.6290 - val_loss: 49.4560 - val_acc: 0.6029\n",
      "Epoch 1162/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3739 - acc: 0.6288 - val_loss: 49.4506 - val_acc: 0.6036\n",
      "Epoch 1163/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.4031 - acc: 0.6286 - val_loss: 49.4769 - val_acc: 0.6034\n",
      "Epoch 1164/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.3879 - acc: 0.6296 - val_loss: 49.4420 - val_acc: 0.6027\n",
      "Epoch 1165/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3838 - acc: 0.6288 - val_loss: 49.4715 - val_acc: 0.6024\n",
      "Epoch 1166/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.3848 - acc: 0.6287 - val_loss: 49.4495 - val_acc: 0.6030\n",
      "Epoch 1167/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.3258 - acc: 0.6291 - val_loss: 49.4457 - val_acc: 0.6044\n",
      "Epoch 1168/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.3225 - acc: 0.6292 - val_loss: 49.4765 - val_acc: 0.6036\n",
      "Epoch 1169/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.3204 - acc: 0.6290 - val_loss: 49.4798 - val_acc: 0.6034\n",
      "Epoch 1170/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.3311 - acc: 0.6289 - val_loss: 49.4584 - val_acc: 0.6031\n",
      "Epoch 1171/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.3561 - acc: 0.6296 - val_loss: 49.4468 - val_acc: 0.6032\n",
      "Epoch 1172/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.3205 - acc: 0.6290 - val_loss: 49.4678 - val_acc: 0.6038\n",
      "Epoch 1173/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.3292 - acc: 0.6288 - val_loss: 49.4582 - val_acc: 0.6042\n",
      "Epoch 1174/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.3420 - acc: 0.6293 - val_loss: 49.4522 - val_acc: 0.6038\n",
      "Epoch 1175/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.3386 - acc: 0.6288 - val_loss: 49.4534 - val_acc: 0.6030\n",
      "Epoch 1176/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3377 - acc: 0.6291 - val_loss: 49.4643 - val_acc: 0.6032\n",
      "Epoch 1177/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.3063 - acc: 0.6291 - val_loss: 49.4180 - val_acc: 0.6029\n",
      "Epoch 1178/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.3351 - acc: 0.6294 - val_loss: 49.4299 - val_acc: 0.6028\n",
      "Epoch 1179/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.3394 - acc: 0.6286 - val_loss: 49.4326 - val_acc: 0.6032\n",
      "Epoch 1180/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.2996 - acc: 0.6292 - val_loss: 49.4420 - val_acc: 0.6036\n",
      "Epoch 1181/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3056 - acc: 0.6294 - val_loss: 49.4345 - val_acc: 0.6037\n",
      "Epoch 1182/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.2826 - acc: 0.6294 - val_loss: 49.4537 - val_acc: 0.6028\n",
      "Epoch 1183/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.2913 - acc: 0.6286 - val_loss: 49.4618 - val_acc: 0.6043\n",
      "Epoch 1184/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.3014 - acc: 0.6294 - val_loss: 49.4394 - val_acc: 0.6037\n",
      "Epoch 1185/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.3256 - acc: 0.6298 - val_loss: 49.4549 - val_acc: 0.6033\n",
      "Epoch 1186/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.3101 - acc: 0.6294 - val_loss: 49.4622 - val_acc: 0.6036\n",
      "Epoch 1187/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.3056 - acc: 0.6295 - val_loss: 49.4224 - val_acc: 0.6033\n",
      "Epoch 1188/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.2772 - acc: 0.6294 - val_loss: 49.4705 - val_acc: 0.6036\n",
      "Epoch 1189/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.2540 - acc: 0.6297 - val_loss: 49.4608 - val_acc: 0.6033\n",
      "Epoch 1190/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.2727 - acc: 0.6293 - val_loss: 49.4474 - val_acc: 0.6028\n",
      "Epoch 1191/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.2794 - acc: 0.6298 - val_loss: 49.4557 - val_acc: 0.6037\n",
      "Epoch 1192/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 51.2523 - acc: 0.6292 - val_loss: 49.4366 - val_acc: 0.6037\n",
      "Epoch 1193/1500\n",
      "55/55 [==============================] - 3s 55ms/step - loss: 51.2185 - acc: 0.6296 - val_loss: 49.4544 - val_acc: 0.6041\n",
      "Epoch 1194/1500\n",
      "55/55 [==============================] - 3s 57ms/step - loss: 51.2677 - acc: 0.6288 - val_loss: 49.4534 - val_acc: 0.6035\n",
      "Epoch 1195/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 51.2438 - acc: 0.6295 - val_loss: 49.4634 - val_acc: 0.6034\n",
      "Epoch 1196/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.2275 - acc: 0.6288 - val_loss: 49.4524 - val_acc: 0.6040\n",
      "Epoch 1197/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.2193 - acc: 0.6300 - val_loss: 49.4627 - val_acc: 0.6037\n",
      "Epoch 1198/1500\n",
      "55/55 [==============================] - 2s 43ms/step - loss: 51.2497 - acc: 0.6296 - val_loss: 49.4599 - val_acc: 0.6037\n",
      "Epoch 1199/1500\n",
      "55/55 [==============================] - 2s 44ms/step - loss: 51.2405 - acc: 0.6293 - val_loss: 49.4581 - val_acc: 0.6037\n",
      "Epoch 1200/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.2222 - acc: 0.6295 - val_loss: 49.4342 - val_acc: 0.6040\n",
      "Epoch 1201/1500\n",
      "55/55 [==============================] - 3s 59ms/step - loss: 51.2262 - acc: 0.6300 - val_loss: 49.4644 - val_acc: 0.6036\n",
      "Epoch 1202/1500\n",
      "55/55 [==============================] - 3s 56ms/step - loss: 51.2056 - acc: 0.6297 - val_loss: 49.4580 - val_acc: 0.6034\n",
      "Epoch 1203/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.2304 - acc: 0.6298 - val_loss: 49.4532 - val_acc: 0.6044\n",
      "Epoch 1204/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.1950 - acc: 0.6300 - val_loss: 49.4468 - val_acc: 0.6030\n",
      "Epoch 1205/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.2003 - acc: 0.6293 - val_loss: 49.4636 - val_acc: 0.6026\n",
      "Epoch 1206/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.1816 - acc: 0.6295 - val_loss: 49.4643 - val_acc: 0.6035\n",
      "Epoch 1207/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 51.1908 - acc: 0.6293 - val_loss: 49.5105 - val_acc: 0.6035\n",
      "Epoch 1208/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1629 - acc: 0.6298 - val_loss: 49.4302 - val_acc: 0.6038\n",
      "Epoch 1209/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1489 - acc: 0.6296 - val_loss: 49.4721 - val_acc: 0.6042\n",
      "Epoch 1210/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.1806 - acc: 0.6297 - val_loss: 49.4555 - val_acc: 0.6035\n",
      "Epoch 1211/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1533 - acc: 0.6294 - val_loss: 49.4561 - val_acc: 0.6043\n",
      "Epoch 1212/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.1989 - acc: 0.6296 - val_loss: 49.4377 - val_acc: 0.6030\n",
      "Epoch 1213/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.1928 - acc: 0.6293 - val_loss: 49.4463 - val_acc: 0.6039\n",
      "Epoch 1214/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.1012 - acc: 0.6299 - val_loss: 49.4578 - val_acc: 0.6030\n",
      "Epoch 1215/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.1621 - acc: 0.6296 - val_loss: 49.4631 - val_acc: 0.6038\n",
      "Epoch 1216/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1212 - acc: 0.6305 - val_loss: 49.4533 - val_acc: 0.6037\n",
      "Epoch 1217/1500\n",
      "55/55 [==============================] - 3s 52ms/step - loss: 51.1065 - acc: 0.6300 - val_loss: 49.4401 - val_acc: 0.6033\n",
      "Epoch 1218/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.1165 - acc: 0.6297 - val_loss: 49.4334 - val_acc: 0.6038\n",
      "Epoch 1219/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1208 - acc: 0.6294 - val_loss: 49.4624 - val_acc: 0.6036\n",
      "Epoch 1220/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.1448 - acc: 0.6301 - val_loss: 49.4546 - val_acc: 0.6036\n",
      "Epoch 1221/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.1154 - acc: 0.6301 - val_loss: 49.4519 - val_acc: 0.6038\n",
      "Epoch 1222/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.0636 - acc: 0.6292 - val_loss: 49.4601 - val_acc: 0.6033\n",
      "Epoch 1223/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.1321 - acc: 0.6299 - val_loss: 49.4289 - val_acc: 0.6036\n",
      "Epoch 1224/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 51.1084 - acc: 0.6303 - val_loss: 49.4432 - val_acc: 0.6040\n",
      "Epoch 1225/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.0952 - acc: 0.6304 - val_loss: 49.4528 - val_acc: 0.6040\n",
      "Epoch 1226/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.0626 - acc: 0.6302 - val_loss: 49.4397 - val_acc: 0.6041\n",
      "Epoch 1227/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.0700 - acc: 0.6298 - val_loss: 49.4706 - val_acc: 0.6036\n",
      "Epoch 1228/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.0770 - acc: 0.6302 - val_loss: 49.4607 - val_acc: 0.6040\n",
      "Epoch 1229/1500\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 51.0961 - acc: 0.6297 - val_loss: 49.4390 - val_acc: 0.6035\n",
      "Epoch 1230/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.0594 - acc: 0.6300 - val_loss: 49.4375 - val_acc: 0.6039\n",
      "Epoch 1231/1500\n",
      "55/55 [==============================] - 3s 57ms/step - loss: 51.0822 - acc: 0.6299 - val_loss: 49.4525 - val_acc: 0.6043\n",
      "Epoch 1232/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 51.0174 - acc: 0.6297 - val_loss: 49.4701 - val_acc: 0.6040\n",
      "Epoch 1233/1500\n",
      "55/55 [==============================] - 2s 45ms/step - loss: 51.0303 - acc: 0.6303 - val_loss: 49.4525 - val_acc: 0.6048\n",
      "Epoch 1234/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 51.0790 - acc: 0.6295 - val_loss: 49.4519 - val_acc: 0.6037\n",
      "Epoch 1235/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 51.0450 - acc: 0.6302 - val_loss: 49.4351 - val_acc: 0.6045\n",
      "Epoch 1236/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.0127 - acc: 0.6305 - val_loss: 49.4532 - val_acc: 0.6049\n",
      "Epoch 1237/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.0533 - acc: 0.6304 - val_loss: 49.4416 - val_acc: 0.6042\n",
      "Epoch 1238/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.0272 - acc: 0.6300 - val_loss: 49.4581 - val_acc: 0.6031\n",
      "Epoch 1239/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.0263 - acc: 0.6299 - val_loss: 49.4397 - val_acc: 0.6040\n",
      "Epoch 1240/1500\n",
      "55/55 [==============================] - 3s 54ms/step - loss: 50.9806 - acc: 0.6303 - val_loss: 49.4849 - val_acc: 0.6034\n",
      "Epoch 1241/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 51.0247 - acc: 0.6300 - val_loss: 49.4598 - val_acc: 0.6044\n",
      "Epoch 1242/1500\n",
      "55/55 [==============================] - 3s 46ms/step - loss: 51.0002 - acc: 0.6302 - val_loss: 49.4860 - val_acc: 0.6040\n",
      "Epoch 1243/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 50.9800 - acc: 0.6306 - val_loss: 49.4561 - val_acc: 0.6043\n",
      "Epoch 1244/1500\n",
      "55/55 [==============================] - 3s 49ms/step - loss: 51.0354 - acc: 0.6301 - val_loss: 49.4701 - val_acc: 0.6041\n",
      "Epoch 1245/1500\n",
      "55/55 [==============================] - 3s 50ms/step - loss: 50.9792 - acc: 0.6298 - val_loss: 49.4421 - val_acc: 0.6045\n",
      "Epoch 1246/1500\n",
      "55/55 [==============================] - 3s 51ms/step - loss: 51.0282 - acc: 0.6299 - val_loss: 49.4607 - val_acc: 0.6039\n",
      "Epoch 1247/1500\n",
      "55/55 [==============================] - 2s 46ms/step - loss: 51.0010 - acc: 0.6302 - val_loss: 49.4465 - val_acc: 0.6035\n",
      "Epoch 1248/1500\n",
      "55/55 [==============================] - 3s 48ms/step - loss: 50.9701 - acc: 0.6302 - val_loss: 49.4522 - val_acc: 0.6041\n",
      "Epoch 1249/1500\n",
      "55/55 [==============================] - 3s 47ms/step - loss: 50.9707 - acc: 0.6303 - val_loss: 49.4539 - val_acc: 0.6038\n"
     ]
    }
   ],
   "source": [
    "if not loadModel:\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=patience),\n",
    "            tensorboard_callback,\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as graph_conv_layer_call_fn, graph_conv_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kerasModels/top50MSE30minsV6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: kerasModels/top50MSE30minsV6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001A3FF452760> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001A4FEB23340> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "if saveModel and not loadModel:\n",
    "    model.save(modelSaveLocation)\n",
    "elif saveModel and loadModel:\n",
    "    raise Exception(\n",
    "        \"You have enabled both loading and saving, which are incompatible. Model has not been saved\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not loadModel:\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadModel:\n",
    "    model = keras.models.load_model(modelSaveLocation)\n",
    "    model.summary()\n",
    "else:\n",
    "    model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analize model performance on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook gives the performance per forecast horizon in the first plot. In the second plot, the predicted airport delay over time is compared to the real delay over time to illustrate the accuracy of the model. The default setting displays a window of 100 hours. As the model has a multi-horizon feature, you can choose for how many hours into the future to predict. The model is trained to predict between 1 and 10 hours into the future. You can choose to display a bigger window with the windowSize feature and how many hours ahead predicted should be illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours ahead = 5.0\n"
     ]
    }
   ],
   "source": [
    "windowSize = 200  # 1-400\n",
    "\n",
    "forecastlen = 10  # 1-10\n",
    "forecastlen = min(forecastlen, forecast_horizon)\n",
    "print(f\"Hours ahead = {forecastlen / (60/timeslotLength)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 49, 2) (862, 49, 2) 862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trez\\AppData\\Local\\Temp/ipykernel_20292/1327526145.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  P[f\"{airport}_dep_predicted\"] = ypredFull[:, idd, 1]\n",
      "C:\\Users\\Trez\\AppData\\Local\\Temp/ipykernel_20292/1327526145.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  P[f\"{airport}_arr_actual\"] = yactualFull[:, idd, 0]\n",
      "C:\\Users\\Trez\\AppData\\Local\\Temp/ipykernel_20292/1327526145.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  P[f\"{airport}_arr_predicted\"] = ypredFull[:, idd, 0]\n",
      "C:\\Users\\Trez\\AppData\\Local\\Temp/ipykernel_20292/1327526145.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  P[f\"{airport}_dep_actual\"] = yactualFull[:, idd, 1]\n"
     ]
    }
   ],
   "source": [
    "def getLabelArrays(hour, input_sequence_length=input_sequence_length):\n",
    "    syncindex = input_sequence_length + hour - 1\n",
    "    yTestPred = model.predict(test_dataset, verbose=0)\n",
    "    ypredFull = yTestPred[:windowSize, hour - 1, :, :]\n",
    "    yactualFull = Ytest[syncindex : windowSize + syncindex :, :]\n",
    "\n",
    "    mae1 = mean_absolute_error(ypredFull[:, :, 0], yactualFull[:, :, 0])\n",
    "    mae2 = mean_absolute_error(ypredFull[:, :, 1], yactualFull[:, :, 1])\n",
    "    rmse1 = np.sqrt(mean_squared_error(ypredFull[:, :, 0], yactualFull[:, :, 0]))\n",
    "    rmse2 = np.sqrt(mean_squared_error(ypredFull[:, :, 1], yactualFull[:, :, 1]))\n",
    "    r2_1 = r2_score(ypredFull[:, :, 0], yactualFull[:, :, 0])\n",
    "    r2_2 = r2_score(ypredFull[:, :, 1], yactualFull[:, :, 1])\n",
    "\n",
    "    time = testTimes[syncindex : windowSize + syncindex :]\n",
    "    return ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time\n",
    "\n",
    "\n",
    "def plotComparison(airport_index, hour):\n",
    "    ypredFull, yactualFull, _, _, _, _, _, _, time = getLabelArrays(hour)\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, num=airport_index)\n",
    "    axs[0].plot(\n",
    "        time, yactualFull[:, 0 + airport_index, 0], label=\"Actual Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, yactualFull[:, airport_index, 1], label=\"Actual Departure Delay\")\n",
    "    axs[0].plot(\n",
    "        time, ypredFull[:, 0 + airport_index, 0], label=\"Predicted Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, ypredFull[:, airport_index, 1], label=\"Predicted Departure Delay\")\n",
    "    axs[0].legend()\n",
    "    # axs[1].legend()\n",
    "    axs[1].set_xlabel(\"Time (hours)\")\n",
    "    axs[0].set_ylabel(\"Arrival Delay (mins)\")\n",
    "    axs[1].set_ylabel(\"Departure Delay (mins)\")\n",
    "    plt.suptitle(\n",
    "        f\"Comparison for: {airports[airport_index]}. Forward: {hour / (60/timeslotLength)}h, Backward: {input_sequence_length / (60/timeslotLength)}h\"\n",
    "    )\n",
    "    axs[1].xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "    axs[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(6)\n",
    "\n",
    "\n",
    "def plotErrorVsLookback(forecast_horizon=forecast_horizon, save=True, saveFolder=\"GNNPredictions\"):\n",
    "    fig2, axss = plt.subplots(3, 1, sharex=True, num=100)\n",
    "    idxs = list(range(1, forecast_horizon + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, forecast_horizon+1)]\n",
    "    maeList = []\n",
    "    maeList2 = []\n",
    "    rmseList = []\n",
    "    rmseList2 = []\n",
    "    r2List = []\n",
    "    r2List2 = []\n",
    "\n",
    "    for h in idxs:\n",
    "        ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time = getLabelArrays(h)\n",
    "        maeList.append(mae1)\n",
    "        maeList2.append(mae2)\n",
    "        rmseList.append(rmse1)\n",
    "        rmseList2.append(rmse2)\n",
    "        r2List.append(r2_1)\n",
    "        r2List2.append(r2_2)\n",
    "\n",
    "\n",
    "    \n",
    "    axss[0].plot(lookforwards, maeList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[0].plot(lookforwards, maeList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List2, label = \"Departure Delay\", marker=\"o\")\n",
    "\n",
    "    axss[0].set_ylabel(\"MAE (minutes)\")\n",
    "    axss[1].set_ylabel(\"RMSE (minutes)\")\n",
    "    axss[2].set_ylabel(\"R2 Score\")\n",
    "    # axss[0].grid()\n",
    "    # axss[1].grid()\n",
    "    # axss[2].grid()\n",
    "\n",
    "    axss[2].set_xlabel(\"Look forward (hours)\")\n",
    "\n",
    "    axss[0].legend()\n",
    "\n",
    "    fig2.set_figwidth(8)\n",
    "    fig2.set_figheight(12)\n",
    "\n",
    "plotErrorVsLookback()\n",
    "plt.show()\n",
    "\n",
    "for airportidx in range(0, int(len(airports)/5)):\n",
    "    plotComparison(airportidx, forecastlen)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Save to csv\n",
    "def savePredictions(saveFolder = \"predictions\"):\n",
    "    print(\"Saving to CSV\")\n",
    "    idxs = list(range(1, forecast_horizon + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, forecast_horizon+1)]\n",
    "    maxlen = len(Ytest)\n",
    "\n",
    "    for idx, lookforward in zip(idxs, lookforwards):\n",
    "        syncindex = input_sequence_length + idx - 1\n",
    "        yTestPred = model.predict(test_dataset, verbose=0)\n",
    "        ypredFull = yTestPred[:maxlen-syncindex, idx - 1, :, :]\n",
    "\n",
    "        yactualFull = Ytest[syncindex:maxlen - idxs[-1] + idx:, :]\n",
    "\n",
    "        time = testTimes[syncindex:maxlen - idxs[-1] + idx]\n",
    "        print(ypredFull.shape, yactualFull.shape, len(time))\n",
    "\n",
    "        P = pd.DataFrame()\n",
    "        P[\"time\"] = time\n",
    "        # Double for loop here unfortunately ~fix later~\n",
    "        for idd, airport in enumerate(airports):\n",
    "            P[f\"{airport}_arr_actual\"] = yactualFull[:, idd, 0]\n",
    "            P[f\"{airport}_arr_predicted\"] = ypredFull[:, idd, 0]\n",
    "            P[f\"{airport}_dep_actual\"] = yactualFull[:, idd, 1]\n",
    "            P[f\"{airport}_dep_predicted\"] = ypredFull[:, idd, 1]\n",
    "        # print(P)\n",
    "        savelocation = saveFolder + \"/\" + f\"{lookforward}h.csv\"\n",
    "        P.to_csv(savelocation)\n",
    "\n",
    "\n",
    "savePredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for Kepler gl visualization\n",
    "If wanted, the data and predictions can be converted to the right format for use with kepler.gl. Kepler is the visualisation tool used for our report cover and in our presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11676/340879366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msyncindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_sequence_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhour\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0myTestPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mypredFull\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myTestPred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhour\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtimeKepler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestTimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msyncindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mYkepler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msyncindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hour' is not defined"
     ]
    }
   ],
   "source": [
    "# syncindex = input_sequence_length + hour - 1\n",
    "# yTestPred = model.predict(test_dataset, verbose=0)\n",
    "# ypredFull = yTestPred[:, hour - 1, :, :]\n",
    "# timeKepler = testTimes[syncindex::]\n",
    "# Ykepler = Ytest[syncindex:, :]\n",
    "# generateKeplerData(\n",
    "#     airports=airports,\n",
    "#     start=timeKepler[0],\n",
    "#     end=end,\n",
    "#     timeslotLength=timeslotLength,\n",
    "#     predictions=ypredFull,\n",
    "#     actual=Ykepler,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add logbook to tensorboard\n",
    "If wanted, the training logbook can be saved to Tensorboard.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./logs \\\n",
    "#   --name \"Tuning model 2\" \\\n",
    "#   --description \"Trying some stuff ans last run on TOP50\"\\\n",
    "#   --one_shot\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
