{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, MeanSquaredLogarithmicError\n",
    "\n",
    "from extraction.extract import *\n",
    "from extraction.extractionvalues import *\n",
    "from extraction.extractadjacency import getAdjacencyMatrix, distance_weight_adjacency\n",
    "# from extraction.adj_data import *\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings control the jupyter notebook. We have generated and stored 2 models, one for the top10 airports and one the top50. The default settings loads the model for the top 50 and displays the results. To try the top10 model, change airportsFull to ICAOTOP10 and runName to \"top10MSE\". To try a custom run, choose an airport list of your liking, change runName to something else, change saveModel to True, and loadModel to False. For reference, top10 took about 15 minutes to train and top50 about 45 minutes. \n",
    "\n",
    "Once your settings are set, you can just click run all in the jupyter interface. In the final plotting cell more settings can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsFull = ICAOTOP50\n",
    "airports = airportsFull[::]\n",
    "airports.remove(\"LTFM\")  # removing the new istanbul airport as it opened mid-2019\n",
    "\n",
    "n_nodes = n_airports = len(airports)\n",
    "start = datetime(2018, 3, 1)\n",
    "end = datetime(2019, 1, 1)\n",
    "timeslotLength = 30\n",
    "\n",
    "# Run settings\n",
    "batch_size = 64\n",
    "epochs = 1500\n",
    "patience = 100\n",
    "input_sequence_length = 4 * int(60/timeslotLength)\n",
    "forecast_horizon = 10\n",
    "multi_horizon = True\n",
    "\n",
    "learning_rate = 0.0001 \n",
    "# learning_rate = 0.0005\n",
    "\n",
    "\n",
    "runName = \"top50MSE30minsV6\"\n",
    "saveModel = False\n",
    "loadModel = True\n",
    "modelSaveFolder = \"kerasModels/\"\n",
    "modelSaveLocation = modelSaveFolder + runName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = generateNNdataMultiple(\n",
    "    airports, timeslotLength, GNNFormat=True, start=start, end=end, disableWeather=True\n",
    ")\n",
    "times = list(dataDict.values())[0][\"T\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the amount of features and normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove some of the most uncorrelated features\n",
    "columnsToDrop = [\n",
    "    \"weekend\",\n",
    "    \"winter\",\n",
    "    \"spring\",\n",
    "    \"summer\",\n",
    "    \"autumn\",\n",
    "    \"night\",\n",
    "    \"morning\",\n",
    "    \"afternoon\",\n",
    "    \"planes\",\n",
    "]\n",
    "\n",
    "Xlist = []\n",
    "Ylist = []\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler = RobustScaler()\n",
    "for airport in airports:\n",
    "    # T x F\n",
    "    X = dataDict[airport][\"X\"].drop(columnsToDrop, axis=1).to_numpy()\n",
    "    # X = scaler.fit_transform(X)\n",
    "    Xlist.append(X)\n",
    "\n",
    "    Y = dataDict[airport][\"Y\"].iloc[:, :].to_numpy()\n",
    "    Ylist.append(Y)\n",
    "\n",
    "\n",
    "Xlist = np.stack(Xlist)\n",
    "Ylist = np.stack(Ylist)\n",
    "# N x T x F\n",
    "Xarray = np.swapaxes(Xlist, 0, 1)\n",
    "Yarray = np.swapaxes(Ylist, 0, 1)\n",
    "\n",
    "# Reshape to a flat array that goes arrival then departure delay\n",
    "# Yarray = np.reshape(Yarray, newshape=[len(times), len(airports)*2], order=\"F\")\n",
    "# # T x N x F\n",
    "\n",
    "# # Normalise over the features\n",
    "Xmean, Xstd = Xarray.mean(axis=0), Xarray.std(axis=0)\n",
    "Xarray = (Xarray - Xmean) / Xstd\n",
    "\n",
    "print(\"T x N x F: \", \"Xarray =\", Xarray.shape, \"|\", \"Yarray =\", Yarray.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make the raw data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = 0.6, 0.25\n",
    "\n",
    "fullLength = len(times)\n",
    "train_idx = int(train_split * fullLength)\n",
    "val_idx = int((val_split + train_split) * fullLength)\n",
    "print(\n",
    "    f\"Train split: {0}:{train_idx} | Validation split: {train_idx}:{val_idx} | Test split: {val_idx}:{fullLength}\"\n",
    ")\n",
    "\n",
    "# generate raw splits\n",
    "Xtrain, Xval, Xtest = Xarray[0:train_idx], Xarray[train_idx:val_idx], Xarray[val_idx::]\n",
    "Ytrain, Yval, Ytest = Yarray[0:train_idx], Yarray[train_idx:val_idx], Yarray[val_idx::]\n",
    "\n",
    "# Save test timeslots for plotting purposes\n",
    "testTimes = times.iloc[val_idx::, 0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensorflow dataset that handles lookback, lookforward and multi-horizon properties. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(\n",
    "    data_array: np.ndarray,\n",
    "    target_array,\n",
    "    input_sequence_length: int,\n",
    "    forecast_horizon: int,\n",
    "    batch_size: int = 128,\n",
    "    shuffle=True,\n",
    "    multi_horizon=False,\n",
    "):\n",
    "    \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "    `inputs` is a Tensor\n",
    "    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "    the `input_sequence_length` past values of the timeseries for each node.\n",
    "    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "    containing the `forecast_horizon`\n",
    "    future values of the timeseries for each node.\n",
    "\n",
    "    Args:\n",
    "        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "        input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "            timeseries `forecast_horizon` steps ahead (only one value).\n",
    "        batch_size: Number of timeseries samples in each batch.\n",
    "        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "        multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "    Returns:\n",
    "        A tf.data.Dataset instance.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = timeseries_dataset_from_array(\n",
    "        data_array[:-forecast_horizon],\n",
    "        None,\n",
    "        sequence_length=input_sequence_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = inputs\n",
    "    target_offset = (\n",
    "        input_sequence_length\n",
    "        if multi_horizon\n",
    "        else input_sequence_length + forecast_horizon - 1\n",
    "    )\n",
    "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "    targets = timeseries_dataset_from_array(\n",
    "        target_array[target_offset:],\n",
    "        None,\n",
    "        sequence_length=target_seq_length,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "    return dataset.prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_dataset = create_tf_dataset(\n",
    "    Xtrain,\n",
    "    Ytrain,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "val_dataset = create_tf_dataset(\n",
    "    Xval,\n",
    "    Yval,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "test_dataset = create_tf_dataset(\n",
    "    Xtest,\n",
    "    Ytest,\n",
    "    input_sequence_length,\n",
    "    forecast_horizon,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    multi_horizon=multi_horizon,\n",
    ")\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was our original intent to use dynamic graphs for our st-gnn, however due to lack of time we kept it as a weighted average of the distance based distance_weight_adjacency() and an average of the adjancy matrices based on flights getAdjacencyMatrix()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_adj = distance_weight_adjacency(airports, threshold=400)\n",
    "# unfortunately we only work with static graphs for now\n",
    "# We take the average of all the dynamic adjacencies from the function above (stored for brevity)\n",
    "flight_adj = np.mean(getAdjacencyMatrix(airports, start, end, timeslotLength), axis=0)\n",
    "\n",
    "# print(flight_adj)\n",
    "\n",
    "# flight_adj = flight_adj_avg\n",
    "\n",
    "adjacency_matrix = distance_adj * 0.4 + (1 - 0.4) * flight_adj\n",
    "\n",
    "node_indices, neighbor_indices = np.where(adjacency_matrix != 0)\n",
    "\n",
    "graph_edges = (node_indices.tolist(), neighbor_indices.tolist())\n",
    "graph_num_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "print(flight_adj.shape)\n",
    "# print(graph_edges)\n",
    "print(graph_num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block shows a heatmap of the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adjacency_matrix)\n",
    "plt.xticks(range(len(airports)), airports, fontsize=12, rotation=-90)\n",
    "plt.yticks(range(len(airports)), airports, fontsize=12, rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our GCN layer. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        # graph_info: GraphInfo,\n",
    "        graph_num_nodes=graph_num_nodes,\n",
    "        graph_edges: tuple = graph_edges,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        activation: typing.Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "\n",
    "        self.graph_edges = graph_edges\n",
    "        self.graph_num_nodes = graph_num_nodes\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=keras.initializers.glorot_uniform()(\n",
    "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.activation = layers.Activation(activation)\n",
    "\n",
    "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
    "        aggregation_func = {\n",
    "            \"sum\": tf.math.unsorted_segment_sum,\n",
    "            \"mean\": tf.math.unsorted_segment_mean,\n",
    "            \"max\": tf.math.unsorted_segment_max,\n",
    "        }.get(self.aggregation_type)\n",
    "\n",
    "        if aggregation_func:\n",
    "            return aggregation_func(\n",
    "                neighbour_representations,\n",
    "                self.graph_edges[0],\n",
    "                num_segments=self.graph_num_nodes,\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
    "\n",
    "    def compute_nodes_representation(self, features: tf.Tensor):\n",
    "        \"\"\"Computes each node's representation.\n",
    "\n",
    "        The nodes' representations are obtained by multiplying the features tensor with\n",
    "        `self.weight`. Note that\n",
    "        `self.weight` has shape `(in_feat, out_feat)`.\n",
    "\n",
    "        Args:\n",
    "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        return tf.matmul(features, self.weight)\n",
    "\n",
    "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
    "        neighbour_representations = tf.gather(features, self.graph_edges[1])\n",
    "        aggregated_messages = self.aggregate(neighbour_representations)\n",
    "        return tf.matmul(aggregated_messages, self.weight)\n",
    "\n",
    "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
    "        if self.combination_type == \"concat\":\n",
    "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            h = nodes_representation + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        return self.activation(h)\n",
    "\n",
    "    def call(self, features: tf.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
    "        \"\"\"\n",
    "        nodes_representation = self.compute_nodes_representation(features)\n",
    "        aggregated_messages = self.compute_aggregated_messages(features)\n",
    "        return self.update(nodes_representation, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM including graph convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our complete model, it is based off a road-traffic example that predicts all features for the next step. We modified it heavily to allow for predictions of 2 labels and multihorizon predictions. Retrieved from a keras example and modified: https://keras.io/examples/timeseries/timeseries_traffic_forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGC(layers.Layer):\n",
    "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        lstm_units: int,\n",
    "        input_seq_len: int,\n",
    "        graph_edges: tuple,\n",
    "        graph_num_nodes: int,\n",
    "        graph_conv_params: typing.Optional[dict] = None,\n",
    "        num_labels: int = 2,\n",
    "        multi_horizon: int = True,\n",
    "        forecast_horizon: int = forecast_horizon,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if multi_horizon:\n",
    "            self.forecast_horizon = forecast_horizon\n",
    "        else:\n",
    "            self.forecast_horizon = 1\n",
    "\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.lstm_units = lstm_units\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.graph_edges = (graph_edges,)\n",
    "        self.graph_num_nodes = (graph_num_nodes,)\n",
    "\n",
    "        self.graph_conv_params = graph_conv_params\n",
    "        self.multi_horizon = multi_horizon\n",
    "        self.num_labels = num_labels\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # graph conv layer\n",
    "        if graph_conv_params is None:\n",
    "            graph_conv_params = {\n",
    "                \"aggregation_type\": \"mean\",\n",
    "                \"combination_type\": \"concat\",\n",
    "                \"activation\": None,\n",
    "            }\n",
    "\n",
    "        # Layer definitions\n",
    "        self.graph_conv = GraphConv(in_feat, out_feat, **graph_conv_params)\n",
    "        # self.graph_conv2 = GraphConv(in_feat, out_feat, self.graph_edges, self.graph_num_nodes, **graph_conv_params)\n",
    "        l2_reg = 2.5e-4  # L2 regularization rate\n",
    "\n",
    "        self.lstm1 = layers.LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=True,\n",
    "            activation=\"tanh\",\n",
    "            dropout=0.2,\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            activity_regularizer=l2(l2_reg),\n",
    "            bias_regularizer=l2(l2_reg),\n",
    "        )\n",
    "        self.lstm2 = layers.LSTM(\n",
    "            lstm_units,\n",
    "            activation=\"tanh\",\n",
    "            dropout=0.2,\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            activity_regularizer=l2(l2_reg),\n",
    "            bias_regularizer=l2(l2_reg),\n",
    "        )\n",
    "        # self.dense = layers.Dense(output_seq_len)\n",
    "        self.denseThick = layers.Dense(128)\n",
    "        self.denseThick2 = layers.Dense(64)\n",
    "        self.denseThick3 = layers.Dense(16)\n",
    "        self.dense = layers.Dense(self.forecast_horizon * self.num_labels)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"in_feat\": self.in_feat,\n",
    "                \"out_feat\": self.out_feat,\n",
    "                \"lstm_units\": self.lstm_units,\n",
    "                \"input_seq_len\": self.input_seq_len,\n",
    "                \"graph_conv_params\": self.graph_conv_params,\n",
    "                \"graph_edges\": self.graph_edges,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
    "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "        gcn_out = self.graph_conv(inputs)\n",
    "\n",
    "        # print(f\"The GCN output shape  = {gcn_out}\")\n",
    "        shape = tf.shape(gcn_out)\n",
    "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            shape[2],\n",
    "            shape[3],\n",
    "        )\n",
    "        # LSTM takes only 3D tensors as input\n",
    "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
    "        # print(f\"The input shape for the LSTM is {gcn_out}\")\n",
    "        lstmLayer1 = self.lstm1(\n",
    "            gcn_out\n",
    "        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
    "        lstmLayer2 = self.lstm2(lstmLayer1)\n",
    "        dense_1 = self.denseThick(lstmLayer2)\n",
    "        dense_2 = self.denseThick2(dense_1)\n",
    "        dense_3 = self.denseThick3(dense_2)\n",
    "\n",
    "        dense_output = self.dense(dense_3)\n",
    "        # dense_output has shape: (batch_size * num_nodes, multi_thing*2)\n",
    "\n",
    "        output = tf.reshape(\n",
    "            dense_output,\n",
    "            (num_nodes, batch_size, self.forecast_horizon, self.num_labels),\n",
    "        )\n",
    "        final = tf.transpose(output, [1, 2, 0, 3])\n",
    "        return final\n",
    "        # # returns Tensor of shape (batch_size, forecast_horizon, num_nodes, nlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feat = Xarray.shape[2]\n",
    "\n",
    "out_feat = 15\n",
    "lstm_units = 128\n",
    "graph_conv_params = {\n",
    "    \"aggregation_type\": \"mean\",\n",
    "    \"combination_type\": \"concat\",\n",
    "    \"activation\": None,\n",
    "}\n",
    "\n",
    "st_gcn = LSTMGC(\n",
    "    in_feat=in_feat,\n",
    "    out_feat=out_feat,\n",
    "    lstm_units=lstm_units,\n",
    "    input_seq_len=input_sequence_length,\n",
    "    graph_edges=graph_edges,\n",
    "    graph_num_nodes=graph_num_nodes,\n",
    "    graph_conv_params=graph_conv_params,\n",
    "    num_labels=2,\n",
    "    multi_horizon=multi_horizon,\n",
    "    forecast_horizon=forecast_horizon,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = layers.Input((input_sequence_length, graph_num_nodes, in_feat))\n",
    "outputs = st_gcn(inputs)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=MeanSquaredError(reduction=\"auto\", name=\"mean_absolute_error\"),\n",
    "    weighted_metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model, if loading the model from a previous run was not requestd. Using tensorboard, training can be followed live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadModel:\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=patience),\n",
    "            tensorboard_callback,\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveModel and not loadModel:\n",
    "    model.save(modelSaveLocation)\n",
    "elif saveModel and loadModel:\n",
    "    raise Exception(\n",
    "        \"You have enabled both loading and saving, which are incompatible. Model has not been saved\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadModel:\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadModel:\n",
    "    model = keras.models.load_model(modelSaveLocation)\n",
    "    model.summary()\n",
    "else:\n",
    "    model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analize model performance on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook gives the performance per forecast horizon in the first plot. In the second plot, the predicted airport delay over time is compared to the real delay over time to illustrate the accuracy of the model. The default setting displays a window of 100 hours. As the model has a multi-horizon feature, you can choose for how many hours into the future to predict. The model is trained to predict between 1 and 10 hours into the future. You can choose to display a bigger window with the windowSize feature and how many hours ahead predicted should be illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 200  # 1-400\n",
    "\n",
    "forecastlen = 10  # 1-10\n",
    "forecastlen = min(forecastlen, forecast_horizon)\n",
    "print(f\"Hours ahead = {forecastlen / (60/timeslotLength)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelArrays(hour, input_sequence_length=input_sequence_length):\n",
    "    syncindex = input_sequence_length + hour - 1\n",
    "    yTestPred = model.predict(test_dataset, verbose=0)\n",
    "    ypredFull = yTestPred[:windowSize, hour - 1, :, :]\n",
    "    yactualFull = Ytest[syncindex : windowSize + syncindex :, :]\n",
    "\n",
    "    mae1 = mean_absolute_error(ypredFull[:, :, 0], yactualFull[:, :, 0])\n",
    "    mae2 = mean_absolute_error(ypredFull[:, :, 1], yactualFull[:, :, 1])\n",
    "    rmse1 = np.sqrt(mean_squared_error(ypredFull[:, :, 0], yactualFull[:, :, 0]))\n",
    "    rmse2 = np.sqrt(mean_squared_error(ypredFull[:, :, 1], yactualFull[:, :, 1]))\n",
    "    r2_1 = r2_score(ypredFull[:, :, 0], yactualFull[:, :, 0])\n",
    "    r2_2 = r2_score(ypredFull[:, :, 1], yactualFull[:, :, 1])\n",
    "\n",
    "    time = testTimes[syncindex : windowSize + syncindex :]\n",
    "    return ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time\n",
    "\n",
    "\n",
    "def plotComparison(airport_index, hour):\n",
    "    ypredFull, yactualFull, _, _, _, _, _, _, time = getLabelArrays(hour)\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, num=airport_index)\n",
    "    axs[0].plot(\n",
    "        time, yactualFull[:, 0 + airport_index, 0], label=\"Actual Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, yactualFull[:, airport_index, 1], label=\"Actual Departure Delay\")\n",
    "    axs[0].plot(\n",
    "        time, ypredFull[:, 0 + airport_index, 0], label=\"Predicted Arrival Delay\"\n",
    "    )\n",
    "    axs[1].plot(time, ypredFull[:, airport_index, 1], label=\"Predicted Departure Delay\")\n",
    "    axs[0].legend()\n",
    "    # axs[1].legend()\n",
    "    axs[1].set_xlabel(\"Time (hours)\")\n",
    "    axs[0].set_ylabel(\"Arrival Delay (mins)\")\n",
    "    axs[1].set_ylabel(\"Departure Delay (mins)\")\n",
    "    plt.suptitle(\n",
    "        f\"Comparison for: {airports[airport_index]}. Forward: {hour / (60/timeslotLength)}h, Backward: {input_sequence_length / (60/timeslotLength)}h\"\n",
    "    )\n",
    "    axs[1].xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "    axs[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(6)\n",
    "\n",
    "\n",
    "def plotErrorVsLookback(forecast_horizon=forecast_horizon, save=True, saveFolder=\"GNNPredictions\"):\n",
    "    fig2, axss = plt.subplots(3, 1, sharex=True, num=100)\n",
    "    idxs = list(range(1, forecast_horizon + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, forecast_horizon+1)]\n",
    "    maeList = []\n",
    "    maeList2 = []\n",
    "    rmseList = []\n",
    "    rmseList2 = []\n",
    "    r2List = []\n",
    "    r2List2 = []\n",
    "\n",
    "    for h in idxs:\n",
    "        ypredFull, yactualFull, mae1, mae2, rmse1, rmse2, r2_1, r2_2, time = getLabelArrays(h)\n",
    "        maeList.append(mae1)\n",
    "        maeList2.append(mae2)\n",
    "        rmseList.append(rmse1)\n",
    "        rmseList2.append(rmse2)\n",
    "        r2List.append(r2_1)\n",
    "        r2List2.append(r2_2)\n",
    "\n",
    "\n",
    "    \n",
    "    axss[0].plot(lookforwards, maeList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[0].plot(lookforwards, maeList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[1].plot(lookforwards, rmseList2, label = \"Departure Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List, label = \"Arrival Delay\", marker=\"o\")\n",
    "    axss[2].plot(lookforwards, r2List2, label = \"Departure Delay\", marker=\"o\")\n",
    "\n",
    "    axss[0].set_ylabel(\"MAE (minutes)\")\n",
    "    axss[1].set_ylabel(\"RMSE (minutes)\")\n",
    "    axss[2].set_ylabel(\"R2 Score\")\n",
    "    # axss[0].grid()\n",
    "    # axss[1].grid()\n",
    "    # axss[2].grid()\n",
    "\n",
    "    axss[2].set_xlabel(\"Look forward (hours)\")\n",
    "\n",
    "    axss[0].legend()\n",
    "\n",
    "    fig2.set_figwidth(8)\n",
    "    fig2.set_figheight(12)\n",
    "\n",
    "plotErrorVsLookback()\n",
    "plt.show()\n",
    "\n",
    "for airportidx in range(0, int(len(airports)/5)):\n",
    "    plotComparison(airportidx, forecastlen)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Save to csv\n",
    "def savePredictions(saveFolder = \"predictions\"):\n",
    "    print(\"Saving to CSV\")\n",
    "    idxs = list(range(1, forecast_horizon + 1))\n",
    "    lookforwards = [x/(60/timeslotLength) for x in range(1, forecast_horizon+1)]\n",
    "    maxlen = len(Ytest)\n",
    "\n",
    "    for idx, lookforward in zip(idxs, lookforwards):\n",
    "        syncindex = input_sequence_length + idx - 1\n",
    "        yTestPred = model.predict(test_dataset, verbose=0)\n",
    "        ypredFull = yTestPred[:maxlen-syncindex, idx - 1, :, :]\n",
    "\n",
    "        yactualFull = Ytest[syncindex:maxlen - idxs[-1] + idx:, :]\n",
    "\n",
    "        time = testTimes[syncindex:maxlen - idxs[-1] + idx]\n",
    "        # print(ypredFull.shape, yactualFull.shape, len(time))\n",
    "\n",
    "        P = pd.DataFrame()\n",
    "        P[\"time\"] = time\n",
    "        # Double for loop here unfortunately ~fix later~\n",
    "        for idd, airport in enumerate(airports):\n",
    "            P[f\"{airport}_arr_actual\"] = yactualFull[:, idd, 0]\n",
    "            P[f\"{airport}_arr_predicted\"] = ypredFull[:, idd, 0]\n",
    "            P[f\"{airport}_dep_actual\"] = yactualFull[:, idd, 1]\n",
    "            P[f\"{airport}_dep_predicted\"] = ypredFull[:, idd, 1]\n",
    "        # print(P)\n",
    "        savelocation = saveFolder + \"/\" + f\"{lookforward}h.csv\"\n",
    "        P.to_csv(savelocation)\n",
    "\n",
    "\n",
    "savePredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for Kepler gl visualization\n",
    "If wanted, the data and predictions can be converted to the right format for use with kepler.gl. Kepler is the visualisation tool used for our report cover and in our presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncindex = input_sequence_length + hour - 1\n",
    "# yTestPred = model.predict(test_dataset, verbose=0)\n",
    "# ypredFull = yTestPred[:, hour - 1, :, :]\n",
    "# timeKepler = testTimes[syncindex::]\n",
    "# Ykepler = Ytest[syncindex:, :]\n",
    "# generateKeplerData(\n",
    "#     airports=airports,\n",
    "#     start=timeKepler[0],\n",
    "#     end=end,\n",
    "#     timeslotLength=timeslotLength,\n",
    "#     predictions=ypredFull,\n",
    "#     actual=Ykepler,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add logbook to tensorboard\n",
    "If wanted, the training logbook can be saved to Tensorboard.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard dev upload --logdir ./logs \\\n",
    "#   --name \"Tuning model 2\" \\\n",
    "#   --description \"Trying some stuff ans last run on TOP50\"\\\n",
    "#   --one_shot\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8b8190b409df59d083e48feca7ac41a34361ff0d7727e2b40e3d45f8724b63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
